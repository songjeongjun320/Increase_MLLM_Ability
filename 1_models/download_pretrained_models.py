#!/usr/bin/env python3
"""
Hugging Face ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸
ì§€ì •ëœ í´ë”ì— ì§ì ‘ ì €ì¥í•©ë‹ˆë‹¤.
"""

import os
import torch
import json
from typing import Dict, Any
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM
)

def download_and_save_model(model_name: str, save_dir: str, model_type: str) -> Dict[str, Any]:
    """
    ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ê³  ì§€ì •ëœ ë””ë ‰í† ë¦¬ì— ì§ì ‘ ì €ì¥
    """
    print(f"ğŸ“¥ {model_name} ë‹¤ìš´ë¡œë“œ ì¤‘...")
    
    try:
        # ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(save_dir, exist_ok=True)
        
        # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë‹¤ìš´ë¡œë“œ
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            trust_remote_code=True
        )
        
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        
        # ì§€ì •ëœ ë””ë ‰í† ë¦¬ì— ì§ì ‘ ì €ì¥
        print(f"ğŸ’¾ {save_dir}ì— ì €ì¥ ì¤‘...")
        tokenizer.save_pretrained(save_dir)
        model.save_pretrained(save_dir)
        
        # config.jsonì— model_type ì¶”ê°€ (ì˜¤ë¥˜ ë°©ì§€)
        config_path = os.path.join(save_dir, "config.json")
        if os.path.exists(config_path):
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            config["model_type"] = model_type
            with open(config_path, 'w', encoding='utf-8') as f:
                json.dump(config, f, indent=2, ensure_ascii=False)
            print(f"ğŸ”§ config.jsonì— model_type='{model_type}' ì¶”ê°€")
        
        print(f"âœ… {model_name} â†’ {save_dir} ì €ì¥ ì™„ë£Œ!")
        
        return {
            "tokenizer": tokenizer, 
            "model": model, 
            "model_name": model_name,
            "local_path": save_dir,
            "success": True
        }
        
    except Exception as e:
        print(f"âŒ {model_name} ì €ì¥ ì‹¤íŒ¨: {e}")
        return {"success": False, "error": str(e)}

def check_gpu_memory():
    """
    GPU ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
    """
    if torch.cuda.is_available():
        print(f"ğŸ–¥ï¸  GPU ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name()}")
        print(f"ğŸ“Š GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB")
    else:
        print("âš ï¸  GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")

def main():
    """
    ë©”ì¸ í•¨ìˆ˜ - ëª¨ë“  ëª¨ë¸ì„ ì§€ì •ëœ í´ë”ì— ì§ì ‘ ë‹¤ìš´ë¡œë“œ
    """
    print("ğŸš€ Hugging Face ëª¨ë¸ ë‹¤ìš´ë¡œë“œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...\n")
    
    # GPU ë©”ëª¨ë¦¬ í™•ì¸
    check_gpu_memory()
    print()
    
    # ê¸°ë³¸ ì €ì¥ ìœ„ì¹˜
    base_models_dir = "/scratch/jsong132/Increase_MLLM_Ability/Base_Models"
    os.makedirs(base_models_dir, exist_ok=True)
    
    # ë‹¤ìš´ë¡œë“œí•  ëª¨ë¸ ì •ë³´
    models_to_download = [
        # {
        #     "model_name": "meta-llama/Llama-3.2-3B",
        #     "folder_name": "llama-3.2-3b-pt",
        #     "model_type": "llama",
        #     "display_name": "ğŸ¦™ Llama 3.2 3B"
        # },
        # {
        #     "model_name": "Qwen/Qwen2.5-3B", 
        #     "folder_name": "qwem-2.5-3b-pt",
        #     "model_type": "qwen2",
        #     "display_name": "ğŸ¤– Qwen 2.5 3B"
        # },
        {
            "model_name": "google/gemma-3-4b-pt",
            "folder_name": "gemma-3-4b-pt", 
            "model_type": "gemma3",
            "display_name": "ğŸ’ Gemma 3 4B"
        }
    ]
    
    print(f"ğŸ“ ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {base_models_dir}")
    print("ğŸ“‚ ê° ëª¨ë¸ì´ ì €ì¥ë  í´ë”:")
    for model_info in models_to_download:
        folder_path = os.path.join(base_models_dir, model_info["folder_name"])
        print(f"   â””â”€â”€ {folder_path}/")
        print(f"       â”œâ”€â”€ config.json")
        print(f"       â”œâ”€â”€ tokenizer.json") 
        print(f"       â”œâ”€â”€ tokenizer_config.json")
        print(f"       â”œâ”€â”€ model files...")
        print(f"       â””â”€â”€ ...")
    print()
    
    # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ì €ì¥
    downloaded_models = []
    
    for i, model_info in enumerate(models_to_download, 1):
        print(f"\n{'='*60}")
        print(f"ğŸ“¥ [{i}/{len(models_to_download)}] {model_info['display_name']} ì²˜ë¦¬ ì¤‘...")
        print(f"{'='*60}")
        
        model_name = model_info["model_name"]
        folder_name = model_info["folder_name"]
        model_type = model_info["model_type"]
        save_path = os.path.join(base_models_dir, folder_name)
        
        print(f"ğŸ”— Hugging Face: {model_name}")
        print(f"ğŸ“ ì €ì¥ ê²½ë¡œ: {save_path}")
        
        # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ì €ì¥
        result = download_and_save_model(model_name, save_path, model_type)
        
        if result.get("success"):
            downloaded_models.append(result)
            
            # ì €ì¥ëœ íŒŒì¼ í™•ì¸
            if os.path.exists(save_path):
                file_count = len([f for f in os.listdir(save_path) if os.path.isfile(os.path.join(save_path, f))])
                folder_size = sum(os.path.getsize(os.path.join(dirpath, filename))
                                for dirpath, dirnames, filenames in os.walk(save_path)
                                for filename in filenames) / (1024**3)
                print(f"ğŸ“Š ì €ì¥ëœ íŒŒì¼: {file_count}ê°œ, í¬ê¸°: {folder_size:.1f}GB")
        else:
            print(f"âŒ {model_info['display_name']} ì €ì¥ ì‹¤íŒ¨")
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        if 'tokenizer' in result:
            del result['tokenizer']
        if 'model' in result:
            del result['model']
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    
    # ìµœì¢… ê²°ê³¼ ìš”ì•½
    print(f"\n{'='*60}")
    print("ğŸ“‹ ë‹¤ìš´ë¡œë“œ ê²°ê³¼ ìš”ì•½")
    print(f"{'='*60}")
    
    if downloaded_models:
        print(f"ğŸ‰ ì´ {len(downloaded_models)}ê°œ ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!\n")
        
        for i, model_info in enumerate(downloaded_models, 1):
            folder_name = os.path.basename(model_info['local_path'])
            print(f"{i}. âœ… {folder_name}")
            print(f"   ğŸ“‚ {model_info['local_path']}")
            
            # ì£¼ìš” íŒŒì¼ í™•ì¸
            key_files = ["config.json", "tokenizer.json", "tokenizer_config.json"]
            for key_file in key_files:
                file_path = os.path.join(model_info['local_path'], key_file)
                status = "âœ…" if os.path.exists(file_path) else "âŒ"
                print(f"   {status} {key_file}")
            print()
        
        print(f"ğŸ“ ëª¨ë“  ëª¨ë¸ì´ {base_models_dir} í•˜ìœ„ì˜ ì§€ì •ëœ í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
    else:
        print("âŒ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    return downloaded_models

def test_model_loading():
    """
    ì €ì¥ëœ ëª¨ë¸ë“¤ì´ ì œëŒ€ë¡œ ë¡œë“œë˜ëŠ”ì§€ í…ŒìŠ¤íŠ¸
    """
    print("\nğŸ§ª ì €ì¥ëœ ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸...")
    
    base_models_dir = "/scratch/jsong132/Increase_MLLM_Ability/Base_Models"
    model_folders = ["llama-3.2-3b-pt", "qwem-2.5-3b-pt", "gemma-3-4b-pt"]
    
    for folder in model_folders:
        model_path = os.path.join(base_models_dir, folder)
        if os.path.exists(model_path):
            try:
                print(f"ğŸ”„ {folder} ë¡œë”© í…ŒìŠ¤íŠ¸ ì¤‘...")
                tokenizer = AutoTokenizer.from_pretrained(model_path)
                print(f"   âœ… í† í¬ë‚˜ì´ì € ë¡œë”© ì„±ê³µ")
                
                # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ ëª¨ë¸ì€ ë¡œë”©í•˜ì§€ ì•Šê³  configë§Œ í™•ì¸
                config_path = os.path.join(model_path, "config.json")
                if os.path.exists(config_path):
                    with open(config_path, 'r') as f:
                        config = json.load(f)
                    print(f"   âœ… config.json í™•ì¸ (model_type: {config.get('model_type', 'None')})")
                else:
                    print(f"   âŒ config.json ì—†ìŒ")
                    
            except Exception as e:
                print(f"   âŒ {folder} ë¡œë”© ì‹¤íŒ¨: {e}")
        else:
            print(f"âŒ {folder} í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")

if __name__ == "__main__":
    # í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ì•ˆë‚´
    print("ğŸ“¦ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬:")
    print("pip install transformers torch accelerate")
    print("pip install sentencepiece protobuf")
    print("pip install Pillow")
    print("-" * 60)
    
    # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤í–‰
    downloaded_models = main()
    
    # ë¡œë”© í…ŒìŠ¤íŠ¸ ì˜µì…˜
    if downloaded_models:
        test_loading = input("\nğŸ§ª ì €ì¥ëœ ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸ë¥¼ ìˆ˜í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").lower().strip()
        if test_loading == 'y':
            test_model_loading()
    
    print(f"\n{'='*60}")
    print("ğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    print("ğŸ“ ì§€ì •ëœ í´ë”ì— ëª¨ë¸ë“¤ì´ ì§ì ‘ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print(f"{'='*60}")