import os
import json
import logging
import torch
import warnings
import transformers
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel
from tqdm import tqdm
import re
try:
    from tqdm.contrib.logging import logging_redirect_tqdm
except ImportError:
    # Fallback for older tqdm versions
    from contextlib import nullcontext
    logging_redirect_tqdm = nullcontext
from dataclasses import dataclass, field
import gc
import sys
import time
import random

# Import ToW token checker
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
from check_tokenizer import check_tow_tokens_for_eval

os.environ['TRANSFORMERS_VERBOSITY'] = 'error'
transformers.logging.set_verbosity_error()
warnings.filterwarnings("ignore", message=".*generation flags.*not valid.*")
torch.backends.cuda.matmul.allow_tf32 = True
if torch.cuda.is_available():
    torch.set_float32_matmul_precision("high") 

# --- Model Configuration ---
@dataclass
class ModelConfig:
    name: str
    model_id: str
    adapter_path: str = None
    use_quantization: bool = True
    torch_dtype: torch.dtype = field(default=torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16)

MODEL_CONFIGS = [
    # ModelConfig(
    #     name="llama-3.2-3b-pt",
    #     model_id="/scratch/jsong132/Increase_MLLM_Ability/Base_Models/llama-3.2-3b-pt",
    #     use_quantization=False
    # ),
    # ModelConfig(
    #     name="qwem-2.5-3b-pt",
    #     model_id="/scratch/jsong132/Increase_MLLM_Ability/Base_Models/qwem-2.5-3b-pt",
    #     use_quantization=False
    # ),
    # ModelConfig(
    #     name="gemma-3-4b-pt",
    #     model_id="/scratch/jsong132/Increase_MLLM_Ability/Base_Models/gemma-3-4b-pt",
    #     use_quantization=False
    # ),
    # ModelConfig(
    #     name="olmo-2-0425-1b",
    #     model_id="/scratch/jsong132/Increase_MLLM_Ability/Base_Models/olmo-2-0425-1b",
    #     use_quantization=False
    # ),

    # ModelConfig(
    #     name="llama-3.2-3b-pt-tow-09_11_2epoch_allenai-merged",
    #     model_id="/scratch/jsong132/Increase_MLLM_Ability/5_training/finetune_org/merged_models/llama-3.2-3b-pt-tow-09_11_2epoch_allenai-merged",
    #     use_quantization=False
    # ),
    # ModelConfig(
    #     name="qwem-2.5-3b-pt-tow-09_11_2epoch_allenai-merged",
    #     model_id="/scratch/jsong132/Increase_MLLM_Ability/5_training/finetune_org/merged_models/qwem-2.5-3b-pt-tow-09_11_2epoch_allenai-merged",
    #     use_quantization=False
    # ),
    # ModelConfig(
    #     name="gemma-3-4b-pt-tow-09_11_2epoch_allenai-merged",
    #     model_id="/scratch/jsong132/Increase_MLLM_Ability/5_training/finetune_org/merged_models/gemma-3-4b-pt-tow-09_11_2epoch_allenai-merged",
    #     use_quantization=False
    # ),
    # ModelConfig(
    #     name="olmo-2-0425-1b-tow-09_11_2epoch_allenai-merged",
    #     model_id="/scratch/jsong132/Increase_MLLM_Ability/5_training/finetune_org/merged_models/olmo-2-0425-1b-tow-09_11_2epoch_allenai-merged",
    #     use_quantization=False
    # ),

    # ModelConfig(
    #     name="llama-3.2-3b-tow-09_11_2epoch_org_initialize-merged",
    #     model_id="/scratch/jsong132/Increase_MLLM_Ability/5_training/finetune_org/merged_models/llama-3.2-3b-tow-09_11_2epoch_org_initialize-merged",
    #     use_quantization=False
    # ),
    # ModelConfig(
    #     name="qwem-2.5-3b-pt-tow-09_11_2epoch_org_initialize-merged",
    #     model_id="/scratch/jsong132/Increase_MLLM_Ability/5_training/finetune_org/merged_models/qwem-2.5-3b-pt-tow-09_11_2epoch_org_initialize-merged",
    #     use_quantization=False
    # ),
    # ModelConfig(
    #     name="gemma-3-4b-pt-tow-09_11_2epoch_org_initialize-merged",
    #     model_id="/scratch/jsong132/Increase_MLLM_Ability/5_training/finetune_org/merged_models/gemma-3-4b-pt-tow-09_11_2epoch_org_initialize-merged",
    #     use_quantization=False
    # ),
    # ModelConfig(
    #     name="olmo-2-0425-1b-tow-09_11_2epoch_org_initialize-merged",
    #     model_id="/scratch/jsong132/Increase_MLLM_Ability/5_training/finetune_org/merged_models/olmo-2-0425-1b-tow-09_11_2epoch_org_initialize-merged",
    #     use_quantization=False
    # ),

    # ModelConfig(
    #     name="llama-3.2-3b-tow-09_11_2epoch_fix_tow-merged",
    #     model_id="/scratch/jsong132/Increase_MLLM_Ability/5_training/finetune_org/merged_models/llama-3.2-3b-tow-09_11_2epoch_fix_tow-merged",
    #     use_quantization=False
    # ),
    # ModelConfig(
    #     name="qwem-2.5-3b-tow-09_11_2epoch_fix_tow-merged",
    #     model_id="/scratch/jsong132/Increase_MLLM_Ability/5_training/finetune_org/merged_models/qwem-2.5-3b-tow-09_11_2epoch_fix_tow-merged",
    #     use_quantization=False
    # ),
    # ModelConfig(
    #     name="gemma-3-4b-tow-09_11_2epoch_fix_tow-merged",
    #     model_id="/scratch/jsong132/Increase_MLLM_Ability/5_training/finetune_org/merged_models/gemma-3-4b-tow-09_11_2epoch_fix_tow-merged",
    #     use_quantization=False
    # ),
    # ModelConfig(
    #     name="olmo-2-0425-1b-tow-09_11_2epoch_fix_tow-merged",
    #     model_id="/scratch/jsong132/Increase_MLLM_Ability/5_training/finetune_org/merged_models/olmo-2-0425-1b-tow-09_11_2epoch_fix_tow-merged",
    #     use_quantization=False
    # ),

    # 10 Epochs
    # ModelConfig(
    #     name="llama-3.2-3b-pt-tow-09_11_10epoch-merged",
    #     model_id="/scratch/jsong132/Increase_MLLM_Ability/5_training/finetune_org/merged_models/llama-3.2-3b-pt-tow-09_11_allenai-merged",
    #     use_quantization=False
    # ),
    # ModelConfig(
    #     name="qwem-2.5-3b-pt-tow-09_11_10epoch-merged",
    #     model_id="/scratch/jsong132/Increase_MLLM_Ability/5_training/finetune_org/merged_models/qwem-2.5-3b-pt-tow-09_11_allenai-merged",
    #     use_quantization=False
    # ),
    # ModelConfig(
    #     name="gemma-3-4b-pt-tow-09_11_10epoch-merged",
    #     model_id="/scratch/jsong132/Increase_MLLM_Ability/5_training/finetune_org/merged_models/gemma-3-4b-pt-tow-09_11_allenai-merged",
    #     use_quantization=False
    # ),
    # ModelConfig(
    #     name="olmo-2-0425-1b-tow-09_11_10epoch-merged",
    #     model_id="/scratch/jsong132/Increase_MLLM_Ability/5_training/finetune_org/merged_models/olmo-2-0425-1b-tow-09_11_allenai-merged",
    #     use_quantization=False
    # ),

    # ModelConfig(
    #     name="llama-2-7b-pretrained",
    #     model_id="/scratch/jsong132/Increase_MLLM_Ability/Base_Models/Llama-2-7b-hf_pretrained",
    #     use_quantization=False
    # ),
    # ModelConfig(
    #     name="qwem-2.5-7b-it",
    #     model_id="/scratch/jsong132/Increase_MLLM_Ability/Base_Models/Qwen2.5-7B-Instruct",
    #     use_quantization=False
    # ),
    # ModelConfig(
    #     name="tow-llama2-7b",
    #     model_id="/scratch/jsong132/Increase_MLLM_Ability/1_models/tow-llama2-7b_downloaded",
    #     use_quantization=False
    # ),
    # ModelConfig(
    #     name="bow-qwen2.5-7b-it",
    #     model_id="/scratch/jsong132/Increase_MLLM_Ability/1_models/bow-qwen2.5-7b-i_downloaded",
    #     use_quantization=False
    # ),


    ModelConfig(
        name="jamba-reasoning-3b",
        model_id="/scratch/jsong132/Increase_MLLM_Ability/Base_Models/AI21-Jamba-Reasoning-3B",
        use_quantization=False
    ),
    ModelConfig(
        name="gemma3-4b-it",
        model_id="/scratch/jsong132/Increase_MLLM_Ability/Base_Models/gemma-3-4b-it",
        use_quantization=False
    ),
    ModelConfig(
        name="llama3.2-3b-it",
        model_id="/scratch/jsong132/Increase_MLLM_Ability/Base_Models/Llama-3.2-3B-Instruct",
        use_quantization=False
    ),
    ModelConfig(
        name="qwen2.5-3b-it",
        model_id="/scratch/jsong132/Increase_MLLM_Ability/Base_Models/Qwen2.5-3B-Instruct",
        use_quantization=False
    ),
]

# --- General Configuration ---
ARC_DATASET_PATH = "../../2_datasets/ARC/ARC.json"
KO_ARC_DATASET_PATH = "../../2_datasets/ARC/Ko-ARC.json"
BASE_OUTPUT_DIR = "10_16_instruction_tuned_models"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
CACHE_DIR = "./cache" if not os.path.exists("/scratch/jsong132/.cache/huggingface") else "/scratch/jsong132/.cache/huggingface"
BATCH_SIZE = 16

# --- Logging Setup ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - [%(name)s] - %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger(__name__)

ARC_5SHOT_EXAMPLES = [
    {
        "question": "Which of the following is the primary source of energy for most ecosystems on Earth?",
        "options": {
            "A": "Fungi",
            "B": "Herbivores",
            "C": "The Sun",
            "D": "Carnivores"
        },
        "cot_content": """Let's think step by step. The question asks for the primary, or ultimate, source of energy for most ecosystems. Let's analyze the options. A food chain shows how energy is transferred. Option A, Fungi, are decomposers. They get energy from dead organic material, so they are part of the energy cycle, but not the primary source. Option B, Herbivores, are primary consumers. They get their energy by eating plants (producers). So they are not the source. Option D, Carnivores, are secondary or tertiary consumers. They get energy by eating other animals. They are even further down the energy transfer chain. Option C, The Sun. Plants (producers) use sunlight to create their own food through photosynthesis. This chemical energy is the foundation of almost every food chain. Therefore, the sun is the primary source of energy.""",
        "answer": "C"
    },
    {
        "question": "Which physical process describes a liquid turning into a gas?",
        "options": {
            "A": "Melting",
            "B": "Freezing",
            "C": "Condensation",
            "D": "Evaporation"
        },
        "cot_content": """Let's think step by step. The question is about the phase transition from liquid to gas. Option A, Melting, is the process of a solid turning into a liquid. This is incorrect. Option B, Freezing, is the process of a liquid turning into a solid. This is incorrect. Option C, Condensation, is the process of a gas turning into a liquid. This is the reverse of what the question asks. This is incorrect. Option D, Evaporation (or boiling), is the process where a liquid substance becomes a gas. This directly matches the question.""",
        "answer": "D"
    },
    {
        "question": "A student wants to test how the amount of sunlight affects the growth of a bean plant. Which of the following is the independent variable in her experiment?",
        "options": {
            "A": "the height of the plant",
            "B": "the amount of water given to the plant",
            "C": "the amount of sunlight",
            "D": "the type of soil"
        },
        "cot_content": """Let's think step by step. An experiment tests how an independent variable affects a dependent variable. The independent variable is the one factor that the scientist intentionally changes or manipulates. The student wants to see the effect *of* the amount of sunlight. This is the factor she will change. Option A, the height of the plant, is what is being measured to see the effect. This is the dependent variable. Option B, the amount of water, and Option D, the type of soil, should be kept the same for all plants to ensure a fair test. These are controlled variables. Option C, the amount of sunlight, is the one thing the student is purposefully changing to observe its effect on growth. Therefore, it is the independent variable.""",
        "answer": "C"
    },
]

KO_ARC_5SHOT_EXAMPLES = [
    {
        "question": "ë‹¤ìŒ ì¤‘ ì§€êµ¬ìƒ ëŒ€ë¶€ë¶„ì˜ ìƒíƒœê³„ì—ì„œ ì£¼ìš” ì—ë„ˆì§€ì›ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ?",
        "options": {
            "A": "ê· ë¥˜",
            "B": "ì´ˆì‹ë™ë¬¼",
            "C": "íƒœì–‘",
            "D": "ìœ¡ì‹ë™ë¬¼"
        },
        "cot_content": """ë‹¨ê³„ë³„ë¡œ ìƒê°í•´ë´…ì‹œë‹¤. ì´ ì§ˆë¬¸ì€ ëŒ€ë¶€ë¶„ì˜ ìƒíƒœê³„ì—ì„œ ê°€ì¥ ê·¼ì›ì ì¸ ì—ë„ˆì§€ ê³µê¸‰ì›ì´ ë¬´ì—‡ì¸ì§€ ë¬»ê³  ìˆìŠµë‹ˆë‹¤. ì„ íƒì§€ë¥¼ ë¶„ì„í•´ ë´…ì‹œë‹¤. ë¨¹ì´ ì‚¬ìŠ¬ì€ ì—ë„ˆì§€ê°€ ì–´ë–»ê²Œ ì „ë‹¬ë˜ëŠ”ì§€ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ì„ íƒì§€ A, ê· ë¥˜ëŠ” ë¶„í•´ìì…ë‹ˆë‹¤. ì£½ì€ ìœ ê¸°ë¬¼ë¡œë¶€í„° ì—ë„ˆì§€ë¥¼ ì–»ìœ¼ë¯€ë¡œ ì—ë„ˆì§€ ìˆœí™˜ì˜ ì¼ë¶€ì´ì§€ë§Œ ê·¼ì›ì ì¸ ì—ë„ˆì§€ì›ì€ ì•„ë‹™ë‹ˆë‹¤. ì„ íƒì§€ B, ì´ˆì‹ë™ë¬¼ì€ 1ì°¨ ì†Œë¹„ìì…ë‹ˆë‹¤. ì‹ë¬¼(ìƒì‚°ì)ì„ ë¨¹ìŒìœ¼ë¡œì¨ ì—ë„ˆì§€ë¥¼ ì–»ìœ¼ë¯€ë¡œ ì—ë„ˆì§€ì›ì´ ì•„ë‹™ë‹ˆë‹¤. ì„ íƒì§€ D, ìœ¡ì‹ë™ë¬¼ì€ 2ì°¨ ë˜ëŠ” 3ì°¨ ì†Œë¹„ìì…ë‹ˆë‹¤. ë‹¤ë¥¸ ë™ë¬¼ì„ ë¨¹ìŒìœ¼ë¡œì¨ ì—ë„ˆì§€ë¥¼ ì–»ìœ¼ë©°, ì—ë„ˆì§€ ì „ë‹¬ ë‹¨ê³„ì—ì„œ ë” ë’¤ì— ìˆìŠµë‹ˆë‹¤. ì„ íƒì§€ C, íƒœì–‘. ì‹ë¬¼(ìƒì‚°ì)ì€ ê´‘í•©ì„±ì„ í†µí•´ íƒœì–‘ë¹›ì„ ì´ìš©í•˜ì—¬ ìŠ¤ìŠ¤ë¡œ ì–‘ë¶„ì„ ë§Œë“­ë‹ˆë‹¤. ì´ í™”í•™ ì—ë„ˆì§€ê°€ ê±°ì˜ ëª¨ë“  ë¨¹ì´ ì‚¬ìŠ¬ì˜ ê¸°ì´ˆê°€ ë©ë‹ˆë‹¤. ë”°ë¼ì„œ íƒœì–‘ì´ ì£¼ìš” ì—ë„ˆì§€ì›ì…ë‹ˆë‹¤.""",
        "answer": "C"
    },
    {
        "question": "ì•¡ì²´ê°€ ê¸°ì²´ë¡œ ë³€í•˜ëŠ” ë¬¼ë¦¬ì  ê³¼ì •ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ?",
        "options": {
            "A": "ìœµí•´",
            "B": "ì‘ê³ ",
            "C": "ì•¡í™”",
            "D": "ì¦ë°œ"
        },
        "cot_content": """ë‹¨ê³„ë³„ë¡œ ìƒê°í•´ë´…ì‹œë‹¤. ì´ ì§ˆë¬¸ì€ ì•¡ì²´ì—ì„œ ê¸°ì²´ë¡œì˜ ìƒíƒœ ë³€í™”ì— ê´€í•œ ê²ƒì…ë‹ˆë‹¤. ì„ íƒì§€ A, ìœµí•´ëŠ” ê³ ì²´ê°€ ì•¡ì²´ë¡œ ë³€í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. í‹€ë ¸ìŠµë‹ˆë‹¤. ì„ íƒì§€ B, ì‘ê³ ëŠ” ì•¡ì²´ê°€ ê³ ì²´ë¡œ ë³€í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. í‹€ë ¸ìŠµë‹ˆë‹¤. ì„ íƒì§€ C, ì•¡í™”ëŠ” ê¸°ì²´ê°€ ì•¡ì²´ë¡œ ë³€í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. ì§ˆë¬¸ê³¼ ë°˜ëŒ€ë˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. í‹€ë ¸ìŠµë‹ˆë‹¤. ì„ íƒì§€ D, ì¦ë°œ(ë˜ëŠ” ë“ìŒ)ì€ ì•¡ì²´ ë¬¼ì§ˆì´ ê¸°ì²´ë¡œ ë³€í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. ì´ëŠ” ì§ˆë¬¸ê³¼ ì •í™•íˆ ì¼ì¹˜í•©ë‹ˆë‹¤.""",
        "answer": "D"
    },
    {
        "question": "í•œ í•™ìƒì´ í–‡ë¹›ì˜ ì–‘ì´ ê°•ë‚­ì½© ì‹ë¬¼ì˜ ì„±ì¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì‹œí—˜í•˜ê³  ì‹¶ì–´ í•©ë‹ˆë‹¤. ì´ ì‹¤í—˜ì—ì„œ ë…ë¦½ ë³€ì¸ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ?",
        "options": {
            "A": "ì‹ë¬¼ì˜ í‚¤",
            "B": "ì‹ë¬¼ì—ê²Œ ì£¼ëŠ” ë¬¼ì˜ ì–‘",
            "C": "í–‡ë¹›ì˜ ì–‘",
            "D": "í† ì–‘ì˜ ì¢…ë¥˜"
        },
        "cot_content": """ë‹¨ê³„ë³„ë¡œ ìƒê°í•´ë´…ì‹œë‹¤. ì‹¤í—˜ì€ ë…ë¦½ ë³€ì¸ì´ ì¢…ì† ë³€ì¸ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì‹œí—˜í•©ë‹ˆë‹¤. ë…ë¦½ ë³€ì¸ì€ ê³¼í•™ìê°€ ì˜ë„ì ìœ¼ë¡œ ë³€í™”ì‹œí‚¤ê±°ë‚˜ ì¡°ì‘í•˜ëŠ” í•˜ë‚˜ì˜ ìš”ì¸ì…ë‹ˆë‹¤. í•™ìƒì€ í–‡ë¹›ì˜ ì–‘ì´ ë¯¸ì¹˜ëŠ” 'ì˜í–¥'ì„ ë³´ê³  ì‹¶ì–´ í•˜ë¯€ë¡œ, í–‡ë¹›ì˜ ì–‘ì´ ë°”ë¡œ í•™ìƒì´ ë³€í™”ì‹œí‚¬ ìš”ì¸ì…ë‹ˆë‹¤. ì„ íƒì§€ A, ì‹ë¬¼ì˜ í‚¤ëŠ” í–‡ë¹›ì˜ ì˜í–¥ì„ í™•ì¸í•˜ê¸° ìœ„í•´ ì¸¡ì •ë˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ê²ƒì€ ì¢…ì† ë³€ì¸ì…ë‹ˆë‹¤. ì„ íƒì§€ B, ë¬¼ì˜ ì–‘ê³¼ ì„ íƒì§€ D, í† ì–‘ì˜ ì¢…ë¥˜ëŠ” ê³µì •í•œ ì‹¤í—˜ì„ ìœ„í•´ ëª¨ë“  ì‹ë¬¼ì—ê²Œ ë™ì¼í•˜ê²Œ ìœ ì§€ë˜ì–´ì•¼ í•©ë‹ˆë‹¤. ì´ê²ƒë“¤ì€ í†µì œ ë³€ì¸ì…ë‹ˆë‹¤. ì„ íƒì§€ C, í–‡ë¹›ì˜ ì–‘ì€ í•™ìƒì´ ì„±ì¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ê´€ì°°í•˜ê¸° ìœ„í•´ ì˜ë„ì ìœ¼ë¡œ ë³€í™”ì‹œí‚¤ëŠ” ìœ ì¼í•œ ê²ƒì…ë‹ˆë‹¤. ë”°ë¼ì„œ ì´ê²ƒì´ ë…ë¦½ ë³€ì¸ì…ë‹ˆë‹¤.""",
        "answer": "C"
    },
]

# --- Helper Functions for 3-shot ARC Evaluation ---
def create_3shot_prompt(item, examples, dataset_type="arc", add_bos_token=False, bos_token=""):
    """
    (ìµœì¢… ê°œì„  ë²„ì „)
    ë”•ì…”ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ í˜•íƒœì˜ ê³ í’ˆì§ˆ 3-shot ì˜ˆì œë¥¼ ì‚¬ìš©í•˜ì—¬
    ARC / Ko-ARC í‰ê°€ í”„ë¡¬í”„íŠ¸ë¥¼ ë™ì ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.
    OLMo ëª¨ë¸ì˜ ê²½ìš° BOS í† í°ì„ ì‹œì‘ì— ì¶”ê°€í•©ë‹ˆë‹¤.
    """
    if dataset_type == "arc":
        prompt_parts = ["The following are multiple choice questions about science and reasoning. You MUST choose one of the option A~D.\n"]
        response_header = "Response:"
        cot_trigger = "Let's think step by step."
        final_answer_prefix = "Therefore Answer:"
        
    else:  # ko-arc
        prompt_parts = ["ë‹¤ìŒì€ ê³¼í•™ê³¼ ì¶”ë¡ ì— ê´€í•œ ê°ê´€ì‹ ë¬¸ì œë“¤ì…ë‹ˆë‹¤. Aë¶€í„° Dê¹Œì§€ì˜ ë³´ê¸°ì¤‘ ë¬´ì¡°ê±´ í•˜ë‚˜ì˜ ë‹µë§Œ ì„ íƒí•˜ì„¸ìš”.\n"]
        response_header = "ì‘ë‹µ:"
        cot_trigger = "ë‹¨ê³„ì ìœ¼ë¡œ ìƒê°í•´ë´…ì‹œë‹¤."
        final_answer_prefix = "ë”°ë¼ì„œ ì •ë‹µ:"

    # 1. 3ê°œì˜ ì˜ˆì œë¥¼ ë™ì ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.
    for example in examples:
        # ì˜ˆì œ ë”•ì…”ë„ˆë¦¬ì—ì„œ ê° ë¶€ë¶„ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
        question = example["question"]
        options_dict = example["options"]
        cot_content = example["cot_content"] # ì‹¤ì œ ì¶”ë¡  ê³¼ì •ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
        answer = example["answer"]

        # ì§ˆë¬¸ê³¼ ì„ íƒì§€ë¥¼ í”„ë¡¬í”„íŠ¸ì— ì¶”ê°€í•©ë‹ˆë‹¤.
        prompt_parts.append(question)
        for key, value in sorted(options_dict.items()):
            prompt_parts.append(f"{key}. {value}")
        
        # ì‹¤ì œ ì¶”ë¡  ê³¼ì •ê³¼ ìµœì¢… ë‹µë³€ í˜•ì‹ì„ í¬í•¨í•œ ì™„ì „í•œ ì‘ë‹µ ë¸”ë¡ì„ ë§Œë“­ë‹ˆë‹¤.
        full_response_block = f"{response_header} {cot_content} #### {final_answer_prefix} {{{answer}}}. #### {{{answer}}}."
        prompt_parts.append(full_response_block)
        prompt_parts.append("") # ì˜ˆì œ ì‚¬ì´ì— ë¹ˆ ì¤„ ì¶”ê°€

    # 2. ëª¨ë¸ì´ í’€ì–´ì•¼ í•  ì‹¤ì œ ë¬¸ì œë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
    test_question = item.get("question", "")
    prompt_parts.append(test_question)
    # ì‹¤ì œ ë°ì´í„°ì…‹('item')ì˜ ì„ íƒì§€ í˜•ì‹ì— ë§ì¶° ì²˜ë¦¬í•©ë‹ˆë‹¤.
    for key in ['A', 'B', 'C', 'D']:
        if key in item:
            prompt_parts.append(f"{key}. {item[key]}")
    prompt_parts.append("")

    # 3. ëª¨ë¸ì˜ ì¶”ë¡ ì„ ìœ ë„í•˜ëŠ” ê¹”ë”í•œ ì‹œì‘ ì‹ í˜¸ë¡œ í”„ë¡¬í”„íŠ¸ë¥¼ ë§ˆë¬´ë¦¬í•©ë‹ˆë‹¤.
    prompt_parts.append(f"{response_header} {cot_trigger}")
    
    final_prompt = "\n".join(prompt_parts)
    
    # OLMo ëª¨ë¸ì˜ ê²½ìš° BOS í† í°ì„ ì‹œì‘ì— ì¶”ê°€ (ë¬¸ì œ ë°œìƒ ì‹œ ë¹„í™œì„±í™”)
    if add_bos_token and bos_token:
        # ì„ì‹œë¡œ BOS í† í° ì¶”ê°€ë¥¼ ë¹„í™œì„±í™”í•˜ì—¬ í…ŒìŠ¤íŠ¸
        # final_prompt = bos_token + final_prompt
        logger.warning("OLMo BOS í† í° ì¶”ê°€ ì„ì‹œ ë¹„í™œì„±í™” (ë””ë²„ê¹…ìš©)")
        pass
    
    return final_prompt


def process_single_with_retry(model, tokenizer, prompt, config, max_retries=0):
    """
    Process a single prompt with retry logic for answer extraction failures
    Only retries when answer extraction fails (not on genuine model errors)
    """
    last_generated_text = None  # Store the last generated text for debugging
    
    # max_retries=0: 1ë²ˆë§Œ ì‹œë„, max_retries>0: retry í¬í•¨
    total_attempts = max_retries + 1
    
    for attempt in range(total_attempts):
        try:
            inputs = tokenizer(prompt, return_tensors="pt", padding=True, truncation=True, max_length=4096).to(DEVICE)
            
            with torch.inference_mode():
                # OLMo ëª¨ë¸ ì „ìš© ìƒì„± íŒŒë¼ë¯¸í„° (ë‹¨ì¼ ìƒ˜í”Œ)
                if "olmo" in config.name.lower():
                    # OLMo ëª¨ë¸ ë””ë²„ê¹… ì •ë³´
                    logger.info(f"OLMo ë””ë²„ê¹…: PAD={tokenizer.pad_token_id}, EOS={tokenizer.eos_token_id}, BOS={getattr(tokenizer, 'bos_token_id', None)}")
                    logger.info(f"OLMo ë””ë²„ê¹…: Input shape={inputs['input_ids'].shape}")
                    
                    # OLMo ë¬¸ì œ í† í°ë“¤ ì°¨ë‹¨
                    bad_words = ["setattr", "ForcedSuppressWarnings", "RI", "kommsetattr", "despre", "empire", "FLICT", "PrivateKey", "TestCase"]
                    bad_words_ids = []
                    for word in bad_words:
                        try:
                            word_ids = tokenizer.encode(word, add_special_tokens=False)
                            if len(word_ids) > 0:
                                bad_words_ids.append(word_ids)
                        except:
                            continue
                    
                    # OLMo under-trained tokens ë¬¸ì œ í•´ê²°
                    generation_kwargs = {
                        "max_new_tokens": 512,      # ì›ë˜ í† í° ìˆ˜ ìœ ì§€
                        "do_sample": True,          # ìƒ˜í”Œë§ í™œì„±í™”
                        "temperature": 0.7,         # ì˜¨ë„ ì„¤ì •
                        "top_p": 0.9,              # Top-p ìƒ˜í”Œë§
                        "repetition_penalty": 1.1, # ë°˜ë³µ ë°©ì§€
                        "pad_token_id": tokenizer.pad_token_id,
                        "eos_token_id": tokenizer.eos_token_id,
                        "use_cache": True,
                    }
                    
                    # Bad words í•„í„° ì¶”ê°€
                    if bad_words_ids:
                        generation_kwargs["bad_words_ids"] = bad_words_ids
                        logger.info(f"OLMo Bad words í•„í„° ì ìš©: {len(bad_words_ids)}ê°œ ë‹¨ì–´")
                    logger.info("OLMo ì„ì‹œ ì„¤ì •: ë°˜ë³µ ë°©ì§€ íŒŒë¼ë¯¸í„° ì ìš©")
                    logger.info(f"OLMo ìƒì„± íŒŒë¼ë¯¸í„°: {generation_kwargs}")
                else:
                    # ë‹¤ë¥¸ ëª¨ë¸ë“¤ì€ ê¸°ì¡´ íŒŒë¼ë¯¸í„° ìœ ì§€
                    generation_kwargs = {
                        "max_new_tokens": 512,
                        "pad_token_id": tokenizer.pad_token_id,
                        "eos_token_id": tokenizer.eos_token_id,
                        "do_sample": False,
                    }

                outputs = model.generate(
                    **inputs,
                    **generation_kwargs
                )
            
            input_lengths = inputs['input_ids'].shape[1]
            output_only_tokens = outputs[:, input_lengths:]
            # OLMoì˜ ê²½ìš° special tokensì„ ì œê±°í•´ì„œ ë””ì½”ë”© (under-trained tokens ë¬¸ì œ í•´ê²°)
            if "olmo" in config.name.lower():
                generated_text = tokenizer.decode(output_only_tokens[0], skip_special_tokens=True).strip()
                logger.info(f"OLMo ë””ë²„ê¹…: Special tokens ì œê±° ë””ì½”ë”©")
            else:
                generated_text = tokenizer.decode(output_only_tokens[0], skip_special_tokens=True).strip()
            last_generated_text = generated_text  # Always store the actual generated text
            
            # OLMo ë””ë²„ê¹…: ìƒì„± ê²°ê³¼ í™•ì¸
            if "olmo" in config.name.lower():
                logger.info(f"OLMo ë””ë²„ê¹…: Output shape={outputs.shape}, Generated tokens={output_only_tokens.shape}")
                logger.info(f"OLMo ë””ë²„ê¹…: Generated text length={len(generated_text)}, Text preview='{generated_text[:100]}'")
                logger.info(f"OLMo ë””ë²„ê¹…: Raw token IDs={output_only_tokens[0][:20].tolist()}")  # ì²˜ìŒ 20ê°œ í† í° ID
                
                # ê°œë³„ í† í° ë””ë²„ê¹…
                logger.info("OLMo ê°œë³„ í† í° ë¶„ì„:")
                for i, token_id in enumerate(output_only_tokens[0][:20].tolist()):
                    try:
                        token_text = tokenizer.decode([token_id])
                        logger.info(f"Token {i}: ID={token_id}, Text='{token_text}'")
                    except Exception as e:
                        logger.error(f"Token {i}: ID={token_id}, Decode error: {e}")
            
            # Try to extract answer
            extracted_answer = extract_answer_robust(generated_text)
            if extracted_answer is not None:
                return generated_text, extracted_answer
            else:
                # Answer extraction failed - try again if we have retries left
                if attempt < total_attempts - 1:
                    logger.warning(f"Retry {attempt + 1}/{total_attempts}: Failed to extract answer, retrying...")
                    # Small delay before retry
                    time.sleep(0.1 + random.random() * 0.1)
                    continue
                else:
                    logger.warning(f"Final attempt failed - could not extract answer after {total_attempts} attempts")
                    return generated_text, None
                    
        except Exception as e:
            logger.error(f"Retry {attempt + 1}/{total_attempts}: Model inference error: {e}")
            if attempt < total_attempts - 1:
                time.sleep(0.2 + random.random() * 0.2)
                continue
            else:
                # Return error info after all retries exhausted, but preserve last generated text if available
                error_message = f"ERROR after {total_attempts} attempts: {str(e)}"
                if last_generated_text is not None:
                    return f"{error_message}\nLAST_GENERATED_TEXT: {last_generated_text}", None
                else:
                    return error_message, None
    
    # If we get here, all retries were exhausted due to extraction failures
    # Return the last generated text for debugging, not a hardcoded message
    if last_generated_text is not None:
        return last_generated_text, None
    else:
        return f"NO_GENERATION_AFTER_{max_retries}_ATTEMPTS", None

def extract_answer_robust(model_output: str) -> str:
    """
    Extract the final answer (A, B, C, D) from model output using STRICT validation.
    Returns None if no clear structured answer is found.
    STRICT MODE: Only accepts {} format - unified across all evaluation scripts.
    """
    if not model_output:
        return None

    cleaned_output = model_output.strip().upper()

    import re

    # STRICT: Only accept {} format for consistency across all evaluation scripts
    box_pattern = r'\{([A-D])\}'
    box_matches = re.findall(box_pattern, cleaned_output)
    if box_matches:
        return box_matches[0]  # Return the last match (final answer)

    # No fallback patterns - forces models to use {} format only
    return None

def load_arc_data(filepath):
    """Loads ARC data from a JSON file."""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
        logger.info(f"Loaded {len(data)} items from {filepath}")
        return data
    except Exception as e:
        logger.error(f"Error loading data from {filepath}: {e}")
        return None

def get_ground_truth(item):
    """Returns the ground truth answer letter."""
    answer = item.get("answer", "")
    if answer in ['A', 'B', 'C', 'D']:
        return answer
    return None

def save_failure_cases(failure_cases, model_name, output_dir):
    """
    Save failure cases to a separate JSON file for analysis.
    """
    failure_filepath = os.path.join(output_dir, f"failure_cases_{model_name}_3shot.json")
    
    with open(failure_filepath, 'w', encoding='utf-8') as f:
        json.dump(failure_cases, f, indent=2, ensure_ascii=False)
    
    logger.info(f"Saved {len(failure_cases)} failure cases to {failure_filepath}")

# --- Single Model Evaluation Function with 3-shot Prompting ---
def evaluate_single_model(config: ModelConfig, arc_data: list, ko_arc_data: list, model_specific_output_dir: str):
    """
    Performs 3-shot ARC evaluation for a single model on both datasets.
    """
    results_filepath = os.path.join(model_specific_output_dir, f"results_{config.name}_3shot.json")
    log_filepath = os.path.join(model_specific_output_dir, f"eval_{config.name}_3shot.log")
    raw_gen_filepath = os.path.join(model_specific_output_dir, f"raw_generations_{config.name}_3shot.json")

    # --- Setup Logging for this specific model ---
    file_handler = logging.FileHandler(log_filepath, mode='w', encoding='utf-8')
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - [%(name)s] - %(message)s')
    file_handler.setFormatter(formatter)
    root_logger = logging.getLogger()
    
    # Remove previous file handlers to avoid duplicate logging
    for handler in list(root_logger.handlers):
        if isinstance(handler, logging.FileHandler):
            root_logger.removeHandler(handler)
    root_logger.addHandler(file_handler)

    logger.info(f"--- Starting 3-shot Evaluation for Model: {config.name} ({config.model_id}) ---")
    logger.info(f"Results will be saved to: {results_filepath}")

    model = None
    tokenizer = None
    raw_generations_list = []

    try:
        # --- Load Model and Tokenizer ---
        tokenizer_load_path = config.adapter_path if config.adapter_path else config.model_id
        logger.info(f"Loading tokenizer from: {os.path.abspath(tokenizer_load_path)}")
        tokenizer = AutoTokenizer.from_pretrained(tokenizer_load_path, cache_dir=CACHE_DIR, padding_side='left')

        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token

        # OLMo ì „ìš© í† í¬ë‚˜ì´ì € ì„¤ì • ê°œì„  (under-trained tokens ë¬¸ì œ í•´ê²°)
        if "olmo" in config.name.lower():
            logger.info("OLMo ëª¨ë¸ ê°ì§€: under-trained tokens ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ í† í¬ë‚˜ì´ì € ì„¤ì •")
            
            # ê¸°ë³¸ íŠ¹ìˆ˜ í† í° ì„¤ì •
            if tokenizer.pad_token is None:
                if tokenizer.unk_token:
                    tokenizer.pad_token = tokenizer.unk_token
                    logger.info(f"OLMo PAD í† í°: UNK í† í° ì‚¬ìš© ({tokenizer.unk_token})")
                else:
                    # ê¸°ì¡´ vocabì—ì„œ ì‚¬ìš©ë˜ì§€ ì•ŠëŠ” í† í°ìœ¼ë¡œ ì„¤ì •
                    tokenizer.pad_token = tokenizer.eos_token
                    logger.info(f"OLMo PAD í† í°: EOS í† í° ì‚¬ìš© ({tokenizer.eos_token})")
            
            # BOS í† í° ì„¤ì • ê°œì„ 
            if tokenizer.bos_token is None:
                tokenizer.bos_token = tokenizer.eos_token
                logger.info(f"OLMo BOS í† í°: EOS í† í° ì‚¬ìš© ({tokenizer.eos_token})")
            
            # í† í¬ë‚˜ì´ì € íŒ¨ë”© ë°©í–¥ ì„¤ì •
            tokenizer.padding_side = 'left'
            logger.info("OLMo í† í¬ë‚˜ì´ì €: left padding ì„¤ì •")

            # OLMo vocab size í™•ì¸
            if hasattr(tokenizer, 'vocab_size') and tokenizer.vocab_size != 50304:
                logger.warning(f"OLMo í† í¬ë‚˜ì´ì € vocab size ë¶ˆì¼ì¹˜: {tokenizer.vocab_size} != 50304")

            logger.info(f"OLMo í† í¬ë‚˜ì´ì € ì„¤ì • ì™„ë£Œ - BOS: {tokenizer.bos_token}, EOS: {tokenizer.eos_token}, PAD: {tokenizer.pad_token}")
            logger.info(f"OLMo í† í¬ë‚˜ì´ì € ìƒì„¸ ì •ë³´ - í´ë˜ìŠ¤: {tokenizer.__class__.__name__}, vocab_size: {len(tokenizer)}")
            logger.info(f"OLMo í† í¬ë‚˜ì´ì € ID - BOS: {tokenizer.bos_token_id}, EOS: {tokenizer.eos_token_id}, PAD: {tokenizer.pad_token_id}")
            
            # ê°„ë‹¨í•œ í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸
            test_text = "Hello, this is a test."
            test_tokens = tokenizer.encode(test_text)
            test_decoded = tokenizer.decode(test_tokens)
            logger.info(f"OLMo í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸ - ì›ë³¸: '{test_text}' -> ë””ì½”ë”©: '{test_decoded}'")
            logger.info(f"OLMo í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸ - í† í° IDs: {test_tokens}")
            
            # ë¬¸ì œê°€ ìˆëŠ” í† í°ë“¤ ê°œë³„ í…ŒìŠ¤íŠ¸
            problem_tokens = [88270, 77081, 22301, 73971]
            for token_id in problem_tokens:
                try:
                    decoded_token = tokenizer.decode([token_id])
                    logger.info(f"OLMo ë¬¸ì œ í† í° {token_id} -> '{decoded_token}'")
                except Exception as e:
                    logger.error(f"OLMo í† í° {token_id} ë””ì½”ë”© ì‹¤íŒ¨: {e}")

        # === TOKENIZER VERIFICATION ===
        tokenizer_status = check_tow_tokens_for_eval(
            tokenizer=tokenizer,
            model_path=tokenizer_load_path,
            model_name=config.name,
            logger=logger
        )

        if not tokenizer_status.is_valid:
            logger.warning(f"âš ï¸ ToW tokens not properly configured for {config.name}")
            for issue in tokenizer_status.issues:
                logger.warning(f"   - {issue}")
        # ===============================

        logger.info(f"Loading model {config.model_id}...")
        quantization_config_bnb = None
        if config.use_quantization:
            quantization_config_bnb = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=config.torch_dtype,
                bnb_4bit_use_double_quant=True,
            )

        model = AutoModelForCausalLM.from_pretrained(
            config.model_id,
            torch_dtype=config.torch_dtype,
            quantization_config=quantization_config_bnb,
            device_map=DEVICE,
            trust_remote_code=True,
            cache_dir=CACHE_DIR
        )

        if config.adapter_path:
            # LoRA ì–´ëŒ‘í„°ê°€ ìˆëŠ” ê²½ìš°, ë¨¼ì € LoRAì˜ ì‹¤ì œ vocab sizeë¥¼ í™•ì¸
            absolute_adapter_path = os.path.abspath(config.adapter_path)
            logger.info(f"LoRA adapter specified. Loading adapter from: {absolute_adapter_path}")
            
            if not os.path.isdir(absolute_adapter_path):
                logger.error(f"Adapter path does not exist or is not a directory: {absolute_adapter_path}")
                raise FileNotFoundError(f"Adapter path not found: {absolute_adapter_path}")
            
            # LoRA ì–´ëŒ‘í„°ì˜ ì‹¤ì œ vocab size í™•ì¸
            try:
                import glob
                pytorch_files = glob.glob(os.path.join(absolute_adapter_path, "*.bin")) + \
                            glob.glob(os.path.join(absolute_adapter_path, "*.safetensors"))
                
                target_vocab_size = None
                if pytorch_files:
                    if pytorch_files[0].endswith('.safetensors'):
                        from safetensors import safe_open
                        with safe_open(pytorch_files[0], framework="pt") as f:
                            for key in f.keys():
                                if 'embed_tokens.weight' in key or 'lm_head.weight' in key:
                                    target_vocab_size = f.get_tensor(key).shape[0]
                                    break
                    else:
                        checkpoint = torch.load(pytorch_files[0], map_location='cpu')
                        for key, tensor in checkpoint.items():
                            if 'embed_tokens.weight' in key or 'lm_head.weight' in key:
                                target_vocab_size = tensor.shape[0]
                                break
                
                # OLMo ëª¨ë¸ì€ LoRAì—ì„œë„ ì„ë² ë”© ë¦¬ì‚¬ì´ì¦ˆ ìƒëµ
                if "olmo" in config.name.lower():
                    current_vocab_size = model.get_input_embeddings().weight.shape[0]
                    logger.info(f"OLMo LoRA: í˜„ì¬ ì„ë² ë”© í¬ê¸° {current_vocab_size}, íƒ€ê²Ÿ í¬ê¸° {target_vocab_size}")
                    logger.warning("OLMo LoRA: ì„ë² ë”© ë¦¬ì‚¬ì´ì¦ˆ ìƒëµ (ëª¨ë¸ ë¬´ê²°ì„± ë³´í˜¸)")
                else:
                    # ë‹¤ë¥¸ ëª¨ë¸ë“¤ì€ ê¸°ì¡´ ë¡œì§ ì‚¬ìš©
                    if target_vocab_size:
                        current_vocab_size = model.get_input_embeddings().weight.shape[0]
                        if current_vocab_size != target_vocab_size:
                            logger.info(f"Resizing model from {current_vocab_size} to {target_vocab_size} for LoRA compatibility")
                            model.resize_token_embeddings(target_vocab_size)
                    else:
                        # fallback: tokenizer ê¸¸ì´ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
                        if len(tokenizer) != model.get_input_embeddings().weight.shape[0]:
                            model.resize_token_embeddings(len(tokenizer))
                        
            except Exception as e:
                logger.warning(f"Could not determine LoRA vocab size: {e}. Using tokenizer length.")
                # OLMo ëª¨ë¸ì€ ì˜ˆì™¸ ìƒí™©ì—ì„œë„ ë¦¬ì‚¬ì´ì¦ˆ ìƒëµ
                if "olmo" in config.name.lower():
                    logger.warning("OLMo ëª¨ë¸: ì˜ˆì™¸ ìƒí™©ì—ì„œë„ ì„ë² ë”© ë¦¬ì‚¬ì´ì¦ˆ ìƒëµ")
                else:
                    if len(tokenizer) != model.get_input_embeddings().weight.shape[0]:
                        model.resize_token_embeddings(len(tokenizer))
            
            try:
                model = PeftModel.from_pretrained(model, absolute_adapter_path)
                logger.info("Successfully loaded LoRA adapter.")
            except Exception as e:
                logger.error(f"Failed to load LoRA adapter from {absolute_adapter_path}: {e}")
                raise e
        else:
            # ëª¨ë¸-í† í¬ë‚˜ì´ì € í˜¸í™˜ì„± í™•ì¸ ë° ì¡°ì •
            model_embed_size = model.get_input_embeddings().weight.shape[0]
            tokenizer_vocab_size = len(tokenizer)
            
            if "olmo" in config.name.lower():
                logger.info(f"OLMo ëª¨ë¸ ì„ë² ë”© í¬ê¸°: {model_embed_size}")
                logger.info(f"OLMo í† í¬ë‚˜ì´ì € vocab í¬ê¸°: {tokenizer_vocab_size}")
                
                if model_embed_size != tokenizer_vocab_size:
                    logger.error(f"âŒ OLMo í¬ê¸° ë¶ˆì¼ì¹˜ ë°œê²¬! ëª¨ë¸: {model_embed_size}, í† í¬ë‚˜ì´ì €: {tokenizer_vocab_size}")
                    logger.info("ğŸ”§ OLMo í† í° ì„ë² ë”© í¬ê¸° ì¡°ì • ì¤‘... (ì´ê²ƒì´ corrupted outputì˜ ì£¼ìš” ì›ì¸ì¼ ê°€ëŠ¥ì„± ë†’ìŒ)")
                    model.resize_token_embeddings(len(tokenizer))
                    logger.info("âœ… OLMo í† í° ì„ë² ë”© í¬ê¸° ì¡°ì • ì™„ë£Œ")
                else:
                    logger.info("âœ… OLMo ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € í¬ê¸° ì¼ì¹˜")
            else:
                # ë‹¤ë¥¸ ëª¨ë¸ë“¤ì€ ê¸°ì¡´ ë¡œì§ ì‚¬ìš©
                if model_embed_size != tokenizer_vocab_size:
                    logger.info(f"Resizing model token embeddings from {model_embed_size} to {tokenizer_vocab_size}")
                    model.resize_token_embeddings(len(tokenizer))
            logger.info("No LoRA adapter path specified. Using the base model directly.")

        model.eval()
        logger.info("Model and tokenizer loaded successfully.")
        
        # OLMo ëª¨ë¸ ìƒì„¸ ì •ë³´ í™•ì¸
        if "olmo" in config.name.lower():
            model_embed_size = model.get_input_embeddings().weight.shape[0]
            tokenizer_vocab_size = len(tokenizer)
            logger.info(f"OLMo ëª¨ë¸ ì„ë² ë”© í¬ê¸°: {model_embed_size}")
            logger.info(f"OLMo í† í¬ë‚˜ì´ì € vocab í¬ê¸°: {tokenizer_vocab_size}")
            
            if model_embed_size != tokenizer_vocab_size:
                logger.error(f"âŒ OLMo í¬ê¸° ë¶ˆì¼ì¹˜: ëª¨ë¸ {model_embed_size} vs í† í¬ë‚˜ì´ì € {tokenizer_vocab_size}")
            else:
                logger.info("âœ… OLMo ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € í¬ê¸° ì¼ì¹˜")
                
            # ëª¨ë¸ ì„¤ì • ì •ë³´
            logger.info(f"OLMo ëª¨ë¸ ì„¤ì •: {model.config}")
            logger.info(f"OLMo ëª¨ë¸ dtype: {model.dtype}")
            logger.info(f"OLMo ëª¨ë¸ device: {next(model.parameters()).device}")

        # Gemma ëª¨ë¸ì—ì„œë§Œ ì»´íŒŒì¼ ë¹„í™œì„±í™”
        if "gemma" in config.name.lower():
            torch._dynamo.config.disable = True
            logger.info("Disabled torch compilation for Gemma model")

        # OLMo ëª¨ë¸ ì „ìš© ì„¤ì •
        if "olmo" in config.name.lower():
            torch._dynamo.config.disable = True
            logger.info("OLMo ëª¨ë¸ ê°ì§€: torch compilation ë¹„í™œì„±í™”")

            # OLMo ëª¨ë¸ì˜ dtypeì´ bfloat16ì¸ì§€ í™•ì¸ (ê¶Œì¥ì‚¬í•­)
            if model.dtype != torch.bfloat16:
                logger.warning(f"OLMo ëª¨ë¸ ê¶Œì¥ì‚¬í•­: í˜„ì¬ dtype {model.dtype}, bfloat16 ê¶Œì¥")

            logger.info("OLMo ì „ìš© ëª¨ë¸ ì„¤ì • ì™„ë£Œ")
            
        # --- Evaluate on both datasets ---
        all_results = {}
        all_failure_cases = {}  # Store failure cases for both datasets
        
        datasets = [
            ("ARC", arc_data, "arc"),
            ("Ko-ARC", ko_arc_data, "ko-arc")
        ]
        
        for dataset_name, dataset, dataset_type in datasets:
            logger.info(f"Starting evaluation on {dataset_name} dataset...")
            
            if dataset_type == "arc":
                examples_to_use = ARC_5SHOT_EXAMPLES
            else:  # "ko-arc"
                examples_to_use = KO_ARC_5SHOT_EXAMPLES

            # OLMo ëª¨ë¸ì˜ ê²½ìš° BOS í† í° ì¶”ê°€ ë¹„í™œì„±í™” (under-trained tokens ë¬¸ì œ í•´ê²°)
            is_olmo_model = "olmo" in config.name.lower()
            add_bos_for_olmo = False  # OLMoëŠ” BOS í† í° ì¶”ê°€í•˜ì§€ ì•ŠìŒ
            if is_olmo_model:
                logger.info("OLMo ëª¨ë¸ ê°ì§€: BOS í† í° ì¶”ê°€ ë¹„í™œì„±í™” (under-trained tokens ë¬¸ì œ í•´ê²°)")

            correct_predictions = 0
            total_predictions = 0
            errors_or_skipped = 0
            results_details = []
            failure_cases = []  # Store failure cases for this dataset
            
            # Batch processing loop with tqdm logging redirect
            num_batches = (len(dataset) + BATCH_SIZE - 1) // BATCH_SIZE
            
            with logging_redirect_tqdm():
                pbar = tqdm(range(num_batches), 
                           desc=f"Evaluating {config.name} on {dataset_name} (3-shot, errors: 0)",
                           ncols=100,  # ê³ ì • ë„ˆë¹„
                           unit="batch",
                           leave=True,
                           dynamic_ncols=False,
                           file=sys.stdout,
                           position=0)
                
                for i in pbar:
                    batch_start = i * BATCH_SIZE
                    batch_end = batch_start + BATCH_SIZE
                    batch = dataset[batch_start:batch_end]
                    
                    prompts = []
                    ground_truths = []
                    valid_items_in_batch = []

                    for item in batch:
                        ground_truth = get_ground_truth(item)
                        if ground_truth is None:
                            errors_or_skipped += 1
                            # Log skipped item if needed
                            continue

                        prompt = create_3shot_prompt(item, examples_to_use, dataset_type, 
                                                    add_bos_token=add_bos_for_olmo, 
                                                    bos_token=tokenizer.bos_token if add_bos_for_olmo else "")
                        prompts.append(prompt)
                        ground_truths.append(ground_truth)
                        valid_items_in_batch.append(item)

                    if not prompts:
                        continue

                    try:
                        inputs = tokenizer(prompts, return_tensors="pt", padding=True, truncation=True, max_length=4096).to(DEVICE)
                        
                        with torch.inference_mode():
                            # OLMo ëª¨ë¸ ì „ìš© ìƒì„± íŒŒë¼ë¯¸í„°
                            if "olmo" in config.name.lower():
                                # OLMo ë¬¸ì œ í† í°ë“¤ ì°¨ë‹¨
                                bad_words = ["setattr", "ForcedSuppressWarnings", "RI", "kommsetattr", "despre", "empire", "FLICT", "PrivateKey", "TestCase"]
                                bad_words_ids = []
                                for word in bad_words:
                                    try:
                                        word_ids = tokenizer.encode(word, add_special_tokens=False)
                                        if len(word_ids) > 0:
                                            bad_words_ids.append(word_ids)
                                    except:
                                        continue
                                
                                generation_kwargs = {
                                    "max_new_tokens": 512,      # ì›ë˜ í† í° ìˆ˜ ìœ ì§€
                                    "do_sample": True,          # ìƒ˜í”Œë§ í™œì„±í™”
                                    "temperature": 0.7,         # ì˜¨ë„ ì„¤ì •
                                    "top_p": 0.9,              # Top-p ìƒ˜í”Œë§
                                    "repetition_penalty": 1.1, # ë°˜ë³µ ë°©ì§€
                                    "pad_token_id": tokenizer.pad_token_id,
                                    "eos_token_id": tokenizer.eos_token_id,
                                    "use_cache": True,
                                }
                                
                                # Bad words í•„í„° ì¶”ê°€
                                if bad_words_ids:
                                    generation_kwargs["bad_words_ids"] = bad_words_ids
                                
                                logger.debug("OLMo ë°°ì¹˜: under-trained tokens ë¬¸ì œ í•´ê²° íŒŒë¼ë¯¸í„° ì ìš©")
                            else:
                                # ë‹¤ë¥¸ ëª¨ë¸ë“¤ì€ ê¸°ì¡´ íŒŒë¼ë¯¸í„° ìœ ì§€
                                generation_kwargs = {
                                    "max_new_tokens": 512,
                                    "pad_token_id": tokenizer.pad_token_id,
                                    "eos_token_id": tokenizer.eos_token_id,
                                    "do_sample": False,
                                }

                            outputs = model.generate(
                                **inputs,
                                **generation_kwargs
                            )
                        
                        generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)
                        
                        input_lengths = inputs['input_ids'].shape[1]
                        # The generated text includes the prompt, so we need to remove it.
                        # Be careful with batch_decode, it might handle prompts differently.
                        # A safer way is to decode only the generated part.
                        output_only_tokens = outputs[:, input_lengths:]
                        decoded_outputs = tokenizer.batch_decode(output_only_tokens, skip_special_tokens=True)


                        for j, (item, ground_truth, gen_text) in enumerate(zip(valid_items_in_batch, ground_truths, decoded_outputs)):
                            generated_text_log = gen_text.strip()
                            model_answer_log = extract_answer_robust(generated_text_log)
                            is_correct_log = False

                            # OLMo ë””ë²„ê¹…: ì²« ë²ˆì§¸ í•­ëª©ë§Œ ë¡œê·¸ ì¶œë ¥
                            if "olmo" in config.name.lower() and j == 0:
                                logger.info(f"OLMo ë°°ì¹˜ ì²« ë²ˆì§¸ í•­ëª© ë””ë²„ê¹…:")
                                logger.info(f"  Generated text: '{generated_text_log[:200]}...'")
                                logger.info(f"  Extracted answer: '{model_answer_log}'")
                                logger.info(f"  Ground truth: '{ground_truth}'")

                            if model_answer_log:
                                total_predictions += 1
                                if model_answer_log == ground_truth:
                                    correct_predictions += 1
                                    is_correct_log = True
                                else:
                                    # This is a wrong answer - add to failure cases
                                    failure_cases.append({
                                        "index": batch_start + j,
                                        "id": item.get("id", ""),
                                        "dataset": dataset_name,
                                        "question": item.get("question", ""),
                                        "options": {k: v for k, v in item.items() if k in ['A', 'B', 'C', 'D']},
                                        "ground_truth": ground_truth,
                                        "predicted_answer": model_answer_log,
                                        "raw_output": generated_text_log,
                                        "failure_type": "incorrect_answer"
                                    })
                            else:
                                # Batch extraction failed - skip individual retry to save time
                                if j == 0:  # ì²« ë²ˆì§¸ í•­ëª©ë§Œ ë¡œê·¸
                                    logger.warning(f"Batch item {batch_start + j}: Failed to extract answer, skipping individual retry")
                                errors_or_skipped += 1
                                generated_text_log = f"BATCH_EXTRACTION_FAILED: {gen_text.strip()}"
                                failure_cases.append({
                                    "index": batch_start + j,
                                    "id": item.get("id", ""),
                                    "dataset": dataset_name,
                                    "question": item.get("question", ""),
                                    "options": {k: v for k, v in item.items() if k in ['A', 'B', 'C', 'D']},
                                    "ground_truth": ground_truth,
                                    "predicted_answer": -1,
                                    "raw_output": generated_text_log,
                                    "failure_type": "batch_extraction_failed"
                                })
                                model_answer_log = None
                                is_correct_log = False

                            current_item_index = batch_start + j # or find a better way to get original index
                            results_details.append({
                                "index": current_item_index, 
                                "id": item.get("id", ""),
                                "ground_truth": ground_truth, 
                                "model_raw_output": generated_text_log,
                                "predicted_answer": model_answer_log, 
                                "is_correct": is_correct_log
                            })
                            
                            raw_generations_list.append({
                                "dataset": dataset_name,
                                "index": current_item_index, 
                                "id": item.get("id", ""),
                                "ground_truth": ground_truth,
                                "raw_output": generated_text_log, 
                                "extracted_answer": model_answer_log
                            })

                    except Exception as e:
                        logger.error(f"Batch {i}: Inference error: {e}", exc_info=False)
                        # Add all items in this batch to failure cases
                        for j, (item, ground_truth) in enumerate(zip(valid_items_in_batch, ground_truths)):
                            failure_cases.append({
                                "index": batch_start + j,
                                "id": item.get("id", ""),
                                "dataset": dataset_name,
                                "question": item.get("question", ""),
                                "options": {k: v for k, v in item.items() if k in ['A', 'B', 'C', 'D']},
                                "ground_truth": ground_truth,
                                "predicted_answer": -1,
                                "raw_output": f"BATCH_ERROR: {str(e)}",
                                "failure_type": "batch_inference_error"
                            })
                        errors_or_skipped += len(prompts)
                
                    # Update progress bar with current error count
                    pbar.set_description(f"Evaluating {config.name} on {dataset_name} (3-shot, errors: {errors_or_skipped})")

            
            # Calculate accuracy
            accuracy_standard = (correct_predictions / total_predictions * 100) if total_predictions > 0 else 0
            accuracy_strict = (correct_predictions / len(dataset) * 100) if len(dataset) > 0 else 0

            logger.info(f"--- 3-shot {dataset_name} Results for {config.name} ---")
            logger.info(f"Test Items: {len(dataset)}")
            logger.info(f"Valid Predictions: {total_predictions}")
            logger.info(f"Correct Predictions: {correct_predictions}")
            logger.info(f"Failure Cases: {len(failure_cases)}")
            logger.info(f"Accuracy Standard: {accuracy_standard:.2f}%")
            logger.info(f"Accuracy Strict: {accuracy_strict:.2f}%")
            
            all_results[dataset_name] = {
                "test_items": len(dataset),
                "valid_predictions": total_predictions,
                "correct_predictions": correct_predictions,
                "failure_cases_count": len(failure_cases),
                "accuracy_standard": accuracy_standard,
                "accuracy_strict": accuracy_strict,
                "details": results_details
            }
            
            # Store failure cases for this dataset
            all_failure_cases[dataset_name] = failure_cases

        # --- Save Results ---
        final_summary = {
            "model_config": {k: str(v) for k, v in config.__dict__.items()},
            "evaluation_type": "3-shot ARC Challenge",
            "datasets": all_results
        }
        
        with open(results_filepath, 'w', encoding='utf-8') as f:
            json.dump(final_summary, f, indent=2, ensure_ascii=False)
        with open(raw_gen_filepath, 'w', encoding='utf-8') as f:
            json.dump(raw_generations_list, f, indent=2, ensure_ascii=False)
        
        # Save failure cases for both datasets combined
        all_failure_cases_combined = []
        for dataset_name, cases in all_failure_cases.items():
            all_failure_cases_combined.extend(cases)
        
        save_failure_cases(all_failure_cases_combined, config.name, model_specific_output_dir)

        return all_results

    except Exception as e:
        logger.exception(f"A critical error occurred during evaluation for {config.name}: {e}")
        return None
    finally:
        del model, tokenizer
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        if 'file_handler' in locals() and file_handler in root_logger.handlers:
             root_logger.removeHandler(file_handler)
             file_handler.close()

def main():
    # Load datasets
    arc_data = load_arc_data(ARC_DATASET_PATH)
    ko_arc_data = load_arc_data(KO_ARC_DATASET_PATH)
    
    if not arc_data or not ko_arc_data:
        logger.error("Failed to load one or both datasets")
        return

    os.makedirs(BASE_OUTPUT_DIR, exist_ok=True)
    
    # Store summary results for all models
    summary_results = {}

    for config in MODEL_CONFIGS:
        model_specific_output_dir = os.path.join(BASE_OUTPUT_DIR, config.name)
        os.makedirs(model_specific_output_dir, exist_ok=True)
        
        results = evaluate_single_model(config, arc_data, ko_arc_data, model_specific_output_dir)
        
        if results:
            summary_results[config.name] = {
                "model_id": config.model_id,
                "adapter_path": config.adapter_path,
                "ARC_accuracy_standard": results["ARC"]["accuracy_standard"],
                "ARC_accuracy_strict": results["ARC"]["accuracy_strict"],
                "ARC_failure_cases": results["ARC"]["failure_cases_count"],
                "Ko-ARC_accuracy_standard": results["Ko-ARC"]["accuracy_standard"],
                "Ko-ARC_accuracy_strict": results["Ko-ARC"]["accuracy_strict"],
                "Ko-ARC_failure_cases": results["Ko-ARC"]["failure_cases_count"]
            }

    # Save summary results
    summary_filepath = os.path.join(BASE_OUTPUT_DIR, "summary.json")
    with open(summary_filepath, 'w', encoding='utf-8') as f:
        json.dump(summary_results, f, indent=2, ensure_ascii=False)
    
    logger.info(f"Summary results saved to: {summary_filepath}")
    
    # Print summary table
    print("\n" + "="*100)
    print("EVALUATION SUMMARY")
    print("="*100)
    print(f"{'Model Name':<30} {'ARC Acc (%)':<15} {'ARC Fails':<12} {'Ko-ARC Acc (%)':<17} {'Ko-ARC Fails':<15}")
    print("-"*100)
    
    for model_name, results in summary_results.items():
        arc_acc = results["ARC_accuracy_standard"]
        ko_arc_acc = results["Ko-ARC_accuracy_standard"]
        arc_fails = results["ARC_failure_cases"]
        ko_arc_fails = results["Ko-ARC_failure_cases"]
        print(f"{model_name:<30} {arc_acc:<15.2f} {arc_fails:<12} {ko_arc_acc:<17.2f} {ko_arc_fails:<15}")
    
    print("="*100)

if __name__ == "__main__":
    main()