import os
import torch
import warnings
import transformers
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import logging

# ê²½ê³  ë° ë¡œê¹… ì„¤ì •
os.environ['TRANSFORMERS_VERBOSITY'] = 'error'
transformers.logging.set_verbosity_error()
warnings.filterwarnings("ignore")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ì„¤ì •
MODEL_PATH = "/scratch/jsong132/Increase_MLLM_Ability/Base_Models/olmo-2-0425-1b"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
CACHE_DIR = "./cache" if not os.path.exists("/scratch/jsong132/.cache/huggingface") else "/scratch/jsong132/.cache/huggingface"

def load_olmo_model():
    """OLMo ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ"""
    logger.info(f"Loading OLMo model from: {MODEL_PATH}")
    
    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, cache_dir=CACHE_DIR, padding_side='left')
    
    # OLMo í† í¬ë‚˜ì´ì € ì„¤ì •
    if tokenizer.pad_token is None:
        if tokenizer.unk_token:
            tokenizer.pad_token = tokenizer.unk_token
            logger.info(f"PAD token set to UNK: {tokenizer.unk_token}")
        else:
            tokenizer.pad_token = tokenizer.eos_token
            logger.info(f"PAD token set to EOS: {tokenizer.eos_token}")
    
    if tokenizer.bos_token is None:
        tokenizer.bos_token = tokenizer.eos_token
        logger.info(f"BOS token set to EOS: {tokenizer.eos_token}")
    
    # í† í¬ë‚˜ì´ì € ì •ë³´ ì¶œë ¥
    logger.info(f"Tokenizer class: {tokenizer.__class__.__name__}")
    logger.info(f"Vocab size: {len(tokenizer)}")
    logger.info(f"PAD: {tokenizer.pad_token_id}, EOS: {tokenizer.eos_token_id}, BOS: {tokenizer.bos_token_id}")
    
    # ëª¨ë¸ ë¡œë“œ
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_PATH,
        torch_dtype=torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16,
        device_map=DEVICE,
        trust_remote_code=True,
        cache_dir=CACHE_DIR
    )
    
    model.eval()
    logger.info("Model loaded successfully")
    
    # ëª¨ë¸ ì •ë³´ ì¶œë ¥
    model_embed_size = model.get_input_embeddings().weight.shape[0]
    logger.info(f"Model embedding size: {model_embed_size}")
    logger.info(f"Model dtype: {model.dtype}")
    logger.info(f"Model device: {next(model.parameters()).device}")
    
    return model, tokenizer

def test_simple_generation(model, tokenizer, prompt, test_name):
    """ê°„ë‹¨í•œ ìƒì„± í…ŒìŠ¤íŠ¸"""
    logger.info(f"\n=== {test_name} ===")
    logger.info(f"Input prompt: '{prompt}'")
    
    # í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸
    inputs = tokenizer(prompt, return_tensors="pt").to(DEVICE)
    logger.info(f"Input shape: {inputs['input_ids'].shape}")
    logger.info(f"Input tokens: {inputs['input_ids'][0].tolist()}")
    
    # Bad words í•„í„° (under-trained tokens ì°¨ë‹¨)
    bad_words = ["setattr", "ForcedSuppressWarnings", "RI", "kommsetattr", "despre", "empire", "FLICT", "PrivateKey", "TestCase"]
    bad_words_ids = []
    for word in bad_words:
        try:
            word_ids = tokenizer.encode(word, add_special_tokens=False)
            if len(word_ids) > 0:
                bad_words_ids.append(word_ids)
        except:
            continue
    
    # ìƒì„± íŒŒë¼ë¯¸í„°
    generation_kwargs = {
        "max_new_tokens": 100,
        "do_sample": True,
        "temperature": 0.7,
        "top_p": 0.9,
        "repetition_penalty": 1.1,
        "pad_token_id": tokenizer.pad_token_id,
        "eos_token_id": tokenizer.eos_token_id,
        "use_cache": True,
    }
    
    if bad_words_ids:
        generation_kwargs["bad_words_ids"] = bad_words_ids
        logger.info(f"Bad words filter applied: {len(bad_words_ids)} words")
    
    logger.info(f"Generation parameters: {generation_kwargs}")
    
    # ìƒì„±
    with torch.inference_mode():
        outputs = model.generate(**inputs, **generation_kwargs)
    
    # ê²°ê³¼ ë¶„ì„
    input_length = inputs['input_ids'].shape[1]
    output_only_tokens = outputs[:, input_length:]
    generated_text = tokenizer.decode(output_only_tokens[0], skip_special_tokens=True).strip()
    
    logger.info(f"Output shape: {outputs.shape}")
    logger.info(f"Generated tokens: {output_only_tokens.shape}")
    logger.info(f"Generated token IDs: {output_only_tokens[0].tolist()}")
    logger.info(f"Generated text: '{generated_text}'")
    
    # ê°œë³„ í† í° ë¶„ì„
    logger.info("Individual token analysis:")
    for i, token_id in enumerate(output_only_tokens[0].tolist()):
        try:
            token_text = tokenizer.decode([token_id])
            logger.info(f"  Token {i}: ID={token_id}, Text='{token_text}'")
        except Exception as e:
            logger.error(f"  Token {i}: ID={token_id}, Decode error: {e}")
    
    return generated_text

def interactive_chat(model, tokenizer):
    """ëŒ€í™”í˜• ì±„íŒ… í•¨ìˆ˜"""
    # Bad words í•„í„° (under-trained tokens ì°¨ë‹¨)
    bad_words = ["setattr", "ForcedSuppressWarnings", "RI", "kommsetattr", "despre", "empire", "FLICT", "PrivateKey", "TestCase"]
    bad_words_ids = []
    for word in bad_words:
        try:
            word_ids = tokenizer.encode(word, add_special_tokens=False)
            if len(word_ids) > 0:
                bad_words_ids.append(word_ids)
        except:
            continue
    
    print("\n" + "="*60)
    print("ğŸ¤– OLMo ëŒ€í™”í˜• í…ŒìŠ¤íŠ¸ ì‹œì‘!")
    print("ğŸ’¡ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œ: 'quit', 'exit', 'q')")
    print("ğŸ”§ ë””ë²„ê¹… ì •ë³´: ê° ë‹µë³€ í›„ 'd' ì…ë ¥")
    print("="*60)
    
    while True:
        try:
            # ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
            user_input = input("\nğŸ‘¤ ì‚¬ìš©ì: ").strip()
            
            # ì¢…ë£Œ ì¡°ê±´
            if user_input.lower() in ['quit', 'exit', 'q', 'ì¢…ë£Œ']:
                print("ğŸ‘‹ ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤!")
                break
            
            if not user_input:
                print("â“ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                continue
            
            # í† í¬ë‚˜ì´ì € ì²˜ë¦¬
            inputs = tokenizer(user_input, return_tensors="pt").to(DEVICE)
            
            # ìƒì„± íŒŒë¼ë¯¸í„°
            generation_kwargs = {
                "max_new_tokens": 150,  # ëŒ€í™”ìš©ìœ¼ë¡œ ì ë‹¹í•œ ê¸¸ì´
                "do_sample": True,
                "temperature": 0.7,
                "top_p": 0.9,
                "repetition_penalty": 1.1,
                "pad_token_id": tokenizer.pad_token_id,
                "eos_token_id": tokenizer.eos_token_id,
                "use_cache": True,
            }
            
            if bad_words_ids:
                generation_kwargs["bad_words_ids"] = bad_words_ids
            
            # ìƒì„±
            with torch.inference_mode():
                outputs = model.generate(**inputs, **generation_kwargs)
            
            # ê²°ê³¼ ì¶”ì¶œ
            input_length = inputs['input_ids'].shape[1]
            output_only_tokens = outputs[:, input_length:]
            generated_text = tokenizer.decode(output_only_tokens[0], skip_special_tokens=True).strip()
            
            # ë‹µë³€ ì¶œë ¥
            print(f"ğŸ¤– OLMo: {generated_text}")
            
            # ë””ë²„ê¹… ì •ë³´ ì˜µì…˜
            debug_input = input("\nğŸ”§ ë””ë²„ê¹… ì •ë³´ë¥¼ ë³´ì‹œê² ìŠµë‹ˆê¹Œ? (d/ì—”í„°): ").strip().lower()
            if debug_input in ['d', 'debug', 'ã„·']:
                print(f"ğŸ“Š ìƒì„±ëœ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(generated_text)}")
                print(f"ğŸ”¢ Raw token IDs: {output_only_tokens[0][:15].tolist()}")
                
                print("ğŸ” ê°œë³„ í† í° ë¶„ì„ (ì²« 10ê°œ):")
                for j, token_id in enumerate(output_only_tokens[0][:10].tolist()):
                    try:
                        token_text = tokenizer.decode([token_id])
                        print(f"   Token {j}: ID={token_id} â†’ '{token_text}'")
                    except Exception as e:
                        print(f"   Token {j}: ID={token_id} â†’ ë””ì½”ë”© ì˜¤ë¥˜: {e}")
        
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ Ctrl+Cë¡œ ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤!")
            break
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            continue

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    try:
        # ëª¨ë¸ ë¡œë“œ
        model, tokenizer = load_olmo_model()
        
        # ëŒ€í™”í˜• ì±„íŒ… ì‹œì‘
        interactive_chat(model, tokenizer)
        
    except Exception as e:
        logger.error(f"Critical error: {e}")
        print(f"âŒ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    main()
