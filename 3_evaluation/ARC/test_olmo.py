import os
import torch
import warnings
import transformers
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import logging

# ê²½ê³  ë° ë¡œê¹… ì„¤ì •
os.environ['TRANSFORMERS_VERBOSITY'] = 'error'
transformers.logging.set_verbosity_error()
warnings.filterwarnings("ignore")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ì„¤ì •
MODEL_PATH = "/scratch/jsong132/Increase_MLLM_Ability/Base_Models/olmo-2-0425-1b"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
CACHE_DIR = "./cache" if not os.path.exists("/scratch/jsong132/.cache/huggingface") else "/scratch/jsong132/.cache/huggingface"

def load_olmo_model():
    """OLMo ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ"""
    logger.info(f"Loading OLMo model from: {MODEL_PATH}")
    
    # í† í¬ë‚˜ì´ì € ë¡œë“œ - ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì‹œë„
    logger.info("ğŸ”§ STEP 1: ê¸°ë³¸ í† í¬ë‚˜ì´ì € ë¡œë“œ")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, cache_dir=CACHE_DIR)
    
    # í† í¬ë‚˜ì´ì € ê¸°ë³¸ ì •ë³´ í™•ì¸
    logger.info(f"ğŸ“Š ì›ë³¸ í† í¬ë‚˜ì´ì € ì •ë³´:")
    logger.info(f"  - Class: {tokenizer.__class__.__name__}")
    logger.info(f"  - Vocab size: {len(tokenizer)}")
    logger.info(f"  - PAD: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})")
    logger.info(f"  - EOS: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})")
    logger.info(f"  - BOS: {tokenizer.bos_token} (ID: {tokenizer.bos_token_id})")
    logger.info(f"  - UNK: {tokenizer.unk_token} (ID: {tokenizer.unk_token_id})")
    logger.info(f"  - Padding side: {tokenizer.padding_side}")
    
    # ê¸°ë³¸ í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸
    logger.info("ğŸ§ª STEP 2: ê¸°ë³¸ í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸")
    test_text = "Hello world"
    test_tokens = tokenizer.encode(test_text)
    test_decoded = tokenizer.decode(test_tokens)
    logger.info(f"  Test encode/decode: '{test_text}' -> {test_tokens} -> '{test_decoded}'")
    
    # ë¬¸ì œ í† í°ë“¤ í™•ì¸
    logger.info("ğŸš¨ STEP 3: ë¬¸ì œ í† í°ë“¤ í™•ì¸")
    problem_words = ["setattr", "PrivateKey", "TestCase", "ForcedSuppressWarnings"]
    for word in problem_words:
        try:
            word_tokens = tokenizer.encode(word, add_special_tokens=False)
            word_decoded = tokenizer.decode(word_tokens)
            logger.info(f"  '{word}' -> {word_tokens} -> '{word_decoded}'")
        except Exception as e:
            logger.error(f"  '{word}' -> ERROR: {e}")
    
    # ìµœì†Œí•œì˜ í† í¬ë‚˜ì´ì € ì„¤ì •ë§Œ ì ìš©
    logger.info("âš™ï¸ STEP 4: ìµœì†Œí•œì˜ í† í¬ë‚˜ì´ì € ì„¤ì •")
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        logger.info(f"  PAD token set to EOS: {tokenizer.eos_token}")
    
    # padding_sideëŠ” ê¸°ë³¸ê°’ ìœ ì§€ (right)
    logger.info(f"  Padding side: {tokenizer.padding_side} (ê¸°ë³¸ê°’ ìœ ì§€)")
    
    # ëª¨ë¸ ë¡œë“œ - ì–‘ìí™” ì—†ì´
    logger.info("ğŸ¤– STEP 5: ëª¨ë¸ ë¡œë“œ (ì–‘ìí™” ì—†ìŒ)")
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_PATH,
        torch_dtype=torch.float16,  # bfloat16 ëŒ€ì‹  float16 ì‚¬ìš©
        device_map=DEVICE,
        trust_remote_code=True,
        cache_dir=CACHE_DIR,
        low_cpu_mem_usage=True
    )
    
    model.eval()
    logger.info("âœ… Model loaded successfully")
    
    # ëª¨ë¸-í† í¬ë‚˜ì´ì € í˜¸í™˜ì„± í™•ì¸
    logger.info("ğŸ” STEP 6: ëª¨ë¸-í† í¬ë‚˜ì´ì € í˜¸í™˜ì„± í™•ì¸")
    model_embed_size = model.get_input_embeddings().weight.shape[0]
    tokenizer_vocab_size = len(tokenizer)
    logger.info(f"  Model embedding size: {model_embed_size}")
    logger.info(f"  Tokenizer vocab size: {tokenizer_vocab_size}")
    
    if model_embed_size != tokenizer_vocab_size:
        logger.error(f"âŒ í¬ê¸° ë¶ˆì¼ì¹˜! ëª¨ë¸: {model_embed_size}, í† í¬ë‚˜ì´ì €: {tokenizer_vocab_size}")
        logger.info("ğŸ”§ í† í° ì„ë² ë”© í¬ê¸° ì¡°ì • ì‹œë„...")
        model.resize_token_embeddings(len(tokenizer))
        logger.info("âœ… í† í° ì„ë² ë”© í¬ê¸° ì¡°ì • ì™„ë£Œ")
    else:
        logger.info("âœ… ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € í¬ê¸° ì¼ì¹˜")
    
    logger.info(f"  Model dtype: {model.dtype}")
    logger.info(f"  Model device: {next(model.parameters()).device}")
    
    return model, tokenizer

def test_simple_generation(model, tokenizer, prompt, test_name):
    """ê°„ë‹¨í•œ ìƒì„± í…ŒìŠ¤íŠ¸"""
    logger.info(f"\n=== {test_name} ===")
    logger.info(f"Input prompt: '{prompt}'")
    
    # í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸
    inputs = tokenizer(prompt, return_tensors="pt").to(DEVICE)
    logger.info(f"Input shape: {inputs['input_ids'].shape}")
    logger.info(f"Input tokens: {inputs['input_ids'][0].tolist()}")
    
    # Bad words í•„í„° (under-trained tokens ì°¨ë‹¨)
    bad_words = ["setattr", "ForcedSuppressWarnings", "RI", "kommsetattr", "despre", "empire", "FLICT", "PrivateKey", "TestCase"]
    bad_words_ids = []
    for word in bad_words:
        try:
            word_ids = tokenizer.encode(word, add_special_tokens=False)
            if len(word_ids) > 0:
                bad_words_ids.append(word_ids)
        except:
            continue
    
    # ìƒì„± íŒŒë¼ë¯¸í„°
    generation_kwargs = {
        "max_new_tokens": 100,
        "do_sample": True,
        "temperature": 0.7,
        "top_p": 0.9,
        "repetition_penalty": 1.1,
        "pad_token_id": tokenizer.pad_token_id,
        "eos_token_id": tokenizer.eos_token_id,
        "use_cache": True,
    }
    
    if bad_words_ids:
        generation_kwargs["bad_words_ids"] = bad_words_ids
        logger.info(f"Bad words filter applied: {len(bad_words_ids)} words")
    
    logger.info(f"Generation parameters: {generation_kwargs}")
    
    # ìƒì„±
    with torch.inference_mode():
        outputs = model.generate(**inputs, **generation_kwargs)
    
    # ê²°ê³¼ ë¶„ì„
    input_length = inputs['input_ids'].shape[1]
    output_only_tokens = outputs[:, input_length:]
    generated_text = tokenizer.decode(output_only_tokens[0], skip_special_tokens=True).strip()
    
    logger.info(f"Output shape: {outputs.shape}")
    logger.info(f"Generated tokens: {output_only_tokens.shape}")
    logger.info(f"Generated token IDs: {output_only_tokens[0].tolist()}")
    logger.info(f"Generated text: '{generated_text}'")
    
    # ê°œë³„ í† í° ë¶„ì„
    logger.info("Individual token analysis:")
    for i, token_id in enumerate(output_only_tokens[0].tolist()):
        try:
            token_text = tokenizer.decode([token_id])
            logger.info(f"  Token {i}: ID={token_id}, Text='{token_text}'")
        except Exception as e:
            logger.error(f"  Token {i}: ID={token_id}, Decode error: {e}")
    
    return generated_text

def interactive_chat(model, tokenizer):
    """ëŒ€í™”í˜• ì±„íŒ… í•¨ìˆ˜"""
    print("\n" + "="*60)
    print("ğŸ¤– OLMo ì§„ë‹¨ ëª¨ë“œ - ë‹¨ê³„ë³„ í…ŒìŠ¤íŠ¸")
    print("ğŸ’¡ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œ: 'quit', 'exit', 'q')")
    print("ğŸ”§ ê° ë‹¨ê³„ë³„ë¡œ ë‹¤ë¥¸ ì„¤ì •ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤")
    print("="*60)
    
    test_configs = [
        {
            "name": "1ï¸âƒ£ ìµœì†Œ ì„¤ì • (Greedy)",
            "params": {
                "max_new_tokens": 20,
                "do_sample": False,
                "pad_token_id": tokenizer.pad_token_id,
                "eos_token_id": tokenizer.eos_token_id,
            }
        },
        {
            "name": "2ï¸âƒ£ ê¸°ë³¸ ìƒ˜í”Œë§",
            "params": {
                "max_new_tokens": 30,
                "do_sample": True,
                "temperature": 1.0,
                "pad_token_id": tokenizer.pad_token_id,
                "eos_token_id": tokenizer.eos_token_id,
            }
        },
        {
            "name": "3ï¸âƒ£ ì•ˆì „í•œ ìƒ˜í”Œë§",
            "params": {
                "max_new_tokens": 50,
                "do_sample": True,
                "temperature": 0.8,
                "top_p": 0.95,
                "repetition_penalty": 1.05,
                "pad_token_id": tokenizer.pad_token_id,
                "eos_token_id": tokenizer.eos_token_id,
            }
        }
    ]
    
    current_config = 0
    
    while True:
        try:
            # ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
            user_input = input(f"\nğŸ‘¤ ì‚¬ìš©ì [{test_configs[current_config]['name']}]: ").strip()
            
            # ì¢…ë£Œ ì¡°ê±´
            if user_input.lower() in ['quit', 'exit', 'q', 'ì¢…ë£Œ']:
                print("ğŸ‘‹ ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤!")
                break
            
            # ì„¤ì • ë³€ê²½
            if user_input.lower() in ['next', 'n', 'ë‹¤ìŒ']:
                current_config = (current_config + 1) % len(test_configs)
                print(f"ğŸ”„ ì„¤ì • ë³€ê²½: {test_configs[current_config]['name']}")
                continue
            
            if user_input.lower() in ['prev', 'p', 'ì´ì „']:
                current_config = (current_config - 1) % len(test_configs)
                print(f"ğŸ”„ ì„¤ì • ë³€ê²½: {test_configs[current_config]['name']}")
                continue
            
            if not user_input:
                print("â“ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (next/prevë¡œ ì„¤ì • ë³€ê²½)")
                continue
            
            print(f"\nğŸ§ª í…ŒìŠ¤íŠ¸ ì¤‘: {test_configs[current_config]['name']}")
            
            # í† í¬ë‚˜ì´ì € ì²˜ë¦¬
            inputs = tokenizer(user_input, return_tensors="pt").to(DEVICE)
            print(f"ğŸ“¥ ì…ë ¥ í† í° ìˆ˜: {inputs['input_ids'].shape[1]}")
            print(f"ğŸ“¥ ì…ë ¥ í† í° IDs: {inputs['input_ids'][0].tolist()}")
            
            # í˜„ì¬ ì„¤ì •ìœ¼ë¡œ ìƒì„±
            generation_kwargs = test_configs[current_config]["params"].copy()
            print(f"âš™ï¸ ìƒì„± ì„¤ì •: {generation_kwargs}")
            
            # ìƒì„±
            with torch.inference_mode():
                outputs = model.generate(**inputs, **generation_kwargs)
            
            # ê²°ê³¼ ì¶”ì¶œ
            input_length = inputs['input_ids'].shape[1]
            output_only_tokens = outputs[:, input_length:]
            
            print(f"ğŸ“¤ ì¶œë ¥ í† í° ìˆ˜: {output_only_tokens.shape[1]}")
            print(f"ğŸ“¤ ì¶œë ¥ í† í° IDs: {output_only_tokens[0].tolist()}")
            
            # ê° í† í°ì„ ê°œë³„ì ìœ¼ë¡œ ë””ì½”ë”©í•´ì„œ í™•ì¸
            print("ğŸ” ê°œë³„ í† í° ë¶„ì„:")
            for j, token_id in enumerate(output_only_tokens[0].tolist()):
                try:
                    token_text = tokenizer.decode([token_id])
                    print(f"   Token {j}: ID={token_id} â†’ '{token_text}'")
                except Exception as e:
                    print(f"   Token {j}: ID={token_id} â†’ ë””ì½”ë”© ì˜¤ë¥˜: {e}")
            
            # ì „ì²´ í…ìŠ¤íŠ¸ ë””ì½”ë”©
            generated_text = tokenizer.decode(output_only_tokens[0], skip_special_tokens=True).strip()
            print(f"\nğŸ¤– OLMo ì „ì²´ ë‹µë³€: '{generated_text}'")
            
            # ë‹¤ìŒ ì„¤ì •ìœ¼ë¡œ ìë™ ë³€ê²½
            if len(generated_text) > 100 or "setattr" in generated_text.lower():
                print("âš ï¸ ë¬¸ì œê°€ ìˆëŠ” ì¶œë ¥ ê°ì§€ë¨")
            else:
                print("âœ… ì •ìƒì ì¸ ì¶œë ¥ìœ¼ë¡œ ë³´ì„")
        
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ Ctrl+Cë¡œ ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤!")
            break
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
            continue

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    try:
        # ëª¨ë¸ ë¡œë“œ
        model, tokenizer = load_olmo_model()
        
        # ëŒ€í™”í˜• ì±„íŒ… ì‹œì‘
        interactive_chat(model, tokenizer)
        
    except Exception as e:
        logger.error(f"Critical error: {e}")
        print(f"âŒ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    main()
