import os
import torch
import warnings
import transformers
from transformers import AutoTokenizer, AutoModelForCausalLM
import logging

# ê²½ê³  ë° ë¡œê¹… ì„¤ì •
os.environ['TRANSFORMERS_VERBOSITY'] = 'error'
transformers.logging.set_verbosity_error()
warnings.filterwarnings("ignore")

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ì„¤ì •
MODEL_PATH = "/scratch/jsong132/Increase_MLLM_Ability/Base_Models/olmo-2-0425-1b"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
CACHE_DIR = "./cache" if not os.path.exists("/scratch/jsong132/.cache/huggingface") else "/scratch/jsong132/.cache/huggingface"

def load_olmo_model_fixed():
    """ìˆ˜ì •ëœ OLMo ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜"""
    logger.info(f"Loading OLMo model from: {MODEL_PATH}")
    
    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, cache_dir=CACHE_DIR)
    
    # PAD í† í° ì„¤ì •
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # ëª¨ë¸ ë¡œë“œ
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_PATH,
        torch_dtype=torch.float16,
        device_map=DEVICE,
        trust_remote_code=True,
        cache_dir=CACHE_DIR,
        low_cpu_mem_usage=True
    )
    
    # ì„ë² ë”© í¬ê¸° ë™ê¸°í™” - í† í¬ë‚˜ì´ì € í¬ê¸°ì— ë§ì¶¤
    model.resize_token_embeddings(len(tokenizer))
    model.eval()
    
    logger.info(f"âœ… Model loaded with embedding size: {model.get_input_embeddings().weight.shape[0]}")
    
    return model, tokenizer

def create_token_filter(tokenizer, max_vocab_size=50000):
    """
    ì•ˆì „í•œ í† í°ë§Œ ì‚¬ìš©í•˜ë„ë¡ í•„í„° ìƒì„±
    - ë‚®ì€ IDì˜ ì˜ í›ˆë ¨ëœ í† í°ë§Œ í—ˆìš©
    - ë¬¸ì œê°€ ë˜ëŠ” ê³ ID í† í° ì°¨ë‹¨
    """
    # ì°¨ë‹¨í•  íŠ¹ì • ë‹¨ì–´ë“¤
    blocked_words = [
        "setattr", "PrivateKey", "TestCase", "ForcedSuppressWarnings",
        "komm", "aight", "Ä±lÄ±", "dernier", "cplusplus", "yscale",
        "GLOSS", "VERTISE", "obao", "iyor", "Mey"
    ]
    
    # ì°¨ë‹¨í•  í† í° ID ìˆ˜ì§‘
    blocked_ids = set()
    
    # íŠ¹ì • ë‹¨ì–´ë“¤ì˜ ID ìˆ˜ì§‘
    for word in blocked_words:
        try:
            ids = tokenizer.encode(word, add_special_tokens=False)
            blocked_ids.update(ids)
        except:
            pass
    
    # ê³ ID í† í° ì°¨ë‹¨ (50000 ì´ìƒ)
    for i in range(max_vocab_size, len(tokenizer)):
        blocked_ids.add(i)
    
    return list(blocked_ids)

def safe_generate(model, tokenizer, prompt, max_new_tokens=50):
    """
    ì•ˆì „í•œ ìƒì„± í•¨ìˆ˜
    - Top-k í•„í„°ë§ìœ¼ë¡œ ê³ í’ˆì§ˆ í† í°ë§Œ ì„ íƒ
    - ë¬¸ì œ í† í° ì°¨ë‹¨
    - ë³´ìˆ˜ì ì¸ ìƒ˜í”Œë§ íŒŒë¼ë¯¸í„°
    """
    inputs = tokenizer(prompt, return_tensors="pt").to(DEVICE)
    
    # ì°¨ë‹¨í•  í† í° ID ë¦¬ìŠ¤íŠ¸ ìƒì„±
    bad_words_ids = create_token_filter(tokenizer)
    
    # ë³´ìˆ˜ì ì¸ ìƒì„± íŒŒë¼ë¯¸í„°
    generation_config = {
        "max_new_tokens": max_new_tokens,
        "do_sample": True,
        "temperature": 0.3,  # ë§¤ìš° ë‚®ì€ ì˜¨ë„
        "top_k": 50,  # ìƒìœ„ 50ê°œ í† í°ë§Œ ê³ ë ¤
        "top_p": 0.9,
        "repetition_penalty": 1.2,
        "pad_token_id": tokenizer.pad_token_id,
        "eos_token_id": tokenizer.eos_token_id,
        "bad_words_ids": [[id] for id in bad_words_ids[:1000]],  # ì²˜ìŒ 1000ê°œë§Œ (ë©”ëª¨ë¦¬ ì œí•œ)
        "min_length": inputs['input_ids'].shape[1] + 5,  # ìµœì†Œ 5ê°œ í† í°ì€ ìƒì„±
    }
    
    with torch.inference_mode():
        outputs = model.generate(**inputs, **generation_config)
    
    # ì…ë ¥ ë¶€ë¶„ ì œì™¸í•˜ê³  ìƒì„±ëœ ë¶€ë¶„ë§Œ ë””ì½”ë”©
    generated_ids = outputs[:, inputs['input_ids'].shape[1]:]
    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
    
    # í›„ì²˜ë¦¬: ì´ìƒí•œ ë¬¸ì ì œê±°
    import re
    generated_text = re.sub(r'[^\w\s\.\,\!\?\-\']', ' ', generated_text)
    generated_text = ' '.join(generated_text.split())  # ì¤‘ë³µ ê³µë°± ì œê±°
    
    return generated_text

def interactive_chat_fixed(model, tokenizer):
    """ê°œì„ ëœ ëŒ€í™”í˜• ì±„íŒ…"""
    print("\n" + "="*60)
    print("ğŸ¤– ìˆ˜ì •ëœ OLMo ì±„íŒ… ëª¨ë“œ")
    print("ğŸ’¡ ì•ˆì „í•œ í† í°ë§Œ ì‚¬ìš©í•˜ë„ë¡ í•„í„°ë§ë¨")
    print("ğŸ“ ì¢…ë£Œ: 'quit', 'exit', 'q'")
    print("="*60)
    
    while True:
        try:
            user_input = input("\nğŸ‘¤ ì‚¬ìš©ì: ").strip()
            
            if user_input.lower() in ['quit', 'exit', 'q']:
                print("ğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤!")
                break
            
            if not user_input:
                continue
            
            print("ğŸ¤” ìƒì„± ì¤‘...")
            
            # ì•ˆì „í•œ ìƒì„± í•¨ìˆ˜ ì‚¬ìš©
            response = safe_generate(model, tokenizer, user_input)
            
            print(f"ğŸ¤– OLMo: {response}")
            
            # í’ˆì§ˆ ì²´í¬
            if len(response) < 10 or any(bad in response.lower() for bad in ['setattr', 'gloss', 'vertise']):
                print("âš ï¸ ì¶œë ¥ í’ˆì§ˆì´ ë‚®ìŠµë‹ˆë‹¤. ëª¨ë¸ ì¬ì¡°ì •ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤!")
            break
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜: {e}")
            continue

def test_model_quality(model, tokenizer):
    """ëª¨ë¸ í’ˆì§ˆ í…ŒìŠ¤íŠ¸"""
    test_prompts = [
        "The capital of France is",
        "1 + 1 equals",
        "Hello, my name is",
        "The sun rises in the",
        "Water freezes at"
    ]
    
    print("\nğŸ§ª ëª¨ë¸ í’ˆì§ˆ í…ŒìŠ¤íŠ¸:")
    print("="*60)
    
    for prompt in test_prompts:
        response = safe_generate(model, tokenizer, prompt, max_new_tokens=10)
        print(f"ì…ë ¥: '{prompt}'")
        print(f"ì¶œë ¥: '{response}'")
        print("-"*40)

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    try:
        # ìˆ˜ì •ëœ ëª¨ë¸ ë¡œë“œ
        model, tokenizer = load_olmo_model_fixed()
        
        # í’ˆì§ˆ í…ŒìŠ¤íŠ¸
        test_model_quality(model, tokenizer)
        
        # ëŒ€í™”í˜• ì±„íŒ…
        interactive_chat_fixed(model, tokenizer)
        
    except Exception as e:
        logger.error(f"Critical error: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()