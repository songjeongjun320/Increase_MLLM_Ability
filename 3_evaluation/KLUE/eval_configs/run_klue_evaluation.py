#!/usr/bin/env python3
"""
KLUE ë²¤ì¹˜ë§ˆí¬ ëª¨ë¸ í‰ê°€ ìë™ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
ëª¨ë“  ëª¨ë¸ì— ëŒ€í•´ KLUE íƒœìŠ¤í¬ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤.
"""

import os
import json
import subprocess
import time
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional
import yaml

class KLUEEvaluationRunner:
    """KLUE í‰ê°€ ìë™ ì‹¤í–‰ í´ë˜ìŠ¤"""
    
    def __init__(self, config_dir: str = ".", results_dir: str = "./klue_evaluation_results"):
        self.config_dir = Path(config_dir)
        self.results_dir = Path(results_dir)
        self.results_dir.mkdir(exist_ok=True)
        
        # KLUE íƒœìŠ¤í¬ ì •ì˜
        self.klue_tasks = [
            "tc",      # Topic Classification (YNAT)
            "sts",     # Semantic Textual Similarity  
            "nli",     # Natural Language Inference
            "re",      # Relation Extraction
            "dp",      # Dependency Parsing
            "mrc",     # Machine Reading Comprehension
            "dst"      # Dialogue State Tracking (WoS)
        ]
        
        # íƒœìŠ¤í¬ë³„ few-shot ì„¤ì • (ë³µì¡ë„ì— ë”°ë¼)
        self.task_fewshots = {
            "tc": 3,
            "sts": 3,
            "nli": 3,
            "re": 2,
            "dp": 1,
            "mrc": 2,
            "dst": 1
        }
    
    def load_model_configs(self) -> List[Dict[str, str]]:
        """ëª¨ë¸ ì„¤ì • ë¡œë“œ"""
        models = []
        
        # í•˜ë“œì½”ë”©ëœ ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ (ì‚¬ìš©ì í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •)
        model_configs = [
            {
                "name": "DeepSeek-R1-Distill-Qwen-1.5B",
                "path": "/scratch/jsong132/Increase_MLLM_Ability/Base_Models/DeepSeek-R1-Distill-Qwen-1.5B",
                "adapter": ""
            },
            {
                "name": "google_gemma-3-4b-it", 
                "path": "/scratch/jsong132/Increase_MLLM_Ability/Base_Models/google_gemma-3-4b-it",
                "adapter": ""
            },
            {
                "name": "Qwen2.5-3B-Instruct",
                "path": "/scratch/jsong132/Increase_MLLM_Ability/Base_Models/Qwen2.5-3B-Instruct", 
                "adapter": ""
            },
            {
                "name": "Llama-3.2-3B-Instruct",
                "path": "/scratch/jsong132/Increase_MLLM_Ability/Base_Models/Llama-3.2-3B-Instruct",
                "adapter": ""
            }
        ]
        
        # ëª¨ë¸ ê²½ë¡œ ì¡´ì¬ í™•ì¸
        for model_config in model_configs:
            if os.path.exists(model_config["path"]):
                models.append(model_config)
                print(f"âœ… ëª¨ë¸ ë°œê²¬: {model_config['name']}")
            else:
                print(f"âš ï¸  ëª¨ë¸ ê²½ë¡œ ì—†ìŒ: {model_config['path']}")
        
        return models
    
    def load_model_configs_from_file(self, config_file: str = "model_configs.yaml") -> List[Dict[str, str]]:
        """YAML íŒŒì¼ì—ì„œ ëª¨ë¸ ì„¤ì • ë¡œë“œ"""
        config_path = self.config_dir / config_file
        
        if not config_path.exists():
            print(f"âš ï¸  ëª¨ë¸ ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {config_path}")
            print("ê¸°ë³¸ ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return self.load_model_configs()
        
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            
            models = []
            for model_config in config.get('models', []):
                if os.path.exists(model_config["path"]):
                    models.append(model_config)
                    print(f"âœ… ëª¨ë¸ ë°œê²¬: {model_config['name']}")
                else:
                    print(f"âš ï¸  ëª¨ë¸ ê²½ë¡œ ì—†ìŒ: {model_config['path']}")
            
            return models
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return self.load_model_configs()
    
    def check_klue_configs(self) -> bool:
        """KLUE ì„¤ì • íŒŒì¼ë“¤ì´ ëª¨ë‘ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸"""
        config_files = {
            "tc": "tc.yaml",
            "sts": "sts.yaml", 
            "nli": "nli.yaml",
            "re": "re.yaml",
            "dp": "dp.yaml",
            "mrc": "mrc.yaml",
            "dst": "dst.yaml"
        }
        
        missing_files = []
        for task, filename in config_files.items():
            config_path = self.config_dir / filename
            if not config_path.exists():
                missing_files.append(filename)
        
        if missing_files:
            print(f"âŒ ëˆ„ë½ëœ KLUE ì„¤ì • íŒŒì¼ë“¤: {missing_files}")
            return False
        
        print("âœ… ëª¨ë“  KLUE ì„¤ì • íŒŒì¼ ì¡´ì¬ í™•ì¸")
        return True
    
    def run_single_evaluation(self, model: Dict[str, str], task: str) -> Dict[str, Any]:
        """ë‹¨ì¼ ëª¨ë¸-íƒœìŠ¤í¬ í‰ê°€ ì‹¤í–‰"""
        print(f"\nğŸš€ í‰ê°€ ì‹œì‘: {model['name']} on {task.upper()}")
        
        # ëª¨ë¸ ì¸ì êµ¬ì„±
        model_args = f"pretrained={model['path']}"
        if model.get('adapter') and model['adapter']:
            model_args += f",peft={model['adapter']},tokenizer={model['adapter']}"
        
        # ì¶œë ¥ íŒŒì¼ëª…
        output_file = self.results_dir / f"{model['name']}_{task}.json"
        
        # lm_eval ëª…ë ¹ì–´ êµ¬ì„±
        cmd = [
            "python", "-m", "lm_eval",
            "--model", "hf",
            "--model_args", model_args,
            "--tasks", task,
            "--num_fewshot", str(self.task_fewshots.get(task, 3)),
            "--batch_size", "auto",
            "--output_path", str(output_file),
            "--verbosity", "INFO"
        ]
        
        # í‰ê°€ ì‹¤í–‰
        start_time = time.time()
        try:
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=3600  # 1ì‹œê°„ íƒ€ì„ì•„ì›ƒ
            )
            
            end_time = time.time()
            duration = end_time - start_time
            
            if result.returncode == 0:
                print(f"âœ… í‰ê°€ ì™„ë£Œ: {model['name']} on {task.upper()} ({duration:.1f}ì´ˆ)")
                
                # ê²°ê³¼ íŒŒì¼ì—ì„œ ì •í™•ë„ ì¶”ì¶œ
                accuracy = self.extract_accuracy(output_file)
                
                return {
                    "model": model['name'],
                    "task": task,
                    "status": "success",
                    "accuracy": accuracy,
                    "duration": duration,
                    "output_file": str(output_file)
                }
            else:
                print(f"âŒ í‰ê°€ ì‹¤íŒ¨: {model['name']} on {task.upper()}")
                print(f"ì˜¤ë¥˜: {result.stderr}")
                
                return {
                    "model": model['name'],
                    "task": task,
                    "status": "failed",
                    "error": result.stderr,
                    "duration": duration
                }
                
        except subprocess.TimeoutExpired:
            print(f"â° í‰ê°€ íƒ€ì„ì•„ì›ƒ: {model['name']} on {task.upper()}")
            return {
                "model": model['name'],
                "task": task,
                "status": "timeout",
                "duration": 3600
            }
        except Exception as e:
            print(f"âŒ í‰ê°€ ì˜¤ë¥˜: {model['name']} on {task.upper()} - {e}")
            return {
                "model": model['name'],
                "task": task,
                "status": "error",
                "error": str(e)
            }
    
    def extract_accuracy(self, result_file: Path) -> Optional[float]:
        """ê²°ê³¼ íŒŒì¼ì—ì„œ ì •í™•ë„ ì¶”ì¶œ"""
        try:
            if not result_file.exists():
                return None
                
            with open(result_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # ê²°ê³¼ì—ì„œ ì£¼ìš” ë©”íŠ¸ë¦­ ì°¾ê¸°
            if 'results' in data:
                for task_key, task_results in data['results'].items():
                    # ë‹¤ì–‘í•œ ì •í™•ë„ í‚¤ ì‹œë„
                    for acc_key in ['acc', 'accuracy', 'exact_match', 'f1', 'pearsonr']:
                        if acc_key in task_results:
                            return float(task_results[acc_key])
            
            return None
            
        except Exception as e:
            print(f"âš ï¸  ì •í™•ë„ ì¶”ì¶œ ì‹¤íŒ¨ {result_file}: {e}")
            return None
    
    def run_all_evaluations(self) -> Dict[str, Any]:
        """ëª¨ë“  ëª¨ë¸ì— ëŒ€í•´ ëª¨ë“  KLUE íƒœìŠ¤í¬ í‰ê°€ ì‹¤í–‰"""
        print("ğŸ¯ KLUE ë²¤ì¹˜ë§ˆí¬ í‰ê°€ ì‹œì‘!")
        print("="*60)
        
        # ì„¤ì • í™•ì¸
        if not self.check_klue_configs():
            return {"status": "failed", "reason": "Missing KLUE config files"}
        
        # ëª¨ë¸ ë¡œë“œ
        models = self.load_model_configs_from_file()
        if not models:
            return {"status": "failed", "reason": "No valid models found"}
        
        print(f"ğŸ“Š í‰ê°€ ëŒ€ìƒ: {len(models)}ê°œ ëª¨ë¸ Ã— {len(self.klue_tasks)}ê°œ íƒœìŠ¤í¬ = {len(models) * len(self.klue_tasks)}ê°œ ì‹¤í—˜")
        print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {self.results_dir}")
        
        # ì „ì²´ ê²°ê³¼ ì €ì¥
        evaluation_results = []
        start_time = datetime.now()
        
        total_experiments = len(models) * len(self.klue_tasks)
        current_experiment = 0
        
        # ê° ëª¨ë¸ë³„ë¡œ í‰ê°€ ì‹¤í–‰
        for model in models:
            print(f"\n{'='*60}")
            print(f"ğŸ“‹ ëª¨ë¸ í‰ê°€ ì¤‘: {model['name']}")
            print(f"{'='*60}")
            
            model_results = []
            
            for task in self.klue_tasks:
                current_experiment += 1
                print(f"\n[{current_experiment}/{total_experiments}] {model['name']} â†’ {task.upper()}")
                
                result = self.run_single_evaluation(model, task)
                evaluation_results.append(result)
                model_results.append(result)
            
            # ëª¨ë¸ë³„ ê²°ê³¼ ìš”ì•½ ì¶œë ¥
            self.print_model_summary(model['name'], model_results)
        
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        # ì „ì²´ ê²°ê³¼ ì •ë¦¬
        summary = {
            "evaluation_info": {
                "start_time": start_time.isoformat(),
                "end_time": end_time.isoformat(), 
                "duration_seconds": duration,
                "total_models": len(models),
                "total_tasks": len(self.klue_tasks),
                "total_experiments": total_experiments
            },
            "models": [m['name'] for m in models],
            "tasks": self.klue_tasks,
            "results": evaluation_results
        }
        
        # ê²°ê³¼ ì €ì¥
        summary_file = self.results_dir / f"klue_evaluation_summary_{start_time.strftime('%Y%m%d_%H%M%S')}.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        print(f"\n{'='*60}")
        print("ğŸ‰ ëª¨ë“  í‰ê°€ ì™„ë£Œ!")
        print(f"â±ï¸  ì´ ì†Œìš” ì‹œê°„: {duration/3600:.1f}ì‹œê°„")
        print(f"ğŸ“Š ì „ì²´ ê²°ê³¼: {summary_file}")
        print(f"{'='*60}")
        
        # ìµœì¢… ê²°ê³¼ í…Œì´ë¸” ì¶œë ¥
        self.print_final_summary(evaluation_results, models)
        
        return summary
    
    def print_model_summary(self, model_name: str, model_results: List[Dict[str, Any]]):
        """ëª¨ë¸ë³„ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print(f"\nğŸ“Š {model_name} ê²°ê³¼ ìš”ì•½:")
        
        for result in model_results:
            task = result['task'].upper()
            status = result['status']
            
            if status == "success":
                acc = result.get('accuracy')
                acc_str = f"{acc:.4f}" if acc is not None else "N/A"
                duration = result.get('duration', 0)
                print(f"  âœ… {task:6}: {acc_str} ({duration:.1f}ì´ˆ)")
            else:
                print(f"  âŒ {task:6}: {status}")
    
    def print_final_summary(self, results: List[Dict[str, Any]], models: List[Dict[str, str]]):
        """ìµœì¢… ê²°ê³¼ ìš”ì•½ í…Œì´ë¸” ì¶œë ¥"""
        print(f"\nğŸ“ˆ KLUE ë²¤ì¹˜ë§ˆí¬ ìµœì¢… ê²°ê³¼")
        print(f"{'='*80}")
        
        # í—¤ë” ì¶œë ¥
        header = "ëª¨ë¸ëª…".ljust(25)
        for task in self.klue_tasks:
            header += task.upper().rjust(8)
        header += "í‰ê· ".rjust(8)
        print(header)
        print("-" * 80)
        
        # ê° ëª¨ë¸ë³„ ê²°ê³¼ ì¶œë ¥
        for model in models:
            model_name = model['name']
            row = model_name[:24].ljust(25)
            
            model_results = [r for r in results if r['model'] == model_name]
            accuracies = []
            
            for task in self.klue_tasks:
                task_result = next((r for r in model_results if r['task'] == task), None)
                
                if task_result and task_result['status'] == 'success':
                    acc = task_result.get('accuracy')
                    if acc is not None:
                        row += f"{acc:.3f}".rjust(8)
                        accuracies.append(acc)
                    else:
                        row += "N/A".rjust(8)
                else:
                    row += "FAIL".rjust(8)
            
            # í‰ê·  ê³„ì‚°
            if accuracies:
                avg_acc = sum(accuracies) / len(accuracies)
                row += f"{avg_acc:.3f}".rjust(8)
            else:
                row += "N/A".rjust(8)
            
            print(row)
        
        print("=" * 80)

def create_model_config_template():
    """ëª¨ë¸ ì„¤ì • í…œí”Œë¦¿ ìƒì„±"""
    template = {
        "models": [
            {
                "name": "DeepSeek-R1-Distill-Qwen-1.5B",
                "path": "/path/to/your/model1",
                "adapter": ""  # LoRA ì–´ëŒ‘í„° ê²½ë¡œ (ìˆëŠ” ê²½ìš°)
            },
            {
                "name": "Qwen2.5-3B-Instruct",
                "path": "/path/to/your/model2", 
                "adapter": ""
            }
        ]
    }
    
    with open("model_configs.yaml", 'w', encoding='utf-8') as f:
        yaml.dump(template, f, default_flow_style=False, allow_unicode=True)
    
    print("âœ… model_configs.yaml í…œí”Œë¦¿ ìƒì„±ë¨")
    print("ëª¨ë¸ ê²½ë¡œë¥¼ ì‹¤ì œ ê²½ë¡œë¡œ ìˆ˜ì •í•´ì£¼ì„¸ìš”.")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description="KLUE ë²¤ì¹˜ë§ˆí¬ ëª¨ë¸ í‰ê°€ ìë™ ì‹¤í–‰")
    parser.add_argument("--config_dir", default=".", help="ì„¤ì • íŒŒì¼ ë””ë ‰í† ë¦¬")
    parser.add_argument("--results_dir", default="./klue_evaluation_results", help="ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--create_template", action="store_true", help="ëª¨ë¸ ì„¤ì • í…œí”Œë¦¿ ìƒì„±")
    
    args = parser.parse_args()
    
    if args.create_template:
        create_model_config_template()
        return
    
    # í‰ê°€ ì‹¤í–‰
    runner = KLUEEvaluationRunner(args.config_dir, args.results_dir)
    results = runner.run_all_evaluations()
    
    if results.get("status") == "failed":
        print(f"âŒ í‰ê°€ ì‹¤íŒ¨: {results.get('reason')}")
        exit(1)

if __name__ == "__main__":
    main()