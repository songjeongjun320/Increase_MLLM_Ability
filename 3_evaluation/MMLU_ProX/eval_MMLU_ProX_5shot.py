import os
import json
import logging
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel
from tqdm import tqdm
import re
from dataclasses import dataclass, field
import gc
import sys
from datetime import datetime
import time
import random

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - [%(name)s] - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# Global Configuration
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
CACHE_DIR = "../cache"  # Cache directory for models
DATASET_PATH = "../../2_datasets/MMLU_ProX/test/"  # Path to MMLU-ProX dataset
KOREAN_DATASET_PATH = "../../2_datasets/MMLU_ProX/ko-test/"  # Path to Korean MMLU-ProX dataset
BASE_OUTPUT_DIR = "../4_evaluation_results/MMLU_ProX_5shot"  # Output directory
BATCH_SIZE = 8

# Import performance analyzer
try:
    import sys
    sys.path.append('../')
    from performance_analyzer import create_enhanced_summary
except ImportError:
    logger.warning("Performance analyzer not available. Using basic summary.")
    create_enhanced_summary = None

# --- Model Configuration ---
@dataclass
class ModelConfig:
    name: str
    model_id: str
    adapter_path: str = None
    use_quantization: bool = True
    torch_dtype: torch.dtype = field(default=torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16)

MODEL_CONFIGS = [
    # Base Models (commented out for now)
    ModelConfig(
        name="Qwen2.5-3B-Instruct",
        model_id="/scratch/jsong132/Increase_MLLM_Ability/Base_Models/Qwen2.5-3B-Instruct",
        use_quantization=False
    ),
    ModelConfig(
        name="google_gemma-3-4b-it",
        model_id="/scratch/jsong132/Increase_MLLM_Ability/Base_Models/google_gemma-3-4b-it",
        use_quantization=False
    ),
    ModelConfig(
        name="Llama-3.2-3B-Instruct",
        model_id="/scratch/jsong132/Increase_MLLM_Ability/Base_Models/Llama-3.2-3B-Instruct",
        use_quantization=False
    ),
    ModelConfig(
        name="DeepSeek-R1-Distill-Qwen-1.5B",
        model_id="/scratch/jsong132/Increase_MLLM_Ability/Base_Models/DeepSeek-R1-Distill-Qwen-1.5B",
        use_quantization=False
    ),

    # ToW Trained Models
    # ModelConfig(
    #     name="Qwen2.5-3B-Instruct-ToW",
    #     model_id="/scratch/jsong132/Increase_MLLM_Ability/Base_Models/Qwen2.5-3B-Instruct",
    #     adapter_path="/scratch/jsong132/Increase_MLLM_Ability/5_training/ToW_Models2/Qwen2.5-3B-Instruct-ToW",
    #     use_quantization=False
    # ),
    # ModelConfig(
    #     name="google_gemma-3-4b-it-ToW",
    #     model_id="/scratch/jsong132/Increase_MLLM_Ability/Base_Models/google_gemma-3-4b-it",
    #     adapter_path="/scratch/jsong132/Increase_MLLM_Ability/5_training/ToW_Models2/google_gemma-3-4b-it-ToW",
    #     use_quantization=False
    # ),
    ModelConfig(
        name="Llama-3.2-3B-Instruct-ToW",
        model_id="/scratch/jsong132/Increase_MLLM_Ability/Base_Models/Llama-3.2-3B-Instruct",
        adapter_path="/scratch/jsong132/Increase_MLLM_Ability/5_training/ToW_Models2/Llama-3.2-3B-Instruct-ToW",
        use_quantization=False
    ),
    ModelConfig(
        name="DeepSeek-R1-Distill-Qwen-1.5B-ToW",
        model_id="/scratch/jsong132/Increase_MLLM_Ability/Base_Models/DeepSeek-R1-Distill-Qwen-1.5B",
        adapter_path="/scratch/jsong132/Increase_MLLM_Ability/5_training/ToW_Models2/DeepSeek-R1-Distill-Qwen-1.5B-ToW",
        use_quantization=False
    ),
]

# --- General Configuration ---
MMLU_PROX_EN_DATASET_PATH = "../../2_datasets/MMLU_ProX/MMLU_ProX_en.json"
MMLU_PROX_KO_DATASET_PATH = "../../2_datasets/MMLU_ProX/MMLU_ProX_Ko.json"
BASE_OUTPUT_DIR = "mmlu_prox_5shot"
BATCH_SIZE = 1
MAX_NEW_TOKENS = 512
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
CACHE_DIR = "./cache" if not os.path.exists("/scratch/jsong132/.cache/huggingface") else "/scratch/jsong132/.cache/huggingface"

# --- Logging Setup ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - [%(name)s] - %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger(__name__)

ENGLISH_FEW_SHOT_EXAMPLES = [
    {
        "question": "In a race, Alice finished before Bob. Charlie finished after David. Bob finished before David. Who finished last?",
        "options": {
            "A": "Alice",
            "B": "Bob",
            "C": "Charlie",
            "D": "David",
            "E": "Cannot be determined"
        },
        "cot_content": """Response: Let's think step by step.
Let's break down the statements. Let '>' mean 'finished before'.
Statement 1: Alice > Bob.
Statement 3: Bob > David.
Combining these two, we get the order: Alice > Bob > David.
Statement 2: Charlie finished after David, which means David > Charlie.
Combining all information, the complete order is: Alice > Bob > David > Charlie.
The person who finished last is the one at the very end of this chain. That person is Charlie.""",
        "answer": "C"
    },
    {
        "question": "A net force of 50 Newtons is applied to a 10 kg object. What is the acceleration of the object?",
        "options": {
            "A": "0.2 m/s¬≤",
            "B": "5 m/s¬≤",
            "C": "40 m/s¬≤",
            "D": "500 m/s¬≤",
            "E": "50 m/s¬≤",
            "F": "10 m/s¬≤"
        },
        "cot_content": """Response: Let's think step by step.
The question asks for acceleration given a net force and a mass.
The relevant physical principle is Newton's Second Law of Motion.
The formula is Force = mass √ó acceleration (F = ma).
We need to rearrange the formula to solve for acceleration: acceleration = Force / mass (a = F/m).
The given values are Force (F) = 50 N and mass (m) = 10 kg.
Substitute the values into the rearranged formula: a = 50 N / 10 kg.
The calculation gives a = 5 m/s¬≤. This matches option B.""",
        "answer": "B"
    },
    {
        "question": "From which country did the United States purchase the Louisiana Territory in 1803?",
        "options": {
            "A": "Spain",
            "B": "Great Britain",
            "C": "Mexico",
            "D": "Russia",
            "E": "France",
            "F": "The Netherlands"
        },
        "cot_content": """Response: Let's think step by step.
The question is about the Louisiana Purchase in 1803.
I need to recall the historical context of that period in North America. The major European powers with territory were Spain, Great Britain, and France.
At that time, the leader of France was Napoleon Bonaparte. He was engaged in wars in Europe and needed funds.
The territory, known as Louisiana, was difficult for France to control and defend from afar.
Therefore, Napoleon decided to sell the vast territory to the young United States to finance his military campaigns. This event is known as the Louisiana Purchase.
This historical fact confirms the purchase was made from France.""",
        "answer": "E"
    },
    {
        "question": "What are the primary products of photosynthesis?",
        "options": {
            "A": "Carbon dioxide and water",
            "B": "Glucose and water",
            "C": "Oxygen and carbon dioxide",
            "D": "Glucose and oxygen",
            "E": "Sunlight and water"
        },
        "cot_content": """Response: Let's think step by step.
Photosynthesis is the process plants use to convert light energy into chemical energy.
First, let's identify the inputs (reactants). Plants take in carbon dioxide (CO‚ÇÇ), water (H‚ÇÇO), and sunlight.
The process then converts these inputs into outputs (products).
One main product is a sugar called glucose (C‚ÇÜH‚ÇÅ‚ÇÇO‚ÇÜ), which the plant uses as food/energy.
The other main product is oxygen (O‚ÇÇ), which is released into the atmosphere as a byproduct.
Therefore, the primary products are glucose and oxygen. This corresponds to option D.""",
        "answer": "D"
    },
    {
        "question": "Who is the author of the famous line, \"To be, or not to be: that is the question\"?",
        "options": {
            "A": "Christopher Marlowe",
            "B": "John Milton",
            "C": "William Shakespeare",
            "D": "Charles Dickens",
            "E": "Jane Austen"
        },
        "cot_content": """Response: Let's think step by step.
This is one of the most famous quotes in English literature.
I need to identify which play and author it comes from.
The line is a soliloquy from the play Hamlet.
The author of Hamlet is William Shakespeare, the famous English playwright.
The other authors are known for different works: John Milton for Paradise Lost, Charles Dickens for novels like A Tale of Two Cities, etc. The style and origin firmly point to Shakespeare.""",
        "answer": "C"
    }
]

KOREAN_FEW_SHOT_EXAMPLES = [
    {
        "question": "Í≤ΩÏ£ºÏóêÏÑú Ïï®Î¶¨Ïä§Îäî Î∞•Î≥¥Îã§ Î®ºÏ†Ä Îì§Ïñ¥ÏôîÎã§. Ï∞∞Î¶¨Îäî Îç∞Ïù¥ÎπÑÎìúÎ≥¥Îã§ Îä¶Í≤å Îì§Ïñ¥ÏôîÎã§. Î∞•ÏùÄ Îç∞Ïù¥ÎπÑÎìúÎ≥¥Îã§ Î®ºÏ†Ä Îì§Ïñ¥ÏôîÎã§. ÎàÑÍ∞Ä Í∞ÄÏû• Íº¥Ï∞åÎ°ú Îì§Ïñ¥ÏôîÎäîÍ∞Ä?",
        "options": {
            "A": "Ïï®Î¶¨Ïä§",
            "B": "Î∞•",
            "C": "Ï∞∞Î¶¨",
            "D": "Îç∞Ïù¥ÎπÑÎìú",
            "E": "Í≤∞Ï†ïÌï† Ïàò ÏóÜÏùå"
        },
        "cot_content": """ÏùëÎãµ: Îã®Í≥ÑÎ≥ÑÎ°ú ÏÉùÍ∞ÅÌï¥Î¥ÖÏãúÎã§.
Ï£ºÏñ¥ÏßÑ Î¨∏Ïû•Îì§ÏùÑ Î∂ÑÏÑùÌï¥ Î≥¥Í≤†ÏäµÎãàÎã§. '>'Î•º 'Î®ºÏ†Ä Îì§Ïñ¥ÏôîÎã§'Îäî ÏùòÎØ∏Î°ú ÏÇ¨Ïö©ÌïòÍ≤†ÏäµÎãàÎã§.
Î¨∏Ïû• 1: Ïï®Î¶¨Ïä§ > Î∞•.
Î¨∏Ïû• 3: Î∞• > Îç∞Ïù¥ÎπÑÎìú.
Ïù¥ Îëê Î¨∏Ïû•ÏùÑ Ï°∞Ìï©ÌïòÎ©¥ ÏàúÏÑúÎäî Ïï®Î¶¨Ïä§ > Î∞• > Îç∞Ïù¥ÎπÑÎìú ÏûÖÎãàÎã§.
Î¨∏Ïû• 2: Ï∞∞Î¶¨Îäî Îç∞Ïù¥ÎπÑÎìúÎ≥¥Îã§ Îä¶Í≤å Îì§Ïñ¥ÏôîÎã§, Ï¶â Îç∞Ïù¥ÎπÑÎìú > Ï∞∞Î¶¨ ÏûÖÎãàÎã§.
Î™®Îì† Ï†ïÎ≥¥Î•º Ï¢ÖÌï©ÌïòÎ©¥, Ï†ÑÏ≤¥ ÏàúÏÑúÎäî Ïï®Î¶¨Ïä§ > Î∞• > Îç∞Ïù¥ÎπÑÎìú > Ï∞∞Î¶¨ ÏûÖÎãàÎã§.
Í∞ÄÏû• Íº¥Ï∞åÎ°ú Îì§Ïñ¥Ïò® ÏÇ¨ÎûåÏùÄ Ïù¥ ÏàúÏÑúÏùò Îß® ÎßàÏßÄÎßâÏóê ÏûàÎäî ÏÇ¨ÎûåÏûÖÎãàÎã§. Í∑∏ ÏÇ¨ÎûåÏùÄ Ï∞∞Î¶¨ÏûÖÎãàÎã§.""",
        "answer": "C"
    },
    {
        "question": "10kgÏùò Î¨ºÏ≤¥Ïóê 50 Îâ¥ÌÑ¥(N)Ïùò ÏïåÏßúÌûòÏù¥ Í∞ÄÌï¥Ï°åÎã§. Ïù¥ Î¨ºÏ≤¥Ïùò Í∞ÄÏÜçÎèÑÎäî ÏñºÎßàÏù∏Í∞Ä?",
        "options": {
            "A": "0.2 m/s¬≤",
            "B": "5 m/s¬≤",
            "C": "40 m/s¬≤",
            "D": "500 m/s¬≤",
            "E": "50 m/s¬≤",
            "F": "10 m/s¬≤"
        },
        "cot_content": """ÏùëÎãµ: Îã®Í≥ÑÎ≥ÑÎ°ú ÏÉùÍ∞ÅÌï¥Î¥ÖÏãúÎã§.
Ïù¥ ÏßàÎ¨∏ÏùÄ ÏïåÏßúÌûòÍ≥º ÏßàÎüâÏù¥ Ï£ºÏñ¥Ï°åÏùÑ Îïå Í∞ÄÏÜçÎèÑÎ•º Íµ¨ÌïòÎäî Î¨∏Ï†úÏûÖÎãàÎã§.
Í¥ÄÎ†®Îêú Î¨ºÎ¶¨ Î≤ïÏπôÏùÄ Îâ¥ÌÑ¥Ïùò Ïö¥Îèô Ï†ú2Î≤ïÏπôÏûÖÎãàÎã§.
Í≥µÏãùÏùÄ Ìûò = ÏßàÎüâ √ó Í∞ÄÏÜçÎèÑ (F = ma) ÏûÖÎãàÎã§.
Í∞ÄÏÜçÎèÑÎ•º Íµ¨ÌïòÍ∏∞ ÏúÑÌï¥ Í≥µÏãùÏùÑ Î≥ÄÌòïÌï¥Ïïº Ìï©ÎãàÎã§: Í∞ÄÏÜçÎèÑ = Ìûò / ÏßàÎüâ (a = F/m).
Ï£ºÏñ¥ÏßÑ Í∞íÏùÄ Ìûò (F) = 50 N Ïù¥Í≥†, ÏßàÎüâ (m) = 10 kg ÏûÖÎãàÎã§.
Î≥ÄÌòïÎêú Í≥µÏãùÏóê Í∞íÏùÑ ÎåÄÏûÖÌï©ÎãàÎã§: a = 50 N / 10 kg.
Í≥ÑÏÇ∞ Í≤∞Í≥º a = 5 m/s¬≤ ÏûÖÎãàÎã§. Ïù¥Îäî ÏÑ†ÌÉùÏßÄ BÏôÄ ÏùºÏπòÌï©ÎãàÎã§.""",
        "answer": "B"
    },
    {
        "question": "1803ÎÖÑ ÎØ∏Íµ≠ÏùÄ Ïñ¥Îäê ÎÇòÎùºÎ°úÎ∂ÄÌÑ∞ Î£®Ïù¥ÏßÄÏï†ÎÇò ÏòÅÌÜ†Î•º Îß§ÏûÖÌñàÎäîÍ∞Ä?",
        "options": {
            "A": "Ïä§ÌéòÏù∏",
            "B": "ÏòÅÍµ≠",
            "C": "Î©ïÏãúÏΩî",
            "D": "Îü¨ÏãúÏïÑ",
            "E": "ÌîÑÎûëÏä§",
            "F": "ÎÑ§ÎçúÎûÄÎìú"
        },
        "cot_content": """ÏùëÎãµ: Îã®Í≥ÑÎ≥ÑÎ°ú ÏÉùÍ∞ÅÌï¥Î¥ÖÏãúÎã§.
Ïù¥ ÏßàÎ¨∏ÏùÄ 1803ÎÖÑÏùò 'Î£®Ïù¥ÏßÄÏï†ÎÇò Îß§ÏûÖ'Ïóê Í¥ÄÌïú Í≤ÉÏûÖÎãàÎã§.
ÎãπÏãú Î∂ÅÎØ∏ ÎåÄÎ•ôÏùò Ïó≠ÏÇ¨Ï†Å ÏÉÅÌô©ÏùÑ Îñ†Ïò¨Î†§Ïïº Ìï©ÎãàÎã§. ÏòÅÌÜ†Î•º Í∞ÄÏßÑ Ï£ºÏöî Ïú†ÎüΩ Íµ≠Í∞ÄÎäî Ïä§ÌéòÏù∏, ÏòÅÍµ≠, ÌîÑÎûëÏä§ÏòÄÏäµÎãàÎã§.
Í∑∏ ÏãúÍ∏∞ ÌîÑÎûëÏä§Ïùò ÏßÄÎèÑÏûêÎäî ÎÇòÌè¥Î†àÏòπ Î≥¥ÎÇòÌååÎ•¥Ìä∏ÏòÄÏäµÎãàÎã§. Í∑∏Îäî Ïú†ÎüΩÏóêÏÑú Ï†ÑÏüÅÏùÑ ÏπòÎ•¥Í≥† ÏûàÏóàÍ≥† ÏûêÍ∏àÏù¥ ÌïÑÏöîÌñàÏäµÎãàÎã§.
Î£®Ïù¥ÏßÄÏï†ÎÇòÎ°ú ÏïåÎ†§ÏßÑ ÏòÅÌÜ†Îäî ÌîÑÎûëÏä§Í∞Ä Î©ÄÎ¶¨ÏÑú ÌÜµÏ†úÌïòÍ≥† Î∞©Ïñ¥ÌïòÍ∏∞ Ïñ¥Î†§Ïõ†ÏäµÎãàÎã§.
Îî∞ÎùºÏÑú ÎÇòÌè¥Î†àÏòπÏùÄ Í∑∏Ïùò Íµ∞ÏÇ¨ ÏûëÏ†Ñ ÏûêÍ∏àÏùÑ ÎßàÎ†®ÌïòÍ∏∞ ÏúÑÌï¥ Í¥ëÎåÄÌïú ÏòÅÌÜ†Î•º Ïã†ÏÉù Íµ≠Í∞ÄÏù∏ ÎØ∏Íµ≠Ïóê ÌåîÍ∏∞Î°ú Í≤∞Ï†ïÌñàÏäµÎãàÎã§. Ïù¥ ÏÇ¨Í±¥Ïù¥ Î∞îÎ°ú Î£®Ïù¥ÏßÄÏï†ÎÇò Îß§ÏûÖÏûÖÎãàÎã§.
Ïù¥ Ïó≠ÏÇ¨Ï†Å ÏÇ¨Ïã§ÏùÄ Ìï¥Îãπ ÏòÅÌÜ†Î•º ÌîÑÎûëÏä§Î°úÎ∂ÄÌÑ∞ Îß§ÏûÖÌñàÏùåÏùÑ ÌôïÏù∏ÏãúÏºú Ï§çÎãàÎã§.""",
        "answer": "E"
    },
    {
        "question": "Í¥ëÌï©ÏÑ±Ïùò Ï£ºÎêú ÏÉùÏÑ±Î¨ºÏùÄ Î¨¥ÏóáÏù∏Í∞Ä?",
        "options": {
            "A": "Ïù¥ÏÇ∞ÌôîÌÉÑÏÜåÏôÄ Î¨º",
            "B": "Ìè¨ÎèÑÎãπÍ≥º Î¨º",
            "C": "ÏÇ∞ÏÜåÏôÄ Ïù¥ÏÇ∞ÌôîÌÉÑÏÜå",
            "D": "Ìè¨ÎèÑÎãπÍ≥º ÏÇ∞ÏÜå",
            "E": "ÌñáÎπõÍ≥º Î¨º"
        },
        "cot_content": """ÏùëÎãµ: Îã®Í≥ÑÎ≥ÑÎ°ú ÏÉùÍ∞ÅÌï¥Î¥ÖÏãúÎã§.
Í¥ëÌï©ÏÑ±ÏùÄ ÏãùÎ¨ºÏù¥ Îπõ ÏóêÎÑàÏßÄÎ•º ÌôîÌïô ÏóêÎÑàÏßÄÎ°ú Ï†ÑÌôòÌïòÎäî Í≥ºÏ†ïÏûÖÎãàÎã§.
Î®ºÏ†Ä, Ìà¨ÏûÖÎ¨º(Î∞òÏùëÎ¨º)Ïù¥ Î¨¥ÏóáÏù∏ÏßÄ ÌôïÏù∏Ìï©ÎãàÎã§. ÏãùÎ¨ºÏùÄ Ïù¥ÏÇ∞ÌôîÌÉÑÏÜå(CO‚ÇÇ), Î¨º(H‚ÇÇO), Í∑∏Î¶¨Í≥† ÌñáÎπõÏùÑ Ìù°ÏàòÌï©ÎãàÎã§.
Ïù¥ Í≥ºÏ†ïÏùÄ Ìà¨ÏûÖÎ¨ºÏùÑ ÏÇ∞Ï∂úÎ¨º(ÏÉùÏÑ±Î¨º)Î°ú Î≥ÄÌôòÌï©ÎãàÎã§.
Ï£ºÏöî ÏÉùÏÑ±Î¨º Ï§ë ÌïòÎÇòÎäî ÏãùÎ¨ºÏù¥ ÏãùÎüâ/ÏóêÎÑàÏßÄÎ°ú ÏÇ¨Ïö©ÌïòÎäî Ìè¨ÎèÑÎãπ(C‚ÇÜH‚ÇÅ‚ÇÇO‚ÇÜ)Ïù¥ÎùºÎäî ÎãπÏûÖÎãàÎã§.
Îã§Î•∏ Ï£ºÏöî ÏÉùÏÑ±Î¨ºÏùÄ Î∂ÄÏÇ∞Î¨ºÎ°úÏÑú ÎåÄÍ∏∞ Ï§ëÏúºÎ°ú Î∞©Ï∂úÎêòÎäî ÏÇ∞ÏÜå(O‚ÇÇ)ÏûÖÎãàÎã§.
Îî∞ÎùºÏÑú, Ï£ºÎêú ÏÉùÏÑ±Î¨ºÏùÄ Ìè¨ÎèÑÎãπÍ≥º ÏÇ∞ÏÜåÏûÖÎãàÎã§. Ïù¥Îäî ÏÑ†ÌÉùÏßÄ DÏóê Ìï¥ÎãπÌï©ÎãàÎã§.""",
        "answer": "D"
    },
    {
        "question": "\"Ï£ΩÎäêÎÉê ÏÇ¨ÎäêÎÉê, Í∑∏Í≤ÉÏù¥ Î¨∏Ï†úÎ°úÎã§\"ÎùºÎäî Ïú†Î™ÖÌïú ÎåÄÏÇ¨Î•º Ïì¥ ÏûëÍ∞ÄÎäî ÎàÑÍµ¨Ïù∏Í∞Ä?",
        "options": {
            "A": "ÌÅ¨Î¶¨Ïä§ÌÜ†Ìçº ÎßêÎ°ú",
            "B": "Ï°¥ Î∞ÄÌÑ¥",
            "C": "ÏúåÎ¶¨ÏóÑ ÏÖ∞ÏùµÏä§ÌîºÏñ¥",
            "D": "Ï∞∞Ïä§ ÎîîÌÇ®Ïä§",
            "E": "Ï†úÏù∏ Ïò§Ïä§Ìã¥"
        },
        "cot_content": """ÏùëÎãµ: Îã®Í≥ÑÎ≥ÑÎ°ú ÏÉùÍ∞ÅÌï¥Î¥ÖÏãúÎã§.
Ïù¥Í≤ÉÏùÄ ÏòÅÎ¨∏ÌïôÏóêÏÑú Í∞ÄÏû• Ïú†Î™ÖÌïú Ïù∏Ïö©Íµ¨ Ï§ë ÌïòÎÇòÏûÖÎãàÎã§.
Ïñ¥Îñ§ Ìù¨Í≥°Í≥º ÏûëÍ∞ÄÎ°úÎ∂ÄÌÑ∞ ÎÇòÏôîÎäîÏßÄ ÏãùÎ≥ÑÌï¥Ïïº Ìï©ÎãàÎã§.
Ïù¥ ÎåÄÏÇ¨Îäî Ìù¨Í≥° ¬´ÌñÑÎ¶ø¬ªÏóê ÎÇòÏò§Îäî ÎèÖÎ∞±ÏûÖÎãàÎã§.
¬´ÌñÑÎ¶ø¬ªÏùò Ï†ÄÏûêÎäî ÏòÅÍµ≠Ïùò Ïú†Î™ÖÌïú Í∑πÏûëÍ∞ÄÏù∏ ÏúåÎ¶¨ÏóÑ ÏÖ∞ÏùµÏä§ÌîºÏñ¥ÏûÖÎãàÎã§.
Îã§Î•∏ ÏûëÍ∞ÄÎì§ÏùÄ Îã§Î•∏ ÏûëÌíàÏúºÎ°ú Ïú†Î™ÖÌï©ÎãàÎã§: Ï°¥ Î∞ÄÌÑ¥ÏùÄ ¬´Ïã§ÎÇôÏõê¬ª, Ï∞∞Ïä§ ÎîîÌÇ®Ïä§Îäî ¬´Îëê ÎèÑÏãú Ïù¥ÏïºÍ∏∞¬ªÏôÄ Í∞ôÏùÄ ÏÜåÏÑ§Î°ú Ïú†Î™ÖÌï©ÎãàÎã§. Î¨∏Ï≤¥ÏôÄ Ï∂úÏ≤òÎ•º Î≥º Îïå ÏÖ∞ÏùµÏä§ÌîºÏñ¥Í∞Ä ÌôïÏã§Ìï©ÎãàÎã§.""",
        "answer": "C"
    }
]

def create_5shot_prompt(item, few_shot_examples, language="en"):
    """
    Creates a 5-shot MMLU-ProX prompt for a given test item.
    (Corrected Version)
    """
    if language == "ko":
        prompt_parts = ["Îã§ÏùåÏùÄ Îã§ÏñëÌïú ÌïôÎ¨∏ Î∂ÑÏïºÏùò Ï†ÑÎ¨∏Ï†ÅÏù¥Í≥† Ïñ¥Î†§Ïö¥ Îã§ÏßÄÏÑ†Îã§Ìòï ÏßàÎ¨∏ÏûÖÎãàÎã§.\n"]
    else:
        prompt_parts = ["The following are challenging multiple choice questions from various academic disciplines.\n"]
    
    # Add few-shot examples
    for example in few_shot_examples:
        # 1. ÏßàÎ¨∏, CoT ÎÇ¥Ïö©, Ï†ïÎãµÏùÑ ÎîïÏÖîÎÑàÎ¶¨ÏóêÏÑú ÏßÅÏ†ë Í∞ÄÏ†∏ÏòµÎãàÎã§.
        question = example["question"]
        correct_answer = example["answer"]
        cot_reasoning = example["cot_content"] # Ïã§Ï†ú Ï∂îÎ°† ÎÇ¥Ïö©ÏùÑ Í∞ÄÏ†∏ÏòµÎãàÎã§.

        prompt_parts.append(f"Question: {question}")
        
        # 2. ÏòàÏ†úÏùò ÏòµÏÖò Ï≤òÎ¶¨ Î∞©ÏãùÏùÑ ÏàòÏ†ïÌï©ÎãàÎã§. (options ÎîïÏÖîÎÑàÎ¶¨ ÏàúÌöå)
        #    sorted()Î•º ÏÇ¨Ïö©ÌïòÏó¨ A, B, C ÏàúÏÑúÎ•º Î≥¥Ïû•Ìï©ÎãàÎã§.
        options = []
        for key, value in sorted(example["options"].items()):
            options.append(f"{key}. {value}")
        prompt_parts.extend(options)
        
        # 3. [ÏÉùÍ∞Å] ÌîåÎ†àÏù¥Ïä§ÌôÄÎçî ÎåÄÏã† Ïã§Ï†ú CoT Ï∂îÎ°† ÎÇ¥Ïö©(cot_reasoning)ÏùÑ Ï∂îÍ∞ÄÌï©ÎãàÎã§.
        prompt_parts.append(cot_reasoning)
        
        if language == "ko":
            prompt_parts.append(f"#### Îî∞ÎùºÏÑú Ï†ïÎãµÏùÄ {correct_answer} ÏûÖÎãàÎã§.")
            prompt_parts.append(f"#### Ï†ïÎãµ: {correct_answer}")
        else:
            prompt_parts.append(f"#### So the answer is {correct_answer}.")
            prompt_parts.append(f"#### Answer: {correct_answer}.")
        prompt_parts.append("")
    
    # Add the test question
    question = item.get("question", "")
    options = []
    for i in range(10):
        option_key = f"option_{i}"
        # MMLU-ProX Îç∞Ïù¥ÌÑ∞ÏÖãÏùò Ïã§Ï†ú 'item'ÏùÄ Ïù¥ ÌòïÏãùÏùÑ Îî∞Î•¥ÎØÄÎ°ú Ïù¥ Î°úÏßÅÏùÄ Ïú†ÏßÄÌï©ÎãàÎã§.
        if option_key in item and item[option_key] and str(item[option_key]).strip() and str(item[option_key]).strip() != "N/A":
            options.append(f"{chr(65+i)}. {item[option_key]}")
    
    prompt_parts.append(f"Question: {question}")
    prompt_parts.extend(options)
    prompt_parts.append("")
    
    if language == "ko":
        prompt_parts.append("Îã®Í≥ÑÎ≥ÑÎ°ú ÏÉùÍ∞ÅÌï¥Î¥ÖÏãúÎã§. ")
    else:
        prompt_parts.append("Let's think step by step. ")
    
    return "\n".join(prompt_parts)


def extract_answer_first_token(model_output):
    """
    Extract answer from model output using structured patterns first, then fallback approaches.
    Supports A-J for 10 options.
    """
    if not model_output:
        return None
        
    cleaned_output = model_output.strip().upper()
    valid_answers = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']
    
    import re
    
    # Priority 1: Structured answer patterns (most reliable)
    structured_patterns = [
        r'####\s*(?:Ï†ïÎãµ|Îãµ|ANSWER)\s*:?\s*([A-J])',  # #### Answer: A or #### Ï†ïÎãµ: A
        r'(?:Ï†ïÎãµ|Îãµ|ANSWER)\s*:?\s*([A-J])',        # Answer: A or Ï†ïÎãµ: A
        r'(?:Îî∞ÎùºÏÑú|Í∑∏Îü¨ÎØÄÎ°ú|SO)\s+(?:Ï†ïÎãµÏùÄ|ÎãµÏùÄ|THE\s+ANSWER\s+IS)\s+([A-J])',  # So the answer is A
    ]
    
    for pattern in structured_patterns:
        matches = re.findall(pattern, cleaned_output)
        if matches:
            return matches[-1]  # Return the last match (final answer)
    
    # Priority 2: Start of text patterns
    start_patterns = [
        r'^\s*([A-J])[\.\.\)\]\s]',  # A. or A) or A] at start
        r'^\s*\(?([A-J])\)?\s*[\.:;]',  # (A): or A. or A:
        r'^\s*([A-J])\s*$',          # Just A at start of line
    ]
    
    for pattern in start_patterns:
        match = re.search(pattern, cleaned_output, re.MULTILINE)
        if match:
            return match.group(1)
    
    # Priority 3: Last resort - find A-J near end of text (avoid random letters in middle)
    # Only look in last 100 characters to avoid picking up random letters
    last_part = cleaned_output[-100:] if len(cleaned_output) > 100 else cleaned_output
    
    # Look for isolated A-J characters near the end
    end_patterns = [
        r'([A-J])(?:\s*[\.:;]?\s*$)',  # A at end with optional punctuation
        r'(?:\s|^)([A-J])(?:\s|$)',    # A surrounded by whitespace
    ]
    
    for pattern in end_patterns:
        matches = re.findall(pattern, last_part)
        if matches:
            return matches[-1]  # Return the last match
    
    # Priority 4: Absolute fallback - scan from end backwards
    # This avoids picking random letters from the beginning/middle of text
    for i in range(len(cleaned_output) - 1, -1, -1):
        if cleaned_output[i] in valid_answers:
            # Check if this letter appears to be part of an answer pattern
            context_start = max(0, i - 20)
            context_end = min(len(cleaned_output), i + 20)
            context = cleaned_output[context_start:context_end]
            
            # Avoid letters that are clearly part of words
            if i > 0 and cleaned_output[i-1].isalnum():
                continue
            if i < len(cleaned_output) - 1 and cleaned_output[i+1].isalnum():
                continue
                
            return cleaned_output[i]
    
    return None

def process_single_with_retry(model, tokenizer, prompt, max_retries=5):
    """
    Process a single prompt with retry logic for answer extraction failures
    Only retries when answer extraction fails (not on genuine model errors)
    """
    for attempt in range(max_retries):
        try:
            inputs = tokenizer(
                prompt,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=4096
            ).to(DEVICE)

            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=5,
                    pad_token_id=tokenizer.pad_token_id,
                    eos_token_id=tokenizer.eos_token_id,
                    do_sample=False,
                    return_dict_in_generate=True
                )
            
            input_length = inputs['input_ids'].shape[1]
            output_tokens = outputs['sequences'][0][input_length:]
            generated_text = tokenizer.decode(output_tokens, skip_special_tokens=True).strip()
            
            # Try to extract answer
            extracted_answer = extract_answer_first_token(generated_text)
            if extracted_answer is not None:
                return generated_text, extracted_answer
            else:
                # Answer extraction failed - try again if we have retries left
                if attempt < max_retries - 1:
                    logger.warning(f"Retry {attempt + 1}/{max_retries}: Failed to extract answer, retrying...")
                    # Small delay before retry
                    time.sleep(0.1 + random.random() * 0.1)
                    continue
                else:
                    logger.warning(f"Final attempt failed - could not extract answer after {max_retries} attempts")
                    return generated_text, None
                    
        except Exception as e:
            logger.error(f"Retry {attempt + 1}/{max_retries}: Model inference error: {e}")
            if attempt < max_retries - 1:
                time.sleep(0.2 + random.random() * 0.2)
                continue
            else:
                # Return error info after all retries exhausted
                return f"ERROR after {max_retries} attempts: {str(e)}", None
    
    return f"EXTRACTION_FAILED after {max_retries} attempts", None

def load_jsonl_dataset(filepath):
    """Loads dataset from a JSONL file."""
    try:
        data = []
        with open(filepath, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line:
                    data.append(json.loads(line))
        logger.info(f"Loaded {len(data)} items from {filepath}")
        return data
    except Exception as e:
        logger.error(f"Error loading data from {filepath}: {e}")
        return None

def get_ground_truth(item):
    """Returns the ground truth answer letter."""
    answer = item.get("answer", "")
    if isinstance(answer, str) and len(answer) == 1 and answer.upper() in "ABCDEFGHIJ":
        return answer.upper()
    
    # Fallback to answer_index
    answer_index = item.get("answer_index", -1)
    if isinstance(answer_index, int) and 0 <= answer_index <= 9:
        return chr(65 + answer_index)
    return None

def process_batch(model, tokenizer, batch_prompts, batch_indices):
    """Process a batch of prompts efficiently."""
    try:
        # Tokenize batch
        batch_inputs = tokenizer(
            batch_prompts, 
            return_tensors="pt", 
            padding=True, 
            truncation=True, 
            max_length=2048  # Longer context for complex questions
        ).to(DEVICE)
        
        with torch.no_grad():
            outputs = model.generate(
                **batch_inputs,
                max_new_tokens=MAX_NEW_TOKENS,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
                do_sample=False,
                temperature=0.0,
                top_p=1.0,
                output_scores=True,
                return_dict_in_generate=True
            )
        
        batch_results = []
        for i, (sequence, input_length) in enumerate(zip(outputs.sequences, batch_inputs['input_ids'].shape[1:])):
            # Decode only the generated part
            output_tokens = sequence[input_length:]
            generated_text = tokenizer.decode(output_tokens, skip_special_tokens=True).strip()
            
            # Decode the full sequence (prompt + generation)
            full_text = tokenizer.decode(sequence, skip_special_tokens=True).strip()
            
            extracted_answer = extract_answer_first_token(generated_text)
            
            batch_results.append({
                'index': batch_indices[i],
                'raw_output': generated_text,
                'full_generation': full_text,
                'extracted_answer': extracted_answer
            })
        
        return batch_results
        
    except Exception as e:
        logger.error(f"Batch processing error: {e}")
        # Fallback to individual processing
        individual_results = []
        for prompt, idx in zip(batch_prompts, batch_indices):
            try:
                inputs = tokenizer(prompt, return_tensors="pt", padding=False, truncation=True, max_length=4096).to(DEVICE)
                with torch.no_grad():
                    outputs = model.generate(
                        **inputs,
                        max_new_tokens=MAX_NEW_TOKENS,
                        pad_token_id=tokenizer.pad_token_id,
                        eos_token_id=tokenizer.eos_token_id,
                        do_sample=False,
                        temperature=0.0,
                    )
                # Decode only the generated part
                output_tokens = outputs[0][inputs['input_ids'].shape[1]:]
                generated_text = tokenizer.decode(output_tokens, skip_special_tokens=True).strip()
                
                # Decode the full sequence (prompt + generation)
                full_text = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()
                
                extracted_answer = extract_answer_first_token(generated_text)
                
                individual_results.append({
                    'index': idx,
                    'raw_output': generated_text,
                    'full_generation': full_text,
                    'extracted_answer': extracted_answer
                })
            except Exception as individual_error:
                logger.error(f"Individual processing error for index {idx}: {individual_error}")
                individual_results.append({
                    'index': idx,
                    'raw_output': f"ERROR: {str(individual_error)[:100]}",
                    'full_generation': f"ERROR: {str(individual_error)[:100]}",
                    'extracted_answer': None
                })
        
        return individual_results

# --- Evaluation Function ---
def evaluate_single_model_on_datasets(config: ModelConfig, mmlu_prox_en_data: list, mmlu_prox_ko_data: list, model_specific_output_dir: str):
    """
    Performs 5-shot MMLU-ProX evaluation for a single model on both English and Korean datasets.
    """
    results_filepath = os.path.join(model_specific_output_dir, f"results_{config.name}_5shot.json")
    log_filepath = os.path.join(model_specific_output_dir, f"eval_{config.name}_5shot.log")
    raw_gen_filepath = os.path.join(model_specific_output_dir, f"raw_generations_{config.name}_5shot.json")

    # Setup Logging
    file_handler = logging.FileHandler(log_filepath, mode='w', encoding='utf-8')
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - [%(name)s] - %(message)s')
    file_handler.setFormatter(formatter)
    root_logger = logging.getLogger()
    for handler in list(root_logger.handlers):
        if isinstance(handler, logging.FileHandler):
            root_logger.removeHandler(handler)
    root_logger.addHandler(file_handler)

    logger.info(f"--- Starting 5-shot MMLU-ProX Evaluation for Model: {config.name} ---")
    logger.info(f"Results will be saved to: {results_filepath}")

    model = None
    tokenizer = None
    
    try:
        # Load Model and Tokenizer
        tokenizer_load_path = config.adapter_path if config.adapter_path else config.model_id
        logger.info(f"Loading tokenizer from: {os.path.abspath(tokenizer_load_path)}")
        tokenizer = AutoTokenizer.from_pretrained(
                    tokenizer_load_path, 
                    cache_dir=CACHE_DIR,
                    padding_side='left',
                    trust_remote_code=True
                )
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token

        logger.info(f"Loading model {config.model_id}...")
        quantization_config_bnb = None
        if config.use_quantization:
            quantization_config_bnb = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=config.torch_dtype,
                bnb_4bit_use_double_quant=True,
            )

        model = AutoModelForCausalLM.from_pretrained(
            config.model_id,
            torch_dtype=config.torch_dtype,
            quantization_config=quantization_config_bnb,
            device_map=DEVICE,
            trust_remote_code=True,
            cache_dir=CACHE_DIR
        )

        if len(tokenizer) != model.get_input_embeddings().weight.shape[0]:
            model.resize_token_embeddings(len(tokenizer))

        if config.adapter_path:
            logger.info(f"Loading adapter from: {config.adapter_path}")
            model = PeftModel.from_pretrained(model, config.adapter_path)
            logger.info("Successfully loaded LoRA adapter.")

        model.eval()
        logger.info("Model and tokenizer loaded successfully.")

        # Prepare results storage
        all_results = {
            "mmlu_prox_en": {"correct": 0, "total": 0, "details": [], "raw_generations": []},
            "mmlu_prox_ko": {"correct": 0, "total": 0, "details": [], "raw_generations": []}
        }

        # Evaluate MMLU-ProX English
        logger.info("Starting MMLU-ProX English evaluation...")
        pbar_en = tqdm(range(0, len(mmlu_prox_en_data), BATCH_SIZE), desc="Evaluating MMLU-ProX English (errors: 0)")
        for i in pbar_en:
            batch_data = mmlu_prox_en_data[i:i+BATCH_SIZE]
            batch_prompts = []
            batch_indices = []
            batch_ground_truths = []
            
            for j, item in enumerate(batch_data):
                ground_truth = get_ground_truth(item)
                if ground_truth is None:
                    continue
                    
                prompt = create_5shot_prompt(item, ENGLISH_FEW_SHOT_EXAMPLES, "en")
                batch_prompts.append(prompt)
                batch_indices.append(i + j)
                batch_ground_truths.append(ground_truth)
            
            if not batch_prompts:
                continue
                
            batch_results = process_batch(model, tokenizer, batch_prompts, batch_indices)
            
            for result, ground_truth, batch_prompt in zip(batch_results, batch_ground_truths, batch_prompts):
                extracted_answer = result['extracted_answer']
                raw_output = result['raw_output']
                
                # Retry logic for failed extractions
                if not extracted_answer and not raw_output.startswith("ERROR"):
                    logger.warning(f"Batch extraction failed for English item {result['index']}, attempting individual retry...")
                    retry_text, retry_answer = process_single_with_retry(model, tokenizer, batch_prompt)
                    
                    if retry_answer is not None:
                        extracted_answer = retry_answer
                        raw_output = retry_text
                        logger.info(f"English retry successful for item {result['index']}: extracted '{retry_answer}'")
                    else:
                        # Even retry failed
                        if not retry_text.startswith("ERROR"):
                            logger.warning(f"English item {result['index']}: Failed to extract answer after retries")
                            raw_output = f"EXTRACTION_FAILED: {retry_text}"
                        else:
                            logger.error(f"English item {result['index']}: Model error: {retry_text}")
                            raw_output = retry_text
                
                is_correct = extracted_answer == ground_truth if extracted_answer else False
                
                # Only count items with valid extracted answers for total
                if extracted_answer:
                    all_results["mmlu_prox_en"]["total"] += 1
                    if is_correct:
                        all_results["mmlu_prox_en"]["correct"] += 1
                
                all_results["mmlu_prox_en"]["details"].append({
                    "index": result['index'],
                    "ground_truth": ground_truth,
                    "predicted_answer": extracted_answer,
                    "is_correct": is_correct,
                    "raw_output": raw_output
                })
                
                all_results["mmlu_prox_en"]["raw_generations"].append({
                    "index": result['index'],
                    "ground_truth": ground_truth,
                    "raw_output": raw_output,
                    "full_generation": raw_output,
                    "extracted_answer": extracted_answer
                })
            
            # Update progress bar with current error count
            current_en_errors = len(mmlu_prox_en_data[:i+BATCH_SIZE]) - all_results["mmlu_prox_en"]["total"]
            pbar_en.set_description(f"Evaluating MMLU-ProX English (errors: {current_en_errors})")

        logger.info(f"MMLU-ProX English evaluation completed: {all_results['mmlu_prox_en']['correct']}/{all_results['mmlu_prox_en']['total']}")

        # Evaluate MMLU-ProX Korean
        logger.info("Starting MMLU-ProX Korean evaluation...")
        pbar_ko = tqdm(range(0, len(mmlu_prox_ko_data), BATCH_SIZE), desc="Evaluating MMLU-ProX Korean (errors: 0)")
        for i in pbar_ko:
            batch_data = mmlu_prox_ko_data[i:i+BATCH_SIZE]
            batch_prompts = []
            batch_indices = []
            batch_ground_truths = []
            
            for j, item in enumerate(batch_data):
                ground_truth = get_ground_truth(item)
                if ground_truth is None:
                    continue
                    
                prompt = create_5shot_prompt(item, KOREAN_FEW_SHOT_EXAMPLES, "ko")
                batch_prompts.append(prompt)
                batch_indices.append(i + j)
                batch_ground_truths.append(ground_truth)
            
            if not batch_prompts:
                continue
                
            batch_results = process_batch(model, tokenizer, batch_prompts, batch_indices)
            
            for result, ground_truth, batch_prompt in zip(batch_results, batch_ground_truths, batch_prompts):
                extracted_answer = result['extracted_answer']
                raw_output = result['raw_output']
                
                # Retry logic for failed extractions
                if not extracted_answer and not raw_output.startswith("ERROR"):
                    logger.warning(f"Batch extraction failed for Korean item {result['index']}, attempting individual retry...")
                    retry_text, retry_answer = process_single_with_retry(model, tokenizer, batch_prompt)
                    
                    if retry_answer is not None:
                        extracted_answer = retry_answer
                        raw_output = retry_text
                        logger.info(f"Korean retry successful for item {result['index']}: extracted '{retry_answer}'")
                    else:
                        # Even retry failed
                        if not retry_text.startswith("ERROR"):
                            logger.warning(f"Korean item {result['index']}: Failed to extract answer after retries")
                            raw_output = f"EXTRACTION_FAILED: {retry_text}"
                        else:
                            logger.error(f"Korean item {result['index']}: Model error: {retry_text}")
                            raw_output = retry_text
                
                is_correct = extracted_answer == ground_truth if extracted_answer else False
                
                # Only count items with valid extracted answers for total
                if extracted_answer:
                    all_results["mmlu_prox_ko"]["total"] += 1
                    if is_correct:
                        all_results["mmlu_prox_ko"]["correct"] += 1
                
                all_results["mmlu_prox_ko"]["details"].append({
                    "index": result['index'],
                    "ground_truth": ground_truth,
                    "predicted_answer": extracted_answer,
                    "is_correct": is_correct,
                    "raw_output": raw_output
                })
                
                all_results["mmlu_prox_ko"]["raw_generations"].append({
                    "index": result['index'],
                    "ground_truth": ground_truth,
                    "raw_output": raw_output,
                    "full_generation": raw_output,
                    "extracted_answer": extracted_answer
                })
            
            # Update progress bar with current error count
            current_ko_errors = len(mmlu_prox_ko_data[:i+BATCH_SIZE]) - all_results["mmlu_prox_ko"]["total"]
            pbar_ko.set_description(f"Evaluating MMLU-ProX Korean (errors: {current_ko_errors})")

        logger.info(f"MMLU-ProX Korean evaluation completed: {all_results['mmlu_prox_ko']['correct']}/{all_results['mmlu_prox_ko']['total']}")

        # Calculate strict accuracies (including errors/skips)
        en_strict_accuracy = (all_results["mmlu_prox_en"]["correct"] / len(mmlu_prox_en_data) * 100) if len(mmlu_prox_en_data) > 0 else 0
        ko_strict_accuracy = (all_results["mmlu_prox_ko"]["correct"] / len(mmlu_prox_ko_data) * 100) if len(mmlu_prox_ko_data) > 0 else 0
        
        # Calculate error/skip counts
        en_errors_skipped = len(mmlu_prox_en_data) - all_results["mmlu_prox_en"]["total"]
        ko_errors_skipped = len(mmlu_prox_ko_data) - all_results["mmlu_prox_ko"]["total"]

        logger.info(f"--- Final Results for {config.name} ---")
        logger.info(f"MMLU-ProX English Strict Accuracy: {en_strict_accuracy:.2f}% ({all_results['mmlu_prox_en']['correct']}/{len(mmlu_prox_en_data)}) [Errors/Skipped: {en_errors_skipped}]")
        logger.info(f"MMLU-ProX Korean Strict Accuracy: {ko_strict_accuracy:.2f}% ({all_results['mmlu_prox_ko']['correct']}/{len(mmlu_prox_ko_data)}) [Errors/Skipped: {ko_errors_skipped}]")

        # Save Results
        final_summary = {
            "model_config": {k: str(v) for k, v in config.__dict__.items()},
            "evaluation_type": "5-shot MMLU-ProX",
            "evaluation_date": datetime.now().isoformat(),
            "mmlu_prox_en_results": {
                "accuracy_strict": en_strict_accuracy,
                "correct_predictions": all_results["mmlu_prox_en"]["correct"],
                "total_predictions": all_results["mmlu_prox_en"]["total"],
                "total_items": len(mmlu_prox_en_data),
                "errors_or_skipped": en_errors_skipped,
                "details": all_results["mmlu_prox_en"]["details"]
            },
            "mmlu_prox_ko_results": {
                "accuracy_strict": ko_strict_accuracy,
                "correct_predictions": all_results["mmlu_prox_ko"]["correct"],
                "total_predictions": all_results["mmlu_prox_ko"]["total"],
                "total_items": len(mmlu_prox_ko_data),
                "errors_or_skipped": ko_errors_skipped,
                "details": all_results["mmlu_prox_ko"]["details"]
            }
        }

        with open(results_filepath, 'w', encoding='utf-8') as f:
            json.dump(final_summary, f, indent=2, ensure_ascii=False)
        
        # Save raw generations
        raw_generations_summary = {
            "mmlu_prox_en": all_results["mmlu_prox_en"]["raw_generations"],
            "mmlu_prox_ko": all_results["mmlu_prox_ko"]["raw_generations"]
        }
        with open(raw_gen_filepath, 'w', encoding='utf-8') as f:
            json.dump(raw_generations_summary, f, indent=2, ensure_ascii=False)

        return {
            "model_name": config.name,
            "mmlu_prox_en_accuracy_strict": en_strict_accuracy,
            "mmlu_prox_ko_accuracy_strict": ko_strict_accuracy,
            "mmlu_prox_en_correct": all_results["mmlu_prox_en"]["correct"],
            "mmlu_prox_en_total": all_results["mmlu_prox_en"]["total"],
            "mmlu_prox_en_total_items": len(mmlu_prox_en_data),
            "mmlu_prox_en_errors_skipped": en_errors_skipped,
            "mmlu_prox_ko_correct": all_results["mmlu_prox_ko"]["correct"],
            "mmlu_prox_ko_total": all_results["mmlu_prox_ko"]["total"],
            "mmlu_prox_ko_total_items": len(mmlu_prox_ko_data),
            "mmlu_prox_ko_errors_skipped": ko_errors_skipped
        }

    except Exception as e:
        logger.exception(f"Critical error during evaluation for {config.name}: {e}")
        return {
            "model_name": config.name,
            "mmlu_prox_en_accuracy_strict": 0.0,
            "mmlu_prox_ko_accuracy_strict": 0.0,
            "mmlu_prox_en_correct": 0,
            "mmlu_prox_en_total": 0,
            "mmlu_prox_en_total_items": len(mmlu_prox_en_data) if 'mmlu_prox_en_data' in locals() else 0,
            "mmlu_prox_en_errors_skipped": len(mmlu_prox_en_data) if 'mmlu_prox_en_data' in locals() else 0,
            "mmlu_prox_ko_correct": 0,
            "mmlu_prox_ko_total": 0,
            "mmlu_prox_ko_total_items": len(mmlu_prox_ko_data) if 'mmlu_prox_ko_data' in locals() else 0,
            "mmlu_prox_ko_errors_skipped": len(mmlu_prox_ko_data) if 'mmlu_prox_ko_data' in locals() else 0,
            "error": str(e)
        }
    finally:
        del model, tokenizer
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        if 'file_handler' in locals() and file_handler in root_logger.handlers:
            root_logger.removeHandler(file_handler)
            file_handler.close()

def main():
    # Load datasets
    mmlu_prox_en_data = load_jsonl_dataset(MMLU_PROX_EN_DATASET_PATH)
    mmlu_prox_ko_data = load_jsonl_dataset(MMLU_PROX_KO_DATASET_PATH)
    
    if not mmlu_prox_en_data or not mmlu_prox_ko_data:
        logger.error("Failed to load datasets.")
        return

    os.makedirs(BASE_OUTPUT_DIR, exist_ok=True)
    
    # Store all model results for summary
    all_model_results = []

    # Evaluate each model
    for config in MODEL_CONFIGS:
        model_specific_output_dir = os.path.join(BASE_OUTPUT_DIR, config.name)
        os.makedirs(model_specific_output_dir, exist_ok=True)
        
        model_result = evaluate_single_model_on_datasets(config, mmlu_prox_en_data, mmlu_prox_ko_data, model_specific_output_dir)
        all_model_results.append(model_result)

    # Generate summary
    summary_data = {
        "evaluation_info": {
            "evaluation_type": "5-shot MMLU-ProX",
            "evaluation_date": datetime.now().isoformat(),
            "batch_size": BATCH_SIZE,
            "max_new_tokens": MAX_NEW_TOKENS,
            "total_mmlu_prox_en_items": len(mmlu_prox_en_data),
            "total_mmlu_prox_ko_items": len(mmlu_prox_ko_data),
            "note": "Only strict accuracy reported (includes errors/skips in total count)"
        },
        "model_results": all_model_results,
        "summary_statistics": {
            "best_mmlu_prox_en_model": max(all_model_results, key=lambda x: x.get("mmlu_prox_en_accuracy_strict", 0))["model_name"] if all_model_results else "N/A",
            "best_mmlu_prox_ko_model": max(all_model_results, key=lambda x: x.get("mmlu_prox_ko_accuracy_strict", 0))["model_name"] if all_model_results else "N/A",
            "average_mmlu_prox_en_accuracy_strict": sum(x.get("mmlu_prox_en_accuracy_strict", 0) for x in all_model_results) / len(all_model_results) if all_model_results else 0,
            "average_mmlu_prox_ko_accuracy_strict": sum(x.get("mmlu_prox_ko_accuracy_strict", 0) for x in all_model_results) / len(all_model_results) if all_model_results else 0
        }
    }

    # Enhanced summary with performance analysis
    if create_enhanced_summary:
        # Prepare model results for analysis
        model_results_for_analysis = []
        for result in all_model_results:
            if 'error' not in result:
                # Create combined accuracy metric for analysis
                en_accuracy = result.get('mmlu_prox_en_accuracy_strict', 0)
                ko_accuracy = result.get('mmlu_prox_ko_accuracy_strict', 0)
                combined_accuracy = (en_accuracy + ko_accuracy) / 2
                
                analysis_result = {
                    "model_name": result["model_name"],
                    "accuracy_strict": combined_accuracy,
                    "mmlu_prox_en_accuracy": en_accuracy,
                    "mmlu_prox_ko_accuracy": ko_accuracy,
                    "correct_predictions": result.get('mmlu_prox_en_correct', 0) + result.get('mmlu_prox_ko_correct', 0),
                    "total_items": result.get('mmlu_prox_en_total_items', 0) + result.get('mmlu_prox_ko_total_items', 0)
                }
                model_results_for_analysis.append(analysis_result)
        
        enhanced_summary = create_enhanced_summary(
            model_results=model_results_for_analysis,
            evaluation_info=summary_data["evaluation_info"],
            primary_metric="accuracy_strict",
            subject_metric=None  # MMLU_ProX doesn't have subject breakdown
        )
        
        # Merge with original summary data
        enhanced_summary["original_detailed_results"] = summary_data
        
        summary_filepath = os.path.join(BASE_OUTPUT_DIR, "SUMMARY.json")
        with open(summary_filepath, 'w', encoding='utf-8') as f:
            json.dump(enhanced_summary, f, indent=2, ensure_ascii=False)
            
        # Log key insights
        perf_analysis = enhanced_summary["performance_analysis"]
        logger.info(f"üèÜ Best performing model: {perf_analysis['best_model']}")
        logger.info(f"üìä Average combined accuracy: {perf_analysis['average_score']:.2f}%")
        logger.info(f"üìà Performance gap: {perf_analysis['performance_gap']:.2f}%p")
        
    else:
        # Fallback to basic summary
        summary_filepath = os.path.join(BASE_OUTPUT_DIR, "SUMMARY.json")
        with open(summary_filepath, 'w', encoding='utf-8') as f:
            json.dump(summary_data, f, indent=2, ensure_ascii=False)

    logger.info(f"Evaluation complete. Summary saved to: {summary_filepath}")
    logger.info("=== FINAL SUMMARY ===")
    for result in all_model_results:
        logger.info(f"{result['model_name']}:")
        logger.info(f"  MMLU-ProX EN Strict: {result.get('mmlu_prox_en_accuracy_strict', 0):.2f}%")
        logger.info(f"  MMLU-ProX KO Strict: {result.get('mmlu_prox_ko_accuracy_strict', 0):.2f}%")

if __name__ == "__main__":
    main()