"""
Performance Analysis Utilities for Evaluation Scripts
ê³µí†µìœ¼ë¡œ ì‚¬ìš©í•  ì„±ëŠ¥ ë¶„ì„ í•¨ìˆ˜ë“¤
"""

import json
from typing import Dict, List, Tuple, Any
import logging

logger = logging.getLogger(__name__)

def analyze_model_performance(model_results: List[Dict], metric_key: str = "accuracy_strict") -> Dict[str, Any]:
    """
    ëª¨ë¸ë³„ ì„±ëŠ¥ ë¶„ì„ì„ ìˆ˜í–‰í•˜ì—¬ Top3/Worst3ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    Args:
        model_results: ëª¨ë¸ë³„ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        metric_key: ë¹„êµí•  ë©”íŠ¸ë¦­ í‚¤ (ì˜ˆ: "accuracy_strict")
    
    Returns:
        ì„±ëŠ¥ ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    if not model_results:
        return {
            "top_3_models": [],
            "worst_3_models": [],
            "performance_gap": 0.0,
            "average_score": 0.0,
            "best_model": "N/A",
            "worst_model": "N/A"
        }
    
    # ìœ íš¨í•œ ê²°ê³¼ë§Œ í•„í„°ë§
    valid_results = []
    for result in model_results:
        if metric_key in result and isinstance(result[metric_key], (int, float)):
            valid_results.append(result)
        else:
            logger.warning(f"Missing or invalid {metric_key} for model {result.get('model_name', 'Unknown')}")
    
    if not valid_results:
        logger.error("No valid results found for performance analysis")
        return analyze_model_performance([], metric_key)
    
    # ì„±ëŠ¥ì— ë”°ë¼ ì •ë ¬
    sorted_results = sorted(valid_results, key=lambda x: x[metric_key], reverse=True)
    
    # Top 3ê³¼ Worst 3 ì¶”ì¶œ
    top_3 = sorted_results[:3]
    worst_3 = sorted_results[-3:]
    worst_3.reverse()  # ë‚®ì€ ìˆœì„œëŒ€ë¡œ ì •ë ¬
    
    # í†µê³„ ê³„ì‚°
    scores = [result[metric_key] for result in valid_results]
    average_score = sum(scores) / len(scores)
    performance_gap = max(scores) - min(scores)
    
    return {
        "top_3_models": [
            {
                "model_name": result["model_name"],
                "score": result[metric_key],
                "rank": idx + 1
            }
            for idx, result in enumerate(top_3)
        ],
        "worst_3_models": [
            {
                "model_name": result["model_name"], 
                "score": result[metric_key],
                "rank": len(valid_results) - idx
            }
            for idx, result in enumerate(worst_3)
        ],
        "performance_gap": performance_gap,
        "average_score": average_score,
        "best_model": sorted_results[0]["model_name"],
        "worst_model": sorted_results[-1]["model_name"],
        "total_models": len(valid_results)
    }

def analyze_subject_performance(model_results: List[Dict], subject_key: str = "subject_wise_accuracy") -> Dict[str, Any]:
    """
    ê³¼ëª©ë³„/ì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    
    Args:
        model_results: ëª¨ë¸ë³„ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        subject_key: ê³¼ëª©ë³„ ë°ì´í„° í‚¤
    
    Returns:
        ê³¼ëª©ë³„ ì„±ëŠ¥ ë¶„ì„ ê²°ê³¼
    """
    all_subjects = set()
    model_subject_scores = {}
    
    # ëª¨ë“  ê³¼ëª© ìˆ˜ì§‘ ë° ëª¨ë¸ë³„ ê³¼ëª© ì ìˆ˜ ì¶”ì¶œ
    for result in model_results:
        model_name = result.get("model_name", "Unknown")
        if subject_key in result and isinstance(result[subject_key], dict):
            model_subject_scores[model_name] = {}
            for subject, subject_data in result[subject_key].items():
                if isinstance(subject_data, dict) and "accuracy" in subject_data:
                    all_subjects.add(subject)
                    model_subject_scores[model_name][subject] = subject_data["accuracy"]
    
    if not all_subjects:
        return {
            "subject_analysis": {},
            "strongest_subjects": [],
            "weakest_subjects": [],
            "subject_performance_variance": {}
        }
    
    # ê³¼ëª©ë³„ í‰ê·  ì ìˆ˜ ê³„ì‚°
    subject_averages = {}
    subject_performance_variance = {}
    
    for subject in all_subjects:
        scores = []
        for model_name, subjects_data in model_subject_scores.items():
            if subject in subjects_data:
                scores.append(subjects_data[subject])
        
        if scores:
            avg_score = sum(scores) / len(scores)
            variance = sum((score - avg_score) ** 2 for score in scores) / len(scores)
            subject_averages[subject] = avg_score
            subject_performance_variance[subject] = {
                "variance": variance,
                "std_dev": variance ** 0.5,
                "min_score": min(scores),
                "max_score": max(scores),
                "score_range": max(scores) - min(scores)
            }
    
    # ê°€ì¥ ê°•í•œ/ì•½í•œ ê³¼ëª© TOP 3
    sorted_subjects = sorted(subject_averages.items(), key=lambda x: x[1], reverse=True)
    strongest_subjects = sorted_subjects[:3]
    weakest_subjects = sorted_subjects[-3:]
    weakest_subjects.reverse()
    
    return {
        "subject_analysis": subject_averages,
        "strongest_subjects": [
            {"subject": subject, "average_score": score}
            for subject, score in strongest_subjects
        ],
        "weakest_subjects": [
            {"subject": subject, "average_score": score}
            for subject, score in weakest_subjects
        ],
        "subject_performance_variance": subject_performance_variance
    }

def generate_model_insights(model_result: Dict, all_results: List[Dict], metric_key: str = "accuracy_strict") -> Dict[str, Any]:
    """
    ê°œë³„ ëª¨ë¸ì— ëŒ€í•œ ì¸ì‚¬ì´íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        model_result: ë¶„ì„í•  ëª¨ë¸ì˜ ê²°ê³¼
        all_results: ëª¨ë“  ëª¨ë¸ì˜ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        metric_key: ë¹„êµí•  ë©”íŠ¸ë¦­ í‚¤
    
    Returns:
        ëª¨ë¸ ì¸ì‚¬ì´íŠ¸ ë”•ì…”ë„ˆë¦¬
    """
    if not model_result or metric_key not in model_result:
        return {"insights": [], "relative_performance": "N/A"}
    
    model_score = model_result[metric_key]
    model_name = model_result.get("model_name", "Unknown")
    
    # ëª¨ë“  ëª¨ë¸ ì ìˆ˜ ìˆ˜ì§‘
    all_scores = []
    for result in all_results:
        if metric_key in result and isinstance(result[metric_key], (int, float)):
            all_scores.append(result[metric_key])
    
    if not all_scores:
        return {"insights": [], "relative_performance": "N/A"}
    
    # ìƒëŒ€ì  ì„±ëŠ¥ ê³„ì‚°
    average_score = sum(all_scores) / len(all_scores)
    percentile = (sum(1 for score in all_scores if score < model_score) / len(all_scores)) * 100
    
    # ì¸ì‚¬ì´íŠ¸ ìƒì„±
    insights = []
    
    if model_score >= max(all_scores):
        insights.append(f"ğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ({model_score:.2f}%)")
    elif model_score <= min(all_scores):
        insights.append(f"âš ï¸ ìµœì € ì„±ëŠ¥ ëª¨ë¸ ({model_score:.2f}%)")
    
    if model_score > average_score:
        diff = model_score - average_score
        insights.append(f"ğŸ“ˆ í‰ê· ë³´ë‹¤ {diff:.2f}%p ë†’ì€ ì„±ëŠ¥")
    elif model_score < average_score:
        diff = average_score - model_score
        insights.append(f"ğŸ“‰ í‰ê· ë³´ë‹¤ {diff:.2f}%p ë‚®ì€ ì„±ëŠ¥")
    
    performance_level = "ìƒìœ„" if percentile >= 66.7 else "ì¤‘ìœ„" if percentile >= 33.3 else "í•˜ìœ„"
    insights.append(f"ğŸ¯ ì „ì²´ ëª¨ë¸ ì¤‘ ìƒìœ„ {100-percentile:.1f}% ({performance_level} ê·¸ë£¹)")
    
    return {
        "insights": insights,
        "relative_performance": {
            "percentile": percentile,
            "performance_level": performance_level,
            "score": model_score,
            "average_score": average_score,
            "score_difference": model_score - average_score
        }
    }

def create_enhanced_summary(model_results: List[Dict], evaluation_info: Dict, 
                          primary_metric: str = "accuracy_strict",
                          subject_metric: str = "subject_wise_accuracy") -> Dict[str, Any]:
    """
    í–¥ìƒëœ summaryë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        model_results: ëª¨ë¸ë³„ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        evaluation_info: í‰ê°€ ì •ë³´
        primary_metric: ì£¼ìš” ë©”íŠ¸ë¦­ í‚¤
        subject_metric: ê³¼ëª©ë³„ ë©”íŠ¸ë¦­ í‚¤
    
    Returns:
        í–¥ìƒëœ summary ë”•ì…”ë„ˆë¦¬
    """
    # ê¸°ë³¸ ì„±ëŠ¥ ë¶„ì„
    performance_analysis = analyze_model_performance(model_results, primary_metric)
    
    # ê³¼ëª©ë³„ ì„±ëŠ¥ ë¶„ì„ (ìˆëŠ” ê²½ìš°)
    subject_analysis = analyze_subject_performance(model_results, subject_metric)
    
    # ê° ëª¨ë¸ë³„ ì¸ì‚¬ì´íŠ¸
    model_insights = {}
    for result in model_results:
        model_name = result.get("model_name", "Unknown")
        model_insights[model_name] = generate_model_insights(result, model_results, primary_metric)
    
    return {
        "evaluation_info": evaluation_info,
        "performance_analysis": performance_analysis,
        "subject_analysis": subject_analysis,
        "model_insights": model_insights,
        "model_results": model_results,
        "summary_statistics": {
            "total_models_evaluated": len(model_results),
            "evaluation_metric": primary_metric,
            "has_subject_breakdown": bool(subject_analysis["subject_analysis"]),
            "performance_range": {
                "highest": max((r.get(primary_metric, 0) for r in model_results), default=0),
                "lowest": min((r.get(primary_metric, 0) for r in model_results), default=0),
                "average": sum(r.get(primary_metric, 0) for r in model_results) / len(model_results) if model_results else 0
            }
        }
    }