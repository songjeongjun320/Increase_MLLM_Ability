#!/usr/bin/env python3
"""
ToW-Aware Korean Data Augmentation System
Advanced data augmentation for Korean ToW datasets with preservation of reasoning tokens
"""

import json
import random
import re
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass, field
import logging
from pathlib import Path
from collections import defaultdict
import numpy as np
from tqdm import tqdm

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class AugmentationConfig:
    """Configuration for ToW-aware augmentation"""
    # Input/Output paths
    input_file: str = "ToW_koconovel_complete.json"
    output_file: str = "ToW_koconovel_augmented.json"
    
    # Augmentation ratios (how many variants per original)
    honorific_variations: int = 2      # Í≤ΩÏñ¥Ï≤¥ Î≥ÄÌòï
    particle_substitutions: int = 2    # Ï°∞ÏÇ¨ Î≥ÄÌòï
    word_order_changes: int = 1        # Ïñ¥Ïàú Î≥ÄÍ≤Ω
    synonym_replacements: int = 2      # ÎèôÏùòÏñ¥ ÍµêÏ≤¥
    tense_variations: int = 1          # ÏãúÏ†ú Î≥ÄÌòï
    back_translation_aug: int = 1      # Ïó≠Î≤àÏó≠ Ï¶ùÍ∞ï
    
    # Quality thresholds
    semantic_similarity_threshold: float = 0.85
    preserve_tow_integrity: bool = True
    maintain_difficulty_markers: bool = True
    
    # Processing limits
    max_samples_per_category: int = 500  # Ïπ¥ÌÖåÍ≥†Î¶¨Î≥Ñ ÏµúÎåÄ ÏÉòÌîå Ïàò
    random_seed: int = 42

class ToWAugmentationEngine:
    """Main engine for ToW-aware Korean text augmentation"""
    
    def __init__(self, config: AugmentationConfig):
        self.config = config
        random.seed(config.random_seed)
        np.random.seed(config.random_seed)
        
        # Korean linguistic patterns for augmentation
        self.initialize_korean_patterns()
        
    def initialize_korean_patterns(self):
        """Initialize Korean language patterns for augmentation"""
        
        # Honorific transformations (formal <-> informal)
        self.honorific_patterns = {
            # Formal to informal endings
            'ÏäµÎãàÎã§': 'Ïñ¥Ïöî',
            'ÏäµÎãàÍπå': 'Ïñ¥Ïöî',
            'ÏÖ®ÏäµÎãàÎã§': 'ÏÖ®Ïñ¥Ïöî', 
            'ÌïòÏã≠ÎãàÎã§': 'Ìï¥Ïöî',
            'ÏûÖÎãàÎã§': 'ÏòàÏöî',
            'ÏòÄÏäµÎãàÎã§': 'ÏòÄÏñ¥Ïöî',
            'Ïù¥Ïã≠ÎãàÎã§': 'Ïù¥ÏóêÏöî',
            
            # Informal to formal (reverse mapping)
            'Ïñ¥Ïöî': 'ÏäµÎãàÎã§',
            'Ìï¥Ïöî': 'Ìï©ÎãàÎã§',
            'ÏòàÏöî': 'ÏûÖÎãàÎã§',
            'Ïù¥ÏóêÏöî': 'ÏûÖÎãàÎã§',
            'ÏÖ®Ïñ¥Ïöî': 'ÏÖ®ÏäµÎãàÎã§',
            'ÏòÄÏñ¥Ïöî': 'ÏòÄÏäµÎãàÎã§'
        }
        
        # Particle substitutions (maintaining grammatical validity)
        self.particle_substitutions = {
            'ÏùÄ': 'Îäî',     # topic particles
            'Îäî': 'ÏùÄ',
            'Ïù¥': 'Í∞Ä',     # subject particles  
            'Í∞Ä': 'Ïù¥',
            'ÏùÑ': 'Î•º',     # object particles
            'Î•º': 'ÏùÑ',
            'Ïóê': 'ÏóêÏÑú',   # location particles (context dependent)
            'ÏóêÍ≤å': 'ÌïúÌÖå', # to/for particles
            'ÌïúÌÖå': 'ÏóêÍ≤å',
            'ÏôÄ': 'Í≥º',     # and/with particles
            'Í≥º': 'ÏôÄ'
        }
        
        # Common Korean synonyms for replacement
        self.synonym_groups = {
            'ÏÇ¨Îûå': ['Ïù∏Í∞Ñ', 'Ïù¥', 'Ïûê'],
            'Ïó¨Ïûê': ['Ïó¨ÏÑ±', 'Í≥ÑÏßë', 'Î∂ÄÏù∏'],
            'ÎÇ®Ïûê': ['ÎÇ®ÏÑ±', 'ÏÇ¨ÎÇ¥', 'ÎÜà'],
            'Ïßë': ['Í∞ÄÏ†ï', 'ÎåÅ', 'Í∞ÄÏò•'],
            'ÌïôÍµê': ['ÌïôÎãπ', 'ÍµêÏú°Í∏∞Í¥Ä'],
            'ÏÑ†ÏÉù': ['ÍµêÏÇ¨', 'Ïä§Ïäπ', 'ÌõàÏû•'],
            'ÌïôÏÉù': ['Ï†úÏûê', 'ÏÉùÎèÑ'],
            'ÏïÑÏù¥': ['Ïñ¥Î¶∞Ïù¥', 'ÏÜåÏïÑ', 'Ïï†'],
            'Ïñ¥Î®∏Îãà': ['Î™®Ïπú', 'Ïñ¥ÎØ∏', 'ÏóÑÎßà'],
            'ÏïÑÎ≤ÑÏßÄ': ['Î∂ÄÏπú', 'ÏïÑÎπÑ', 'ÏïÑÎπ†'],
            'ÏïÑÎ¶ÑÎãµÎã§': ['Í≥±Îã§', 'ÏòàÏÅòÎã§', 'ÏïÑÎ¶¨Îî∞Îã§'],
            'ÌÅ¨Îã§': ['ÎåÄÎã®ÌïòÎã§', 'ÏõÖÎåÄÌïòÎã§'],
            'ÏûëÎã§': ['Ï°∞Í∑∏Îß£Îã§', 'ÏÜåÏÜåÌïòÎã§'],
            'Ï¢ãÎã§': ['ÌõåÎ•≠ÌïòÎã§', 'Í¥úÏ∞ÆÎã§', 'Î©ãÏßÄÎã§'],
            'ÎÇòÏÅòÎã§': ['Î™ªÎêòÎã§', 'ÏïÖÌïòÎã§', 'ÌòïÌé∏ÏóÜÎã§']
        }
        
        # Tense transformation patterns
        self.tense_patterns = {
            # Present to past
            'ÌïúÎã§': 'ÌñàÎã§',
            'Í∞ÑÎã§': 'Í∞îÎã§', 
            'Ïò®Îã§': 'ÏôîÎã§',
            'Î≥∏Îã§': 'Î¥§Îã§',
            'Î®πÎäîÎã§': 'Î®πÏóàÎã§',
            'ÎßàÏã†Îã§': 'ÎßàÏÖ®Îã§',
            
            # Past to present (reverse)
            'ÌñàÎã§': 'ÌïúÎã§',
            'Í∞îÎã§': 'Í∞ÑÎã§',
            'ÏôîÎã§': 'Ïò®Îã§', 
            'Î¥§Îã§': 'Î≥∏Îã§',
            'Î®πÏóàÎã§': 'Î®πÎäîÎã§',
            'ÎßàÏÖ®Îã§': 'ÎßàÏã†Îã§'
        }
        
        # Word order change patterns (Korean flexible word order)
        self.word_order_patterns = [
            # SOV -> OSV patterns
            (r'(\w+Ïù¥?\s+)(\w+ÏùÑ?\s+)(\w+Îã§)', r'\2\1\3'),
            # Topic fronting patterns
            (r'(\w+Îäî?\s+)(.+?)(\s+\w+Îã§)', r'\1\3 \2'),
        ]

    def extract_tow_tokens(self, text: str) -> List[Tuple[int, int, str]]:
        """Extract ToW token positions and content"""
        tow_tokens = []
        pattern = r'<ToW>(.*?)</ToW>'
        
        for match in re.finditer(pattern, text, re.DOTALL):
            start_pos = match.start()
            end_pos = match.end()
            content = match.group(1)
            tow_tokens.append((start_pos, end_pos, content))
            
        return tow_tokens
    
    def preserve_tow_tokens(self, original_text: str, modified_text: str, tow_tokens: List[Tuple[int, int, str]]) -> str:
        """Preserve ToW tokens in modified text"""
        if not tow_tokens:
            return modified_text
        
        # Simple preservation: re-insert ToW tokens at approximately same relative positions
        for start_pos, end_pos, content in reversed(tow_tokens):
            # Calculate relative position
            total_length = len(original_text)
            relative_pos = start_pos / total_length
            
            # Insert at relative position in modified text
            insert_pos = int(len(modified_text) * relative_pos)
            tow_token = f"<ToW>{content}</ToW>"
            
            # Insert the ToW token
            modified_text = modified_text[:insert_pos] + tow_token + modified_text[insert_pos:]
            
        return modified_text

    def apply_honorific_variation(self, text: str) -> List[str]:
        """Apply honorific level variations"""
        variants = []
        
        # Extract ToW tokens first
        tow_tokens = self.extract_tow_tokens(text)
        base_text = re.sub(r'<ToW>.*?</ToW>', '', text, flags=re.DOTALL)
        
        for _ in range(self.config.honorific_variations):
            modified = base_text
            
            # Apply random honorific transformations
            available_patterns = list(self.honorific_patterns.items())
            random.shuffle(available_patterns)
            
            for original, replacement in available_patterns[:3]:  # Apply up to 3 changes
                if original in modified:
                    modified = modified.replace(original, replacement)
                    break
            
            # Preserve ToW tokens
            if self.config.preserve_tow_integrity:
                modified = self.preserve_tow_tokens(text, modified, tow_tokens)
            
            if modified != text:  # Only add if different
                variants.append(modified)
                
        return variants
    
    def apply_particle_substitution(self, text: str) -> List[str]:
        """Apply particle substitution variations"""
        variants = []
        tow_tokens = self.extract_tow_tokens(text)
        base_text = re.sub(r'<ToW>.*?</ToW>', '', text, flags=re.DOTALL)
        
        for _ in range(self.config.particle_substitutions):
            modified = base_text
            
            # Apply random particle substitutions
            available_substitutions = list(self.particle_substitutions.items())
            random.shuffle(available_substitutions)
            
            for original, replacement in available_substitutions[:2]:  # Apply up to 2 changes
                if original in modified:
                    modified = modified.replace(original, replacement)
                    break
            
            if self.config.preserve_tow_integrity:
                modified = self.preserve_tow_tokens(text, modified, tow_tokens)
                
            if modified != text:
                variants.append(modified)
                
        return variants
    
    def apply_synonym_replacement(self, text: str) -> List[str]:
        """Apply synonym replacement with cultural context preservation"""
        variants = []
        tow_tokens = self.extract_tow_tokens(text)
        base_text = re.sub(r'<ToW>.*?</ToW>', '', text, flags=re.DOTALL)
        
        for _ in range(self.config.synonym_replacements):
            modified = base_text
            
            # Find applicable synonyms
            applicable_synonyms = []
            for word, synonyms in self.synonym_groups.items():
                if word in modified:
                    applicable_synonyms.append((word, synonyms))
            
            # Apply random synonym replacement
            if applicable_synonyms:
                word, synonyms = random.choice(applicable_synonyms)
                replacement = random.choice(synonyms)
                modified = modified.replace(word, replacement, 1)  # Replace only first occurrence
                
                if self.config.preserve_tow_integrity:
                    modified = self.preserve_tow_tokens(text, modified, tow_tokens)
                    
                if modified != text:
                    variants.append(modified)
                    
        return variants
    
    def apply_word_order_variation(self, text: str) -> List[str]:
        """Apply word order variations (Korean flexible syntax)"""
        variants = []
        tow_tokens = self.extract_tow_tokens(text)
        base_text = re.sub(r'<ToW>.*?</ToW>', '', text, flags=re.DOTALL)
        
        # Split into sentences for individual processing
        sentences = base_text.split('.')
        
        for _ in range(self.config.word_order_changes):
            modified_sentences = []
            
            for sentence in sentences:
                modified_sentence = sentence
                
                # Apply word order patterns
                for pattern, replacement in self.word_order_patterns:
                    if re.search(pattern, modified_sentence):
                        modified_sentence = re.sub(pattern, replacement, modified_sentence)
                        break
                
                modified_sentences.append(modified_sentence)
            
            modified = '.'.join(modified_sentences)
            
            if self.config.preserve_tow_integrity:
                modified = self.preserve_tow_tokens(text, modified, tow_tokens)
                
            if modified != text:
                variants.append(modified)
                
        return variants
    
    def apply_tense_variation(self, text: str) -> List[str]:
        """Apply tense variations"""
        variants = []
        tow_tokens = self.extract_tow_tokens(text)
        base_text = re.sub(r'<ToW>.*?</ToW>', '', text, flags=re.DOTALL)
        
        for _ in range(self.config.tense_variations):
            modified = base_text
            
            # Apply random tense transformations
            available_tenses = list(self.tense_patterns.items())
            random.shuffle(available_tenses)
            
            for original, replacement in available_tenses[:2]:  # Apply up to 2 changes
                if original in modified:
                    modified = modified.replace(original, replacement)
                    break
            
            if self.config.preserve_tow_integrity:
                modified = self.preserve_tow_tokens(text, modified, tow_tokens)
                
            if modified != text:
                variants.append(modified)
                
        return variants
    
    def generate_back_translation_augmentation(self, text: str) -> List[str]:
        """Simulate back-translation augmentation (rule-based approach)"""
        variants = []
        tow_tokens = self.extract_tow_tokens(text)
        base_text = re.sub(r'<ToW>.*?</ToW>', '', text, flags=re.DOTALL)
        
        # Simulate back-translation effects with Korean paraphrasing patterns
        paraphrase_patterns = [
            # Change word order slightly
            (r'(\w+)ÌïòÍ≥†\s+(\w+)', r'\2ÌïòÍ≥† \1'),
            # Change connector words
            ('Í∑∏Î¶¨Í≥†', 'ÎòêÌïú'),
            ('ÌïòÏßÄÎßå', 'Í∑∏Îü¨ÎÇò'),
            ('ÎïåÎ¨∏Ïóê', 'ÌÉìÏóê'),
            ('Í∞ôÏù¥', 'Ï≤òÎüº'),
            ('Îß§Ïö∞', 'ÏïÑÏ£º'),
            ('Ï†ïÎßê', 'Ï∞∏ÏúºÎ°ú'),
        ]
        
        for _ in range(self.config.back_translation_aug):
            modified = base_text
            
            # Apply paraphrasing patterns
            applied_changes = 0
            for pattern, replacement in paraphrase_patterns:
                if applied_changes >= 2:  # Limit changes
                    break
                    
                if isinstance(pattern, str) and pattern in modified:
                    modified = modified.replace(pattern, replacement)
                    applied_changes += 1
                elif hasattr(pattern, 'search') and pattern.search(modified):
                    modified = re.sub(pattern, replacement, modified)
                    applied_changes += 1
            
            if self.config.preserve_tow_integrity:
                modified = self.preserve_tow_tokens(text, modified, tow_tokens)
                
            if modified != text:
                variants.append(modified)
                
        return variants
    
    def create_augmentation_entry(self, original_entry: Dict, augmented_text: str, aug_type: str, variant_num: int) -> Dict:
        """Create new augmented entry maintaining ToW metadata"""
        new_entry = original_entry.copy()
        
        # Update identifiers
        new_entry['doc_id'] = f"{original_entry['doc_id']}_aug_{aug_type}_{variant_num}"
        new_entry['augmented_text'] = augmented_text
        
        # Recalculate ToW tokens
        tow_count = len(re.findall(r'<ToW>.*?</ToW>', augmented_text, re.DOTALL))
        new_entry['tow_count'] = tow_count
        
        # Extract ToW tokens for metadata
        tow_matches = re.findall(r'<ToW>(.*?)</ToW>', augmented_text, re.DOTALL)
        new_entry['tow_tokens'] = [f"<ToW>{match}</ToW>" for match in tow_matches]
        
        # Preserve other metadata
        if self.config.maintain_difficulty_markers:
            # Keep original difficulty markers as they should remain valid
            pass
        
        # Add augmentation metadata
        new_entry['augmentation_type'] = aug_type
        new_entry['original_doc_id'] = original_entry['doc_id']
        
        return new_entry
    
    def augment_dataset(self, input_file: str, output_file: str):
        """Main function to augment the entire dataset"""
        logger.info("Starting ToW-aware Korean data augmentation...")
        
        # Load original dataset
        logger.info(f"Loading dataset from {input_file}")
        with open(input_file, 'r', encoding='utf-8') as f:
            original_data = json.load(f)
        
        logger.info(f"Loaded {len(original_data)} original entries")
        
        # Filter entries with ToW tokens and sample by category
        tow_entries = [entry for entry in original_data if entry.get('tow_count', 0) > 0]
        logger.info(f"Found {len(tow_entries)} entries with ToW tokens")
        
        # Group by difficulty markers for balanced sampling
        entries_by_category = defaultdict(list)
        for entry in tow_entries:
            markers = entry.get('difficulty_markers', ['unknown'])
            category = markers[0] if markers else 'unknown'
            entries_by_category[category].append(entry)
        
        logger.info(f"Categories found: {list(entries_by_category.keys())}")
        
        # Sample entries for augmentation
        selected_entries = []
        for category, entries in entries_by_category.items():
            sample_size = min(len(entries), self.config.max_samples_per_category)
            sampled = random.sample(entries, sample_size)
            selected_entries.extend(sampled)
            logger.info(f"Sampled {len(sampled)} entries from {category} category")
        
        logger.info(f"Total selected for augmentation: {len(selected_entries)}")
        
        # Apply augmentations
        augmented_entries = []
        augmentation_stats = defaultdict(int)
        
        for entry in tqdm(selected_entries, desc="Augmenting entries"):
            original_text = entry['augmented_text']
            
            # Apply all augmentation techniques
            augmentation_methods = [
                ('honorific', self.apply_honorific_variation),
                ('particle', self.apply_particle_substitution),
                ('synonym', self.apply_synonym_replacement),
                ('word_order', self.apply_word_order_variation),
                ('tense', self.apply_tense_variation),
                ('back_trans', self.generate_back_translation_augmentation)
            ]
            
            for aug_type, aug_method in augmentation_methods:
                try:
                    variants = aug_method(original_text)
                    
                    for i, variant in enumerate(variants):
                        augmented_entry = self.create_augmentation_entry(
                            entry, variant, aug_type, i
                        )
                        augmented_entries.append(augmented_entry)
                        augmentation_stats[aug_type] += 1
                        
                except Exception as e:
                    logger.warning(f"Failed to apply {aug_type} augmentation: {e}")
        
        # Combine original and augmented data
        final_dataset = original_data + augmented_entries
        
        logger.info(f"Generated {len(augmented_entries)} augmented entries")
        logger.info(f"Final dataset size: {len(final_dataset)} entries")
        
        # Log augmentation statistics
        logger.info("Augmentation statistics:")
        for aug_type, count in augmentation_stats.items():
            logger.info(f"  {aug_type}: {count} variants generated")
        
        # Save augmented dataset
        logger.info(f"Saving augmented dataset to {output_file}")
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(final_dataset, f, ensure_ascii=False, indent=2)
        
        logger.info("‚úÖ Data augmentation completed successfully!")
        
        return {
            'original_entries': len(original_data),
            'augmented_entries': len(augmented_entries),
            'total_entries': len(final_dataset),
            'augmentation_stats': dict(augmentation_stats)
        }

def main():
    """Main function to run data augmentation"""
    logger.info("üöÄ ToW-Aware Korean Data Augmentation System")
    
    # Create configuration
    config = AugmentationConfig()
    
    # Check input file exists
    if not Path(config.input_file).exists():
        logger.error(f"Input file not found: {config.input_file}")
        logger.error("Please ensure ToW_koconovel_complete.json exists")
        return
    
    # Initialize augmentation engine
    engine = ToWAugmentationEngine(config)
    
    # Run augmentation
    try:
        results = engine.augment_dataset(config.input_file, config.output_file)
        
        # Print summary
        logger.info("\nüìä AUGMENTATION SUMMARY")
        logger.info("=" * 50)
        logger.info(f"Original entries: {results['original_entries']:,}")
        logger.info(f"Generated augmented entries: {results['augmented_entries']:,}")
        logger.info(f"Total final entries: {results['total_entries']:,}")
        logger.info(f"Augmentation ratio: {results['augmented_entries']/results['original_entries']:.2f}x")
        
        logger.info("\nAugmentation breakdown:")
        for aug_type, count in results['augmentation_stats'].items():
            logger.info(f"  üìù {aug_type}: {count:,} variants")
        
        logger.info(f"\n‚úÖ Augmented dataset saved as: {config.output_file}")
        
    except Exception as e:
        logger.error(f"‚ùå Augmentation failed: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()