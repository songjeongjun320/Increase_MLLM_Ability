#!/usr/bin/env python3
"""
ëŒ€ì•ˆì ì¸ GPT-OSS 120B ëª¨ë¸ ë¡œë”© ë°©ë²•ë“¤
ë‹¤ì–‘í•œ ì ‘ê·¼ë²•ìœ¼ë¡œ ëª¨ë¸ ë¡œë”© ì‹œë„
"""

import torch
import json
import os
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForCausalLM
import warnings
warnings.filterwarnings("ignore")

def method1_direct_loading(model_path):
    """ë°©ë²• 1: ì§ì ‘ ë¡œë”© (ê°€ì¥ ê¸°ë³¸)"""
    print("\n[ë°©ë²• 1] ì§ì ‘ ë¡œë”©")
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        model = AutoModelForCausalLM.from_pretrained(model_path)
        print("  âœ… ì„±ê³µ")
        return model, tokenizer
    except Exception as e:
        print(f"  âŒ ì‹¤íŒ¨: {e}")
        return None, None

def method2_no_trust_code(model_path):
    """ë°©ë²• 2: trust_remote_code=False"""
    print("\n[ë°©ë²• 2] trust_remote_code=False")
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=False)
        model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=False)
        print("  âœ… ì„±ê³µ")
        return model, tokenizer
    except Exception as e:
        print(f"  âŒ ì‹¤íŒ¨: {e}")
        return None, None

def method3_force_download(model_path):
    """ë°©ë²• 3: ê°•ì œ ë‹¤ìš´ë¡œë“œ"""
    print("\n[ë°©ë²• 3] ê°•ì œ ë‹¤ìš´ë¡œë“œ")
    try:
        tokenizer = AutoTokenizer.from_pretrained(
            model_path, 
            force_download=True,
            trust_remote_code=True
        )
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            force_download=True,
            trust_remote_code=True
        )
        print("  âœ… ì„±ê³µ")
        return model, tokenizer
    except Exception as e:
        print(f"  âŒ ì‹¤íŒ¨: {e}")
        return None, None

def method4_specific_revision(model_path):
    """ë°©ë²• 4: íŠ¹ì • revision ì‚¬ìš©"""
    print("\n[ë°©ë²• 4] íŠ¹ì • revision")
    revisions = ["main", "master", None]
    
    for revision in revisions:
        try:
            print(f"    revision: {revision}")
            tokenizer = AutoTokenizer.from_pretrained(
                model_path,
                revision=revision,
                trust_remote_code=True
            )
            model = AutoModelForCausalLM.from_pretrained(
                model_path,
                revision=revision,
                trust_remote_code=True
            )
            print("  âœ… ì„±ê³µ")
            return model, tokenizer
        except Exception as e:
            print(f"    âŒ revision {revision} ì‹¤íŒ¨: {e}")
            continue
    
    return None, None

def method5_manual_config(model_path):
    """ë°©ë²• 5: ìˆ˜ë™ config ìˆ˜ì •"""
    print("\n[ë°©ë²• 5] ìˆ˜ë™ config ìˆ˜ì •")
    
    config_path = Path(model_path) / "config.json"
    if not config_path.exists():
        print("  âŒ config.json íŒŒì¼ ì—†ìŒ")
        return None, None
    
    try:
        # config ë°±ì—… ë° ìˆ˜ì •
        with open(config_path, 'r') as f:
            config = json.load(f)
        
        backup_path = config_path.with_suffix('.json.backup')
        with open(backup_path, 'w') as f:
            json.dump(config, f, indent=2)
        
        # auto_map ì œê±° ì‹œë„
        if 'auto_map' in config:
            print("    auto_map ì œê±° ì‹œë„")
            config_modified = config.copy()
            del config_modified['auto_map']
            
            temp_config_path = config_path.with_suffix('.json.temp')
            with open(temp_config_path, 'w') as f:
                json.dump(config_modified, f, indent=2)
            
            # ì›ë³¸ íŒŒì¼ ëŒ€ì²´
            os.rename(temp_config_path, config_path)
            
            try:
                tokenizer = AutoTokenizer.from_pretrained(model_path)
                model = AutoModelForCausalLM.from_pretrained(model_path)
                print("  âœ… ì„±ê³µ (auto_map ì œê±°)")
                return model, tokenizer
            except Exception as e:
                print(f"  âŒ auto_map ì œê±° í›„ì—ë„ ì‹¤íŒ¨: {e}")
            finally:
                # ì›ë³¸ ë³µêµ¬
                os.rename(backup_path, config_path)
        
        return None, None
        
    except Exception as e:
        print(f"  âŒ config ìˆ˜ì • ì‹¤íŒ¨: {e}")
        return None, None

def method6_torch_load(model_path):
    """ë°©ë²• 6: torch.load ì§ì ‘ ì‚¬ìš©"""
    print("\n[ë°©ë²• 6] torch.load ì§ì ‘ ì‚¬ìš©")
    
    try:
        # safetensors íŒŒì¼ë“¤ ì°¾ê¸°
        safetensor_files = list(Path(model_path).glob("*.safetensors"))
        
        if not safetensor_files:
            print("  âŒ safetensors íŒŒì¼ ì—†ìŒ")
            return None, None
        
        print(f"    {len(safetensor_files)}ê°œì˜ safetensors íŒŒì¼ ë°œê²¬")
        
        # tokenizerë§Œ ë¡œë“œ
        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=False)
        print("  âœ… tokenizer ë¡œë“œ ì„±ê³µ")
        
        # ì´ ë°©ë²•ì€ ë³µì¡í•˜ë¯€ë¡œ ì¼ë‹¨ tokenizerë§Œ ë°˜í™˜
        return None, tokenizer
        
    except Exception as e:
        print(f"  âŒ ì‹¤íŒ¨: {e}")
        return None, None

def method7_transformers_legacy(model_path):
    """ë°©ë²• 7: ë ˆê±°ì‹œ transformers ë°©ì‹"""
    print("\n[ë°©ë²• 7] ë ˆê±°ì‹œ ë°©ì‹")
    
    try:
        # ë²„ì „ë³„ í˜¸í™˜ì„± ì‹œë„
        from transformers import GPTNeoXForCausalLM, GPTNeoXTokenizer
        
        try:
            tokenizer = GPTNeoXTokenizer.from_pretrained(model_path)
            model = GPTNeoXForCausalLM.from_pretrained(model_path)
            print("  âœ… GPTNeoXë¡œ ì„±ê³µ")
            return model, tokenizer
        except Exception as e:
            print(f"  âŒ GPTNeoX ì‹¤íŒ¨: {e}")
        
        # LLaMA ì‹œë„
        try:
            from transformers import LlamaForCausalLM, LlamaTokenizer
            tokenizer = LlamaTokenizer.from_pretrained(model_path)
            model = LlamaForCausalLM.from_pretrained(model_path)
            print("  âœ… Llamaë¡œ ì„±ê³µ")
            return model, tokenizer
        except Exception as e:
            print(f"  âŒ Llama ì‹¤íŒ¨: {e}")
        
        return None, None
        
    except ImportError as e:
        print(f"  âŒ ë ˆê±°ì‹œ ëª¨ë¸ import ì‹¤íŒ¨: {e}")
        return None, None

def method8_local_files_only(model_path):
    """ë°©ë²• 8: ë¡œì»¬ íŒŒì¼ë§Œ ì‚¬ìš©"""
    print("\n[ë°©ë²• 8] ë¡œì»¬ íŒŒì¼ë§Œ ì‚¬ìš©")
    
    try:
        tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            local_files_only=True,
            trust_remote_code=True
        )
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            local_files_only=True,
            trust_remote_code=True
        )
        print("  âœ… ì„±ê³µ")
        return model, tokenizer
    except Exception as e:
        print(f"  âŒ ì‹¤íŒ¨: {e}")
        return None, None

def method9_minimal_load(model_path):
    """ë°©ë²• 9: ìµœì†Œ ì„¤ì •ìœ¼ë¡œ ë¡œë“œ"""
    print("\n[ë°©ë²• 9] ìµœì†Œ ì„¤ì •")
    
    try:
        # ê°€ì¥ ê¸°ë³¸ì ì¸ ì„¤ì •ë§Œ ì‚¬ìš©
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float32,  # ê¸°ë³¸ íƒ€ì… ì‚¬ìš©
            device_map=None,  # device_map ì‚¬ìš© ì•ˆí•¨
        )
        print("  âœ… ì„±ê³µ")
        return model, tokenizer
    except Exception as e:
        print(f"  âŒ ì‹¤íŒ¨: {e}")
        return None, None

def test_all_methods(model_path):
    """ëª¨ë“  ë°©ë²•ì„ ìˆœì°¨ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸"""
    
    methods = [
        method1_direct_loading,
        method2_no_trust_code,
        method3_force_download,
        method4_specific_revision,
        method5_manual_config,
        method6_torch_load,
        method7_transformers_legacy,
        method8_local_files_only,
        method9_minimal_load,
    ]
    
    print("="*60)
    print("GPT-OSS 120B ëŒ€ì•ˆ ë¡œë”© ë°©ë²• í…ŒìŠ¤íŠ¸")
    print("="*60)
    
    for i, method in enumerate(methods, 1):
        try:
            model, tokenizer = method(model_path)
            if model is not None and tokenizer is not None:
                print(f"\nğŸ‰ ì„±ê³µ! ë°©ë²• {i}ë¡œ ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
                
                # ê°„ë‹¨í•œ ìƒì„± í…ŒìŠ¤íŠ¸
                try:
                    inputs = tokenizer("ì•ˆë…•í•˜ì„¸ìš”", return_tensors="pt")
                    with torch.no_grad():
                        outputs = model.generate(**inputs, max_new_tokens=5, do_sample=False)
                    result = tokenizer.decode(outputs[0], skip_special_tokens=True)
                    print(f"ìƒì„± í…ŒìŠ¤íŠ¸: {result}")
                except Exception as gen_e:
                    print(f"ìƒì„± í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {gen_e}")
                
                return model, tokenizer
            
        except Exception as e:
            print(f"\nğŸ’¥ ë°©ë²• {i} ì˜ˆì™¸ ë°œìƒ: {e}")
            continue
    
    print("\nâŒ ëª¨ë“  ë°©ë²• ì‹¤íŒ¨")
    return None, None

if __name__ == "__main__":
    model_path = "../1_models/gpt_oss/gpt-oss-120b"
    model, tokenizer = test_all_methods(model_path)