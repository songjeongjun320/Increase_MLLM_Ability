#!/usr/bin/env python3
"""
Enhanced Diversity Engine for Korean ToW Data Augmentation
ë‹¤ì–‘ì„± ì ìˆ˜ 39.5% â†’ 65%+ ë‹¬ì„±ì„ ìœ„í•œ ê³ ê¸‰ ì¦ê°• ì‹œìŠ¤í…œ
"""

import json
import re
import random
import numpy as np
from typing import Dict, List, Tuple, Set, Optional
from collections import defaultdict, Counter
import logging
from dataclasses import dataclass
import hashlib

logger = logging.getLogger(__name__)

@dataclass
class DiversityMetrics:
    """ë‹¤ì–‘ì„± ì¸¡ì • ì§€í‘œ"""
    lexical_diversity: float = 0.0    # ì–´íœ˜ ë‹¤ì–‘ì„±
    structural_diversity: float = 0.0  # êµ¬ì¡°ì  ë‹¤ì–‘ì„±
    semantic_diversity: float = 0.0    # ì˜ë¯¸ì  ë‹¤ì–‘ì„±
    length_diversity: float = 0.0      # ê¸¸ì´ ë‹¤ì–‘ì„±
    overall_diversity: float = 0.0     # ì¢…í•© ë‹¤ì–‘ì„±

class AdvancedDiversityEngine:
    """ê³ ê¸‰ ë‹¤ì–‘ì„± ì¦ê°• ì—”ì§„"""
    
    def __init__(self):
        self.initialize_diversity_patterns()
        self.diversity_cache = {}
        
    def initialize_diversity_patterns(self):
        """ë‹¤ì–‘ì„± ì¦ê°• íŒ¨í„´ ì´ˆê¸°í™”"""
        
        # 1. êµ¬ì¡°ì  ë³€í˜• íŒ¨í„´
        self.structural_patterns = {
            # ëŠ¥ë™-ìˆ˜ë™ ë³€í™˜
            'active_passive': [
                (r'(\w+)ê°€ (\w+)ì„ (\w+ë‹¤)', r'\2ì´ \1ì— ì˜í•´ \3'),
                (r'(\w+)ì´ (\w+)ë¥¼ (\w+ë‹¤)', r'\2ê°€ \1ì— ì˜í•´ \3'),
            ],
            
            # ë¬¸ì¥ ì¬êµ¬ì„± (ì£¼ì–´-ëª©ì ì–´ ìˆœì„œ ë³€ê²½)
            'sentence_restructure': [
                (r'(\w+ê°€) (.+) (\w+ì„|ë¥¼) (.+ë‹¤)', r'\3 \1 \2 \4'),
                (r'(\w+ì€|ëŠ”) (.+) (\w+ì„|ë¥¼) (.+ë‹¤)', r'\3 \1 \2 \4'),
            ],
            
            # ê´€í˜•ì ˆ ë³€í™˜
            'relative_clause': [
                (r'(\w+)í•œ (\w+)', r'\2ëŠ” \1í•˜ë‹¤'),
                (r'(\w+)ëœ (\w+)', r'\2ëŠ” \1ë˜ë‹¤'),
            ]
        }
        
        # 2. ë¬¸ì²´ì  ë³€í˜•
        self.style_variations = {
            'formal_to_casual': {
                'ìŠµë‹ˆë‹¤': ['ì–´ìš”', 'ì•„ìš”', 'ì—ìš”'],
                'í–ˆìŠµë‹ˆë‹¤': ['í–ˆì–´ìš”', 'í–ˆë„¤ìš”', 'í–ˆë‹µë‹ˆë‹¤'],
                'ì…ë‹ˆë‹¤': ['ì´ì—ìš”', 'ì˜ˆìš”', 'ì´ëë‹ˆë‹¤'],
                'ê²ƒì…ë‹ˆë‹¤': ['ê±°ì˜ˆìš”', 'ê²ƒì´ì—ìš”', 'ê²ë‹ˆë‹¤']
            },
            
            'declarative_to_interrogative': {
                'ë‹¤.': ['ê¹Œ?', 'ë‚˜?', 'ì§€?'],
                'ì–´ìš”.': ['ì–´ìš”?', 'ë‚˜ìš”?'],
                'ìŠµë‹ˆë‹¤.': ['ìŠµë‹ˆê¹Œ?', 'ë‚˜ìš”?']
            },
            
            'add_discourse_markers': [
                'ê·¸ëŸ°ë°', 'ê·¸ëŸ¬ë‹ˆê¹Œ', 'ì‚¬ì‹¤', 'ë¬¼ë¡ ', 'í•˜ì§€ë§Œ',
                'ê·¸ë˜ì„œ', 'ë”°ë¼ì„œ', 'ì¦‰', 'ì˜ˆë¥¼ ë“¤ì–´', 'í•œí¸'
            ]
        }
        
        # 3. ì˜ë¯¸ ë³´ì¡´ íŒ¨ëŸ¬í”„ë ˆì´ì¦ˆ
        self.paraphrase_patterns = {
            # ì—°ê²°ì–´êµ¬ ë‹¤ì–‘í™”
            'connectors': {
                'ê·¸ë¦¬ê³ ': ['ë˜í•œ', 'ë”ìš±ì´', 'ê²Œë‹¤ê°€', 'ì•„ìš¸ëŸ¬', 'í•œí¸'],
                'í•˜ì§€ë§Œ': ['ê·¸ëŸ¬ë‚˜', 'ë‹¤ë§Œ', 'ê·¸ëŸ°ë°', 'ë°˜ë©´ì—', 'ê·¸ë ‡ì§€ë§Œ'],
                'ê·¸ë˜ì„œ': ['ë”°ë¼ì„œ', 'ê·¸ëŸ¬ë¯€ë¡œ', 'ë•Œë¬¸ì—', 'ê·¸ë¦¬í•˜ì—¬', 'ì´ì—'],
                'ì™œëƒí•˜ë©´': ['ì´ëŠ”', 'ê·¸ ì´ìœ ëŠ”', '~ê¸° ë•Œë¬¸ì—', '~ìœ¼ë¡œ ì¸í•´']
            },
            
            # ê°•ì¡° í‘œí˜„ ë‹¤ì–‘í™”
            'emphasis': {
                'ë§¤ìš°': ['ì•„ì£¼', 'ë¬´ì²™', 'ìƒë‹¹íˆ', 'ëŒ€ë‹¨íˆ', 'ê½¤', 'ì œë²•'],
                'ì •ë§': ['ì°¸ìœ¼ë¡œ', 'ì§„ì§œë¡œ', 'ì‹¤ë¡œ', 'ê³¼ì—°', 'ì •ë…•'],
                'í•­ìƒ': ['ì–¸ì œë‚˜', 'ëŠ˜', 'ê³„ì†', 'ì§€ì†ì ìœ¼ë¡œ', 'ëŠì„ì—†ì´']
            },
            
            # ê´€ìš©ì  í‘œí˜„ ë³€í˜•
            'idiomatic': {
                'ì¤‘ìš”í•˜ë‹¤': ['í•µì‹¬ì ì´ë‹¤', 'í•„ìˆ˜ì ì´ë‹¤', 'ê²°ì •ì ì´ë‹¤', 'ì£¼ìš”í•˜ë‹¤'],
                'ì–´ë µë‹¤': ['í˜ë“¤ë‹¤', 'ê³¤ë€í•˜ë‹¤', 'ë²„ê²ë‹¤', 'ë‚œí•´í•˜ë‹¤'],
                'ì‰½ë‹¤': ['ê°„ë‹¨í•˜ë‹¤', 'ìš©ì´í•˜ë‹¤', 'ì†ì‰½ë‹¤', 'ìˆ˜ì›”í•˜ë‹¤']
            }
        }
        
        # 4. ê¸¸ì´ ë‹¤ì–‘ì„±ì„ ìœ„í•œ í™•ì¥/ì¶•ì•½ íŒ¨í„´
        self.length_patterns = {
            'expansion': {
                # ë‹¨ë¬¸ì„ ë³µë¬¸ìœ¼ë¡œ
                'simple_to_complex': [
                    (r'(\w+ë‹¤)\. (\w+ë‹¤)', r'\1ê³ , \2'),
                    (r'(\w+ë‹¤)\. (ê·¸\w+)', r'\1ë©°, \2'),
                ],
                # ë¶€ì—°ì„¤ëª… ì¶”ê°€
                'add_explanation': [
                    'ì¦‰, ', 'ë‹¤ì‹œ ë§í•´, ', 'êµ¬ì²´ì ìœ¼ë¡œ, ', 'ì˜ˆë¥¼ ë“¤ì–´, '
                ]
            },
            
            'compression': {
                # ë³µë¬¸ì„ ë‹¨ë¬¸ìœ¼ë¡œ
                'complex_to_simple': [
                    (r'(\w+)í•˜ê³ , (\w+ë‹¤)', r'\1í•˜ì—¬ \2'),
                    (r'(\w+)ë©°, (\w+ë‹¤)', r'\1í•˜ê³  \2'),
                ],
                # ë¶ˆí•„ìš”í•œ ìˆ˜ì‹ì–´ ì œê±°
                'remove_modifiers': ['ë§¤ìš°', 'ì •ë§', 'ì•„ì£¼', 'ê½¤', 'ìƒë‹¹íˆ']
            }
        }
    
    def calculate_text_signature(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì‹œê·¸ë‹ˆì²˜ ìƒì„± (ë‹¤ì–‘ì„± ì¸¡ì •ìš©)"""
        # ì˜ë¯¸ í•µì‹¬ ë‹¨ì–´ë§Œ ì¶”ì¶œ
        core_words = re.findall(r'\b\w{2,}\b', text)
        # ì¡°ì‚¬ì™€ ì–´ë¯¸ ì œê±°ëœ í•µì‹¬ ë‹¨ì–´ë“¤ë¡œ ì‹œê·¸ë‹ˆì²˜ ìƒì„±
        signature = ''.join(sorted(core_words[:20]))  # ìƒìœ„ 20ê°œ ë‹¨ì–´
        return hashlib.md5(signature.encode()).hexdigest()[:8]
    
    def measure_lexical_diversity(self, texts: List[str]) -> float:
        """ì–´íœ˜ ë‹¤ì–‘ì„± ì¸¡ì •"""
        all_words = []
        for text in texts:
            # ToW í† í° ì œì™¸í•˜ê³  ë‹¨ì–´ ì¶”ì¶œ
            clean_text = re.sub(r'<ToW>.*?</ToW>', '', text, flags=re.DOTALL)
            words = re.findall(r'\b\w{2,}\b', clean_text)
            all_words.extend(words)
        
        if not all_words:
            return 0.0
        
        unique_words = len(set(all_words))
        total_words = len(all_words)
        return (unique_words / total_words) * 100
    
    def measure_structural_diversity(self, texts: List[str]) -> float:
        """êµ¬ì¡°ì  ë‹¤ì–‘ì„± ì¸¡ì •"""
        structural_patterns = []
        
        for text in texts:
            clean_text = re.sub(r'<ToW>.*?</ToW>', '', text, flags=re.DOTALL)
            
            # ë¬¸ì¥ êµ¬ì¡° íŒ¨í„´ ì¶”ì¶œ
            patterns = []
            
            # ì£¼ì–´-ì„œìˆ ì–´ íŒ¨í„´
            if re.search(r'\w+ì´ \w+ë‹¤', clean_text):
                patterns.append('subject_predicate')
            if re.search(r'\w+ê°€ \w+ë‹¤', clean_text):
                patterns.append('subject_predicate_casual')
            
            # ëª©ì ì–´ í¬í•¨ íŒ¨í„´
            if re.search(r'\w+ì„ \w+ë‹¤', clean_text):
                patterns.append('object_verb')
            if re.search(r'\w+ë¥¼ \w+ë‹¤', clean_text):
                patterns.append('object_verb_casual')
            
            # ì—°ê²°ì–´ë¯¸ íŒ¨í„´
            if re.search(r'\w+ê³ ', clean_text):
                patterns.append('connective_go')
            if re.search(r'\w+ë©°', clean_text):
                patterns.append('connective_myeo')
            
            structural_patterns.append(tuple(sorted(patterns)))
        
        unique_patterns = len(set(structural_patterns))
        total_patterns = len(structural_patterns)
        
        return (unique_patterns / max(total_patterns, 1)) * 100
    
    def measure_semantic_diversity(self, texts: List[str]) -> float:
        """ì˜ë¯¸ì  ë‹¤ì–‘ì„± ì¸¡ì • (í…ìŠ¤íŠ¸ ì‹œê·¸ë‹ˆì²˜ ê¸°ë°˜)"""
        signatures = []
        for text in texts:
            sig = self.calculate_text_signature(text)
            signatures.append(sig)
        
        unique_signatures = len(set(signatures))
        total_signatures = len(signatures)
        
        return (unique_signatures / max(total_signatures, 1)) * 100
    
    def measure_length_diversity(self, texts: List[str]) -> float:
        """ê¸¸ì´ ë‹¤ì–‘ì„± ì¸¡ì •"""
        lengths = [len(text) for text in texts]
        if not lengths:
            return 0.0
        
        # í‘œì¤€í¸ì°¨ ê¸°ë°˜ ë‹¤ì–‘ì„± ì ìˆ˜
        mean_length = np.mean(lengths)
        std_length = np.std(lengths)
        
        # ìƒëŒ€ì  í‘œì¤€í¸ì°¨ë¥¼ ë°±ë¶„ìœ¨ë¡œ ë³€í™˜
        coefficient_of_variation = (std_length / max(mean_length, 1)) * 100
        
        # 0-100 ë²”ìœ„ë¡œ ì •ê·œí™”
        return min(coefficient_of_variation, 100.0)
    
    def calculate_overall_diversity(self, metrics: DiversityMetrics) -> float:
        """ì¢…í•© ë‹¤ì–‘ì„± ì ìˆ˜ ê³„ì‚°"""
        weights = {
            'lexical': 0.25,
            'structural': 0.30,
            'semantic': 0.35,
            'length': 0.10
        }
        
        weighted_score = (
            metrics.lexical_diversity * weights['lexical'] +
            metrics.structural_diversity * weights['structural'] +
            metrics.semantic_diversity * weights['semantic'] +
            metrics.length_diversity * weights['length']
        )
        
        return weighted_score
    
    def apply_structural_variations(self, text: str) -> List[str]:
        """êµ¬ì¡°ì  ë³€í˜• ì ìš©"""
        variants = []
        
        # ToW í† í° ë³´ì¡´
        tow_pattern = r'<ToW>(.*?)</ToW>'
        tow_tokens = [(m.start(), m.end(), m.group(1)) for m in re.finditer(tow_pattern, text, re.DOTALL)]
        base_text = re.sub(tow_pattern, '', text, flags=re.DOTALL)
        
        # ê° êµ¬ì¡°ì  íŒ¨í„´ ì ìš©
        for pattern_type, patterns in self.structural_patterns.items():
            for pattern, replacement in patterns:
                if re.search(pattern, base_text):
                    modified = re.sub(pattern, replacement, base_text, count=1)
                    
                    # ToW í† í° ì¬ì‚½ì…
                    for start_pos, end_pos, content in reversed(tow_tokens):
                        relative_pos = start_pos / len(text)
                        insert_pos = int(len(modified) * relative_pos)
                        tow_token = f"<ToW>{content}</ToW>"
                        modified = modified[:insert_pos] + tow_token + modified[insert_pos:]
                    
                    if modified != text and modified not in variants:
                        variants.append(modified)
                        break
        
        return variants
    
    def apply_style_variations(self, text: str) -> List[str]:
        """ë¬¸ì²´ì  ë³€í˜• ì ìš©"""
        variants = []
        
        # ToW í† í° ë³´ì¡´
        tow_pattern = r'<ToW>(.*?)</ToW>'
        tow_tokens = [(m.start(), m.end(), m.group(1)) for m in re.finditer(tow_pattern, text, re.DOTALL)]
        base_text = re.sub(tow_pattern, '', text, flags=re.DOTALL)
        
        # ê²©ì‹ì²´ â†’ ë¹„ê²©ì‹ì²´
        modified = base_text
        for formal, casuals in self.style_variations['formal_to_casual'].items():
            if formal in modified:
                casual = random.choice(casuals)
                modified = modified.replace(formal, casual, 1)
                break
        
        # í‰ì„œë¬¸ â†’ ì˜ë¬¸ë¬¸
        if modified == base_text:  # ì´ì „ì— ë³€ê²½ë˜ì§€ ì•Šì•˜ë‹¤ë©´
            for decl, interr_list in self.style_variations['declarative_to_interrogative'].items():
                if decl in modified:
                    interr = random.choice(interr_list)
                    modified = modified.replace(decl, interr, 1)
                    break
        
        # ë‹´í™” í‘œì§€ì–´ ì¶”ê°€
        if modified == base_text:  # ì´ì „ì— ë³€ê²½ë˜ì§€ ì•Šì•˜ë‹¤ë©´
            marker = random.choice(self.style_variations['add_discourse_markers'])
            sentences = modified.split('.')
            if len(sentences) > 1:
                # ë‘ ë²ˆì§¸ ë¬¸ì¥ ì•ì— ë‹´í™” í‘œì§€ì–´ ì¶”ê°€
                sentences[1] = f" {marker} " + sentences[1].strip()
                modified = '.'.join(sentences)
        
        # ToW í† í° ì¬ì‚½ì…
        if modified != base_text:
            for start_pos, end_pos, content in reversed(tow_tokens):
                relative_pos = start_pos / len(text)
                insert_pos = int(len(modified) * relative_pos)
                tow_token = f"<ToW>{content}</ToW>"
                modified = modified[:insert_pos] + tow_token + modified[insert_pos:]
            
            variants.append(modified)
        
        return variants
    
    def apply_semantic_paraphrasing(self, text: str) -> List[str]:
        """ì˜ë¯¸ ë³´ì¡´ íŒ¨ëŸ¬í”„ë ˆì´ì¦ˆ ì ìš©"""
        variants = []
        
        # ToW í† í° ë³´ì¡´
        tow_pattern = r'<ToW>(.*?)</ToW>'
        tow_tokens = [(m.start(), m.end(), m.group(1)) for m in re.finditer(tow_pattern, text, re.DOTALL)]
        base_text = re.sub(tow_pattern, '', text, flags=re.DOTALL)
        
        # ì—°ê²°ì–´êµ¬ ë‹¤ì–‘í™”
        modified = base_text
        for category, replacements in self.paraphrase_patterns.items():
            for original, alternatives in replacements.items():
                if original in modified:
                    alternative = random.choice(alternatives)
                    modified = modified.replace(original, alternative, 1)
                    break
            if modified != base_text:
                break
        
        # ToW í† í° ì¬ì‚½ì…
        if modified != base_text:
            for start_pos, end_pos, content in reversed(tow_tokens):
                relative_pos = start_pos / len(text)
                insert_pos = int(len(modified) * relative_pos)
                tow_token = f"<ToW>{content}</ToW>"
                modified = modified[:insert_pos] + tow_token + modified[insert_pos:]
            
            variants.append(modified)
        
        return variants
    
    def apply_length_variations(self, text: str) -> List[str]:
        """ê¸¸ì´ ë‹¤ì–‘ì„± ë³€í˜• ì ìš©"""
        variants = []
        
        # ToW í† í° ë³´ì¡´
        tow_pattern = r'<ToW>(.*?)</ToW>'
        tow_tokens = [(m.start(), m.end(), m.group(1)) for m in re.finditer(tow_pattern, text, re.DOTALL)]
        base_text = re.sub(tow_pattern, '', text, flags=re.DOTALL)
        
        # í™•ì¥ ë³€í˜• (50% í™•ë¥ )
        if random.random() < 0.5:
            modified = base_text
            
            # ë‹¨ë¬¸ì„ ë³µë¬¸ìœ¼ë¡œ
            for pattern, replacement in self.length_patterns['expansion']['simple_to_complex']:
                if re.search(pattern, modified):
                    modified = re.sub(pattern, replacement, modified, count=1)
                    break
            
            # ë¶€ì—°ì„¤ëª… ì¶”ê°€
            if modified == base_text:
                explanation = random.choice(self.length_patterns['expansion']['add_explanation'])
                sentences = modified.split('.')
                if len(sentences) > 1:
                    sentences[1] = f" {explanation}" + sentences[1].strip()
                    modified = '.'.join(sentences)
        
        # ì¶•ì•½ ë³€í˜• (50% í™•ë¥ )
        else:
            modified = base_text
            
            # ë³µë¬¸ì„ ë‹¨ë¬¸ìœ¼ë¡œ
            for pattern, replacement in self.length_patterns['compression']['complex_to_simple']:
                if re.search(pattern, modified):
                    modified = re.sub(pattern, replacement, modified, count=1)
                    break
            
            # ìˆ˜ì‹ì–´ ì œê±°
            if modified == base_text:
                for modifier in self.length_patterns['compression']['remove_modifiers']:
                    if modifier in modified:
                        modified = modified.replace(f" {modifier} ", " ", 1)
                        break
        
        # ToW í† í° ì¬ì‚½ì…
        if modified != base_text:
            for start_pos, end_pos, content in reversed(tow_tokens):
                relative_pos = start_pos / len(text)
                insert_pos = int(len(modified) * relative_pos)
                tow_token = f"<ToW>{content}</ToW>"
                modified = modified[:insert_pos] + tow_token + modified[insert_pos:]
            
            variants.append(modified)
        
        return variants
    
    def generate_high_diversity_variants(self, text: str, target_count: int = 5) -> List[str]:
        """ê³ ë‹¤ì–‘ì„± ë³€í˜• ìƒì„±"""
        all_variants = []
        
        # ê° ë³€í˜• ê¸°ë²• ì ìš©
        structural_variants = self.apply_structural_variations(text)
        style_variants = self.apply_style_variations(text)
        semantic_variants = self.apply_semantic_paraphrasing(text)
        length_variants = self.apply_length_variations(text)
        
        # ëª¨ë“  ë³€í˜• ìˆ˜ì§‘
        all_variants.extend(structural_variants)
        all_variants.extend(style_variants)
        all_variants.extend(semantic_variants)
        all_variants.extend(length_variants)
        
        # ì¤‘ë³µ ì œê±° (ì˜ë¯¸ ì‹œê·¸ë‹ˆì²˜ ê¸°ë°˜)
        unique_variants = []
        seen_signatures = {self.calculate_text_signature(text)}
        
        for variant in all_variants:
            signature = self.calculate_text_signature(variant)
            if signature not in seen_signatures:
                unique_variants.append(variant)
                seen_signatures.add(signature)
        
        # ëª©í‘œ ê°œìˆ˜ë§Œí¼ ë°˜í™˜
        return unique_variants[:target_count]
    
    def measure_diversity_improvement(self, original_variants: List[str], enhanced_variants: List[str]) -> Dict:
        """ë‹¤ì–‘ì„± ê°œì„  íš¨ê³¼ ì¸¡ì •"""
        
        # ì›ë³¸ ë‹¤ì–‘ì„± ì¸¡ì •
        original_metrics = DiversityMetrics(
            lexical_diversity=self.measure_lexical_diversity(original_variants),
            structural_diversity=self.measure_structural_diversity(original_variants),
            semantic_diversity=self.measure_semantic_diversity(original_variants),
            length_diversity=self.measure_length_diversity(original_variants)
        )
        original_metrics.overall_diversity = self.calculate_overall_diversity(original_metrics)
        
        # ê°œì„ ëœ ë‹¤ì–‘ì„± ì¸¡ì •
        enhanced_metrics = DiversityMetrics(
            lexical_diversity=self.measure_lexical_diversity(enhanced_variants),
            structural_diversity=self.measure_structural_diversity(enhanced_variants),
            semantic_diversity=self.measure_semantic_diversity(enhanced_variants),
            length_diversity=self.measure_length_diversity(enhanced_variants)
        )
        enhanced_metrics.overall_diversity = self.calculate_overall_diversity(enhanced_metrics)
        
        # ê°œì„ ìœ¨ ê³„ì‚°
        improvement = {
            'original_diversity': original_metrics.overall_diversity,
            'enhanced_diversity': enhanced_metrics.overall_diversity,
            'improvement_rate': enhanced_metrics.overall_diversity - original_metrics.overall_diversity,
            'detailed_improvements': {
                'lexical': enhanced_metrics.lexical_diversity - original_metrics.lexical_diversity,
                'structural': enhanced_metrics.structural_diversity - original_metrics.structural_diversity,
                'semantic': enhanced_metrics.semantic_diversity - original_metrics.semantic_diversity,
                'length': enhanced_metrics.length_diversity - original_metrics.length_diversity
            }
        }
        
        return improvement

def main():
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    logging.basicConfig(level=logging.INFO)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    test_text = """í•™êµì—ì„œ <ToW>This requires understanding Korean honorific complexity</ToW> ì„ ìƒë‹˜ì´ í•™ìƒë“¤ì„ ê°€ë¥´ì¹œë‹¤. 
    í•™ìƒë“¤ì€ ë§¤ìš° ì—´ì‹¬íˆ ê³µë¶€í•œë‹¤. ê·¸ë¦¬ê³  ì‹œí—˜ì„ ì˜ ë³¸ë‹¤."""
    
    engine = AdvancedDiversityEngine()
    
    print("ğŸŒˆ Enhanced Diversity Engine í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    print(f"ì›ë³¸ í…ìŠ¤íŠ¸: {test_text[:100]}...")
    
    # ê³ ë‹¤ì–‘ì„± ë³€í˜• ìƒì„±
    variants = engine.generate_high_diversity_variants(test_text, 5)
    
    print(f"\nìƒì„±ëœ ë³€í˜• {len(variants)}ê°œ:")
    for i, variant in enumerate(variants, 1):
        print(f"{i}. {variant[:100]}...")
    
    # ë‹¤ì–‘ì„± ì¸¡ì •
    original_variants = [test_text] * 10  # ì‹œë®¬ë ˆì´ì…˜
    enhanced_variants = variants + [test_text]
    
    improvement = engine.measure_diversity_improvement(original_variants, enhanced_variants)
    
    print(f"\nğŸ“Š ë‹¤ì–‘ì„± ê°œì„  ê²°ê³¼:")
    print(f"ì›ë³¸ ë‹¤ì–‘ì„±: {improvement['original_diversity']:.1f}%")
    print(f"ê°œì„ ëœ ë‹¤ì–‘ì„±: {improvement['enhanced_diversity']:.1f}%")
    print(f"ê°œì„ ìœ¨: +{improvement['improvement_rate']:.1f}%")
    
    for aspect, improvement_val in improvement['detailed_improvements'].items():
        print(f"  {aspect}: +{improvement_val:.1f}%")

if __name__ == "__main__":
    main()