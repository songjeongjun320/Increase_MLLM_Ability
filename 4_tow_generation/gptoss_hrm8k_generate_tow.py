#!/usr/bin/env python3
"""
gptoss_generate_tow.py

GPT-OSS 120B ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ 'context'ì™€ 'gold_label'ì„ ë°”íƒ•ìœ¼ë¡œ
ToW(Thought-of-Word) ì„¤ëª…ì„ ìƒì„±í•©ë‹ˆë‹¤.

gold_word.pyì—ì„œ ìƒì„±ëœ ë°ì´í„°ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ìŠµë‹ˆë‹¤.
"""
import json
import os
from tqdm import tqdm
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# --- ì„¤ì • (Configuration) ---
MODEL_PATH = "../1_models/gpt-oss-120b"
INPUT_JSON_PATH = "./gold_labels/hrm8k_gold_labels_gptoss120b.json"
OUTPUT_JSON_PATH = "./tow_data/hrm8k_tow_gptoss120b.json"

# Multi-GPU ì„¤ì •
NUM_GPUS = torch.cuda.device_count()
DEVICES = [f"cuda:{i}" for i in range(NUM_GPUS)] if NUM_GPUS > 0 else ["cpu"]

# ë°°ì¹˜ ì²˜ë¦¬ ì„¤ì •
BATCH_SIZE = 1  # GPT-OSS 120BëŠ” í° ëª¨ë¸ì´ë¯€ë¡œ ë°°ì¹˜ í¬ê¸°ë¥¼ ì‘ê²Œ ì„¤ì •
SAVE_INTERVAL = 50  # 50ê°œ ì²˜ë¦¬í•  ë•Œë§ˆë‹¤ ì €ì¥

# ToW í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
FEW_SHOT_PROMPT_TEMPLATE = """
**[Role and Instructions]**
You are an expert mathematical reasoning AI and Korean language analyst. Your mission is to analyze and explain precisely why a specific 'Next Word' is the necessary and logical continuation of the  given mathematical 'Context'. Your entire explanation must be enclosed within <ToW> and </ToW> tags, adhering to the following rules:

1. **Mathematical Logic**: Analyze the mathematical flow, problem structure, and operational relationships within the context to explain how the word is logically connected to the mathematical reasoning.
2. **Mathematical Necessity**: Emphasize why this particular word is the most fitting and essential choice for understanding the mathematical concept, operation, or question type compared to any other alternatives.
3. **Clarity and Brevity**: Provide a concise and clear explanation, focusing on the core mathematical reasons and conceptual importance.        
4. **Output Language**: Output in English.

---

**[Example 1]**
**Input:**
- **Context:** ë‹«íŒêµ¬ê°„ [0,2Ï€]ì—ì„œ ì •ì´ëœ í•¨ìˆ˜ f(x) = acosbx + 3 ì´ x = Ï€/3ì—ì„œ ìµœëŒ“ê°’ 13ì„ì„ ê°–ë„ë¡ í•˜ëŠ” ë‘ ìì—°ìˆ˜ a,b ì˜ ìˆœì„œìŒ (a,b) ì— ëŒ€í•˜ì—¬ a+bì˜
- **Next Word:** ìµœì†Ÿê°’ì€ì€

**Output:**
<ToW>The word "ìµœì†Ÿê°’ì€ì€" follows logically because the problem asks for the minimum sum of ğ‘ and ğ‘ after determining their values for the function's maximum. It is necessary to indicate the minimum value of ğ‘ +ğ‘. </ToW>

---

**[Example 2]**
**Input:**
- **Context:** ì‹œê° t = 0 ì¼ ë•Œ, ì¶œë°œí•˜ì—¬ ìˆ˜ì§ì„  ìœ„ë¥¼ ì›€ì§ì´ëŠ” ì  Pì˜ ì‹œê° t(t>=0)ì—ì„œì˜ ìœ„ì¹˜ xê°€ x=t^3-(3t^2)/2-6t ì´ë‹¤. ì¶œë°œí•œ í›„ ì  Pì˜ ìš´ë™ ë°©í–¥ì´ ë°”ë€ŒëŠ” ì‹œê°ì—ì„œì˜ ì  Pì˜ ê°€ì†ë„ëŠ”?
- **Next Word:** ê°€ì†ë„ëŠ”

**Output:**
<ToW>"ê°€ì†ë„ëŠ”" is needed because the problem asks for the acceleration, which is the second derivative of the position function, at the point when the movement direction changes. </ToW>

---

**[Example 3]**
**Input:**
- **Context:** ìµœê³ ì°¨í•­ì˜ ê³„ìˆ˜ê°€ 1ì¸ ì‚¼ì°¨í•¨ìˆ˜ f(x)ê°€ f(1)=f(2)=0, f'(0)=-7ì„ ë§Œì¡±ì‹œí‚¨ë‹¤. ì›ì  Oì™€ ì  P(3,f(3))ì— ëŒ€í•˜ì—¬ ì„ ë¶„ OPê°€ ê³¡ì„  y=f(x)ì™€ ë§Œë‚˜ëŠ” ì  ì¤‘ Pê°€ ì•„ë‹Œ ì ì„ Që¼ í•˜ì. ê³¡ì„  y=f(x)ì™€ yì¶• ë° ì„ ë¶„ OQë¡œ ë‘˜ëŸ¬ì‹¸ì¸ ë¶€ë¶„ì˜ ë„“ì´ë¥¼ A, ê³¡ì„  y=f(x)ì™€ ì„ ë¶„ PQë¡œ ë‘˜ëŸ¬ì‹¸ì¸ ë¶€ë¶„ì˜ ë„™ì´ë¥¼ Bë¼ í•  ë•Œ, B-Aì˜ ê°’ì€?
- **Next Word:** ë„“ì´ë¥¼

**Output:**
<ToW>"ë„“ì´ë¥¼" is the logical continuation as the question requires calculating the areas ğ´ and ğµ, and their difference ğµâˆ’ğ´. </ToW>
---

**[Actual Work]**

**Input:**
- **Context:** {context}
- **Next Word:** {gold_label}

**Output:**
"""

def load_model():
    """GPT-OSS 120B ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤ (ë©”ëª¨ë¦¬ ìµœì í™”)."""
    print(f"[INFO] GPT-OSS 120B ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤: {MODEL_PATH}")
    print(f"[INFO] Available devices: {DEVICES}")
    
    try:
        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
        
        # íŒ¨ë”© í† í° ì„¤ì •
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            
        # ë©”ëª¨ë¦¬ ìµœì í™”ëœ device_map ìƒì„±
        if NUM_GPUS > 1:
            print(f"[INFO] Using {NUM_GPUS} GPUs for model distribution with memory optimization")
            device_map = "auto"
        else:
            device_map = DEVICES[0] if DEVICES[0] != "cpu" else "cpu"
            
        print("[INFO] Loading model with aggressive memory optimization...")
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_PATH,
            torch_dtype=torch.float16,  # í•­ìƒ float16 ì‚¬ìš©ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
            device_map=device_map,
            trust_remote_code=True,
            low_cpu_mem_usage=True,
            # ì¶”ê°€ ë©”ëª¨ë¦¬ ìµœì í™” ì˜µì…˜ë“¤
            load_in_8bit=False,  # 8bit ì–‘ìí™”ëŠ” ì¼ë‹¨ ë¹„í™œì„±í™” (ì•ˆì •ì„± ìœ„í•´)
            load_in_4bit=False,  # 4bit ì–‘ìí™”ë„ ë¹„í™œì„±í™”
            max_memory={i: "20GiB" for i in range(NUM_GPUS)},  # GPUë‹¹ ìµœëŒ€ ë©”ëª¨ë¦¬ ì œí•œ
            offload_folder="./model_offload",  # ì¼ë¶€ ê°€ì¤‘ì¹˜ë¥¼ ë””ìŠ¤í¬ì— ì €ì¥
            offload_state_dict=True,
        )
        
        model.eval()
        print(f"[INFO] Model loaded successfully with memory optimization across {NUM_GPUS} GPU(s)")
        return model, tokenizer
        
    except Exception as e:
        print(f"[ERROR] Model loading failed: {e}")
        print("[INFO] Trying with 8-bit quantization...")
        
        # 8-bit ì–‘ìí™”ë¡œ ì¬ì‹œë„
        try:
            model = AutoModelForCausalLM.from_pretrained(
                MODEL_PATH,
                torch_dtype=torch.float16,
                device_map=device_map,
                trust_remote_code=True,
                low_cpu_mem_usage=True,
                load_in_8bit=True,  # 8bit ì–‘ìí™” í™œì„±í™”
                max_memory={i: "15GiB" for i in range(NUM_GPUS)},
                offload_folder="./model_offload",
            )
            model.eval()
            print(f"[INFO] Model loaded with 8-bit quantization across {NUM_GPUS} GPU(s)")
            return model, tokenizer
            
        except Exception as e2:
            print(f"[ERROR] 8-bit quantization also failed: {e2}")
            return None, None

def generate_with_model(model, tokenizer, prompt, max_new_tokens=512):
    """ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    try:
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048)
        
        # ëª¨ë¸ì˜ ì²« ë²ˆì§¸ ë§¤ê°œë³€ìˆ˜ì—ì„œ deviceì™€ dtype í™•ì¸
        model_device = next(model.parameters()).device
        model_dtype = next(model.parameters()).dtype
        
        # ì…ë ¥ í…ì„œë¥¼ ëª¨ë¸ê³¼ ë™ì¼í•œ deviceì™€ dtypeìœ¼ë¡œ ì´ë™
        if 'input_ids' in inputs:
            inputs['input_ids'] = inputs['input_ids'].to(model_device)
        if 'attention_mask' in inputs:
            inputs['attention_mask'] = inputs['attention_mask'].to(model_device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=0.2,  # ì•½ê°„ì˜ ì°½ì˜ì„±ì„ í—ˆìš©í•˜ë˜ ì¼ê´€ì„±ì„ ìœ ì§€
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id
            )
        
        # ìƒˆë¡œ ìƒì„±ëœ í† í°ë§Œ ë””ì½”ë”©
        new_tokens = outputs[0][inputs['input_ids'].shape[1]:]
        generated_text = tokenizer.decode(new_tokens, skip_special_tokens=True)
        
        return generated_text.strip()
        
    except Exception as e:
        print(f"[ERROR] í…ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
        print(f"[DEBUG] Model device: {next(model.parameters()).device}")
        print(f"[DEBUG] Model dtype: {next(model.parameters()).dtype}")
        return None

def generate_tow_dataset():
    """GPT-OSS 120B ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ToW ë°ì´í„°ì…‹ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(os.path.dirname(OUTPUT_JSON_PATH), exist_ok=True)
    
    # ëª¨ë¸ ë¡œë“œ
    model, tokenizer = load_model()
    if model is None or tokenizer is None:
        print("[ERROR] ëª¨ë¸ ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return
    
    # ì…ë ¥ ë°ì´í„° ë¡œë“œ
    print(f"[INFO] '{INPUT_JSON_PATH}' íŒŒì¼ì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.")
    try:
        with open(INPUT_JSON_PATH, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except FileNotFoundError:
        print(f"[ERROR] ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {INPUT_JSON_PATH}")
        print("[INFO] ë¨¼ì € gptoss_hrm8k_generate_gold_word.pyë¥¼ ì‹¤í–‰í•˜ì—¬ gold label ë°ì´í„°ë¥¼ ìƒì„±í•˜ì„¸ìš”.")
        return
    
    # ì´ë¯¸ ì²˜ë¦¬ëœ ê²°ê³¼ë¥¼ ë¶ˆëŸ¬ì™€ì„œ ì´ì–´í•˜ê¸°
    results = []
    processed_ids = set()
    if os.path.exists(OUTPUT_JSON_PATH):
        print(f"[INFO] ê¸°ì¡´ ì¶œë ¥ íŒŒì¼ '{OUTPUT_JSON_PATH}'ì„(ë¥¼) ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ì´ì–´ì„œ ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
        with open(OUTPUT_JSON_PATH, 'r', encoding='utf-8') as f:
            results = json.load(f)
        processed_ids = {item['id'] for item in results}
        print(f"[INFO] {len(processed_ids)}ê°œì˜ í•­ëª©ì´ ì´ë¯¸ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")

    # ì²˜ë¦¬í•´ì•¼ í•  ë°ì´í„°ë§Œ í•„í„°ë§
    tasks_to_run = [item for item in data if item['id'] not in processed_ids]
    if not tasks_to_run:
        print("[SUCCESS] ëª¨ë“  í•­ëª©ì´ ì´ë¯¸ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return

    error_count = 0
    last_save_count = len(results)

    print(f"[INFO] ì´ {len(tasks_to_run)}ê°œì˜ ì‹ ê·œ í•­ëª©ì— ëŒ€í•´ ToW ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤.")

    for item in tqdm(tasks_to_run, desc="Generating ToW"):
        try:
            # ToW í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = FEW_SHOT_PROMPT_TEMPLATE.format(
                context=item['context'], 
                gold_label=item['gold_label']
            )
            
            # ëª¨ë¸ ìƒì„±
            tow_content = generate_with_model(model, tokenizer, prompt)
            
            if tow_content is None:
                error_count += 1
                continue
            
            # ê¸°ì¡´ itemì— 'tow' í‚¤ ì¶”ê°€
            enhanced_item = item.copy()
            enhanced_item['tow'] = tow_content
            results.append(enhanced_item)

        except Exception as e:
            print(f"[ERROR] ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ID: {item['id']}): {e}")
            error_count += 1
            continue
        
        # ì£¼ê¸°ì  ì €ì¥
        if len(results) - last_save_count >= SAVE_INTERVAL:
            print(f"\n[INFO] ì¤‘ê°„ ì €ì¥: {len(results)}ê°œì˜ ëˆ„ì  ê²°ê³¼ë¥¼ íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤.")
            with open(OUTPUT_JSON_PATH, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            last_save_count = len(results)

    # ëª¨ë“  ì²˜ë¦¬ê°€ ëë‚œ í›„ ìµœì¢… ì €ì¥
    print(f"\n[SUCCESS] ToW ë°ì´í„°ì…‹ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    print(f"  - ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ëœ ì‹ ê·œ í•­ëª©: {len(tasks_to_run) - error_count}")
    print(f"  - ì˜¤ë¥˜ ë˜ëŠ” ê±´ë„ˆë›´ í•­ëª©: {error_count}")
    print(f"  - ì´ ì €ì¥ëœ í•­ëª© ìˆ˜: {len(results)}")
    print(f"  - ê²°ê³¼ íŒŒì¼: {OUTPUT_JSON_PATH}")
    
    with open(OUTPUT_JSON_PATH, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

if __name__ == "__main__":
    generate_tow_dataset()