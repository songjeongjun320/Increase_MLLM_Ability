#!/usr/bin/env python3
"""
Integrated ToW Augmentation System
í†µí•© ToW ì¦ê°• ì‹œìŠ¤í…œ - ëª¨ë“  ê°œì„ ì‚¬í•­ í†µí•© ì ìš©
"""

import json
import logging
from typing import Dict, List, Optional
from pathlib import Path
from tqdm import tqdm
import time
from collections import defaultdict

# ê°œë°œí•œ ëª¨ë“ˆë“¤ import
from improved_particle_handler import ToWParticleIntegrator
from enhanced_diversity_engine import AdvancedDiversityEngine
from advanced_augmentation_techniques import AdvancedAugmentationEngine

logger = logging.getLogger(__name__)

class IntegratedToWAugmentationSystem:
    """í†µí•© ToW ì¦ê°• ì‹œìŠ¤í…œ"""
    
    def __init__(self, config: Optional[Dict] = None):
        self.config = config or self.get_default_config()
        
        # ê° ì—”ì§„ ì´ˆê¸°í™”
        self.particle_integrator = ToWParticleIntegrator()
        self.diversity_engine = AdvancedDiversityEngine()
        self.advanced_engine = AdvancedAugmentationEngine()
        
        # í†µê³„ ì •ë³´
        self.processing_stats = defaultdict(int)
        
    def get_default_config(self) -> Dict:
        """ê¸°ë³¸ ì„¤ì •"""
        return {
            'input_file': 'ToW_koconovel_augmented.json',
            'output_file': 'ToW_koconovel_enhanced_final.json',
            'batch_size': 100,
            'max_variants_per_entry': 8,
            'enable_particle_fixing': True,
            'enable_diversity_enhancement': True,
            'enable_advanced_techniques': True,
            'target_diversity_score': 65.0,
            'quality_threshold': 0.9,
            'domain_detection': True,
            'preserve_tow_integrity': True,
            'validation_enabled': True
        }
    
    def detect_domain(self, text: str) -> str:
        """ë„ë©”ì¸ ìë™ ê°ì§€"""
        # í‚¤ì›Œë“œ ê¸°ë°˜ ë„ë©”ì¸ ë¶„ë¥˜
        domain_keywords = {
            'mathematics': ['ìˆ˜í•™', 'ê³„ì‚°', 'ê³µì‹', 'ë°©ì •ì‹', 'í•¨ìˆ˜', 'ê·¸ë˜í”„', 'ì¦ëª…', 'ë¬¸ì œ', 'ë‹µ', 'í•´', 'ê°’', 'ë³€ìˆ˜'],
            'science': ['ì‹¤í—˜', 'ê´€ì°°', 'ê°€ì„¤', 'ì´ë¡ ', 'ë²•ì¹™', 'í˜„ìƒ', 'ë¬¼ì§ˆ', 'í™”í•™', 'ë¬¼ë¦¬', 'ìƒë¬¼', 'ê³¼í•™'],
            'history': ['ì‹œëŒ€', 'ì™•ì¡°', 'ì „ìŸ', 'ë¬¸í™”', 'ì •ì¹˜', 'ê²½ì œ', 'ì¡°ì„ ', 'ê³ ë ¤', 'ì—­ì‚¬', 'ì „í†µ', 'ê³¼ê±°'],
            'literature': ['ì†Œì„¤', 'ì‹œ', 'ë¬¸í•™', 'ì‘í’ˆ', 'ì‘ê°€', 'ë“±ì¥ì¸ë¬¼', 'ì£¼ì œ', 'í‘œí˜„', 'ë¬¸ì²´', 'ì„œìˆ '],
            'general': []
        }
        
        text_lower = text.lower()
        domain_scores = defaultdict(int)
        
        for domain, keywords in domain_keywords.items():
            if domain == 'general':
                continue
            for keyword in keywords:
                if keyword in text_lower:
                    domain_scores[domain] += 1
        
        if domain_scores:
            return max(domain_scores, key=domain_scores.get)
        else:
            return 'general'
    
    def process_single_entry(self, entry: Dict) -> List[Dict]:
        """ë‹¨ì¼ ì—”íŠ¸ë¦¬ ì²˜ë¦¬"""
        original_text = entry.get('augmented_text', '')
        if not original_text:
            return [entry]
        
        results = [entry]  # ì›ë³¸ í¬í•¨
        generated_variants = []
        
        try:
            # 1. ì¡°ì‚¬ ì˜¤ë¥˜ ìˆ˜ì • (ê¸°ë³¸ì ìœ¼ë¡œ ì ìš©)
            if self.config['enable_particle_fixing']:
                corrected_variants = self.particle_integrator.enhanced_particle_substitution(
                    original_text, max_variants=2
                )
                generated_variants.extend(corrected_variants)
                self.processing_stats['particle_corrections'] += len(corrected_variants)
            
            # 2. ë‹¤ì–‘ì„± ì¦ê°•
            if self.config['enable_diversity_enhancement']:
                diversity_variants = self.diversity_engine.generate_high_diversity_variants(
                    original_text, target_count=3
                )
                generated_variants.extend(diversity_variants)
                self.processing_stats['diversity_enhancements'] += len(diversity_variants)
            
            # 3. ê³ ê¸‰ ì¦ê°• ê¸°ë²• ì ìš©
            if self.config['enable_advanced_techniques']:
                domain = 'general'
                if self.config['domain_detection']:
                    domain = self.detect_domain(original_text)
                
                advanced_variants = self.advanced_engine.generate_advanced_variants(
                    original_text, domain=domain
                )
                generated_variants.extend(advanced_variants)
                self.processing_stats['advanced_techniques'] += len(advanced_variants)
                self.processing_stats[f'domain_{domain}'] += 1
            
            # 4. ë³€í˜• í’ˆì§ˆ ê²€ì¦ ë° ì„ ë³„
            validated_variants = self.validate_and_select_variants(
                original_text, generated_variants, entry
            )
            
            # 5. ìƒˆë¡œìš´ ì—”íŠ¸ë¦¬ ìƒì„±
            for i, variant in enumerate(validated_variants):
                if len(results) >= self.config['max_variants_per_entry']:
                    break
                
                new_entry = self.create_enhanced_entry(entry, variant, i)
                results.append(new_entry)
                self.processing_stats['total_variants_created'] += 1
        
        except Exception as e:
            logger.warning(f"Entry processing failed for {entry.get('doc_id', 'unknown')}: {e}")
            self.processing_stats['processing_errors'] += 1
        
        return results
    
    def validate_and_select_variants(self, original_text: str, variants: List[str], entry: Dict) -> List[str]:
        """ë³€í˜• í’ˆì§ˆ ê²€ì¦ ë° ì„ ë³„"""
        if not variants:
            return []
        
        validated = []
        original_signature = self.diversity_engine.calculate_text_signature(original_text)
        
        for variant in variants:
            # 1. ê¸°ë³¸ í’ˆì§ˆ ê²€ì‚¬
            if len(variant.strip()) < 10:  # ë„ˆë¬´ ì§§ìŒ
                continue
            
            if variant == original_text:  # ì›ë³¸ê³¼ ë™ì¼
                continue
            
            # 2. ToW í† í° ë³´ì¡´ í™•ì¸
            if self.config.get('preserve_tow_integrity', True):
                original_tow_count = len(self.diversity_engine.extract_tow_tokens(original_text))
                variant_tow_count = len(self.diversity_engine.extract_tow_tokens(variant))
                
                if original_tow_count != variant_tow_count:
                    self.processing_stats['tow_integrity_failures'] += 1
                    continue
            
            # 3. ë‹¤ì–‘ì„± í™•ì¸ (ì¤‘ë³µ ë°©ì§€)
            variant_signature = self.diversity_engine.calculate_text_signature(variant)
            if variant_signature == original_signature:
                continue
            
            # 4. í•œêµ­ì–´ ì¡°ì‚¬ ì˜¤ë¥˜ í™•ì¸
            particle_errors = self.particle_integrator.particle_handler.validate_particle_usage(variant)
            if len(particle_errors) > 2:  # 2ê°œ ì´ˆê³¼ ì˜¤ë¥˜ëŠ” ì œì™¸
                self.processing_stats['particle_validation_failures'] += 1
                continue
            
            validated.append(variant)
        
        return validated
    
    def create_enhanced_entry(self, original_entry: Dict, variant_text: str, variant_index: int) -> Dict:
        """ê°œì„ ëœ ì—”íŠ¸ë¦¬ ìƒì„±"""
        new_entry = original_entry.copy()
        
        # ID ì—…ë°ì´íŠ¸
        base_id = original_entry.get('doc_id', f'entry_{int(time.time())}')
        new_entry['doc_id'] = f"{base_id}_enhanced_{variant_index + 1}"
        
        # í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
        new_entry['augmented_text'] = variant_text
        
        # ToW ë©”íƒ€ë°ì´í„° ì¬ê³„ì‚°
        tow_tokens = self.diversity_engine.extract_tow_tokens(variant_text)
        new_entry['tow_count'] = len(tow_tokens)
        new_entry['tow_tokens'] = [f"<ToW>{token}</ToW>" for token in tow_tokens]
        
        # ê°œì„  ë©”íƒ€ë°ì´í„° ì¶”ê°€
        new_entry['enhancement_type'] = 'integrated_system'
        new_entry['original_doc_id'] = original_entry.get('doc_id')
        new_entry['enhancement_timestamp'] = int(time.time())
        
        # í’ˆì§ˆ ì§€í‘œ
        diversity_score = self.calculate_entry_diversity_score(variant_text, original_entry.get('augmented_text', ''))
        new_entry['diversity_score'] = diversity_score
        
        return new_entry
    
    def calculate_entry_diversity_score(self, variant_text: str, original_text: str) -> float:
        """ê°œë³„ ì—”íŠ¸ë¦¬ ë‹¤ì–‘ì„± ì ìˆ˜ ê³„ì‚°"""
        variants = [original_text, variant_text]
        
        lexical = self.diversity_engine.measure_lexical_diversity(variants)
        structural = self.diversity_engine.measure_structural_diversity(variants)
        semantic = self.diversity_engine.measure_semantic_diversity(variants)
        
        # ê°€ì¤‘ í‰ê· 
        diversity_score = (lexical * 0.3 + structural * 0.4 + semantic * 0.3)
        return round(diversity_score, 2)
    
    def process_dataset(self, input_file: str, output_file: str) -> Dict:
        """ì „ì²´ ë°ì´í„°ì…‹ ì²˜ë¦¬"""
        logger.info(f"ğŸš€ Starting integrated ToW augmentation system")
        logger.info(f"Input: {input_file}")
        logger.info(f"Output: {output_file}")
        
        # ë°ì´í„° ë¡œë“œ
        if not Path(input_file).exists():
            raise FileNotFoundError(f"Input file not found: {input_file}")
        
        with open(input_file, 'r', encoding='utf-8') as f:
            original_data = json.load(f)
        
        logger.info(f"Loaded {len(original_data)} entries")
        
        # ë°°ì¹˜ ì²˜ë¦¬
        batch_size = self.config['batch_size']
        all_enhanced_entries = []
        
        for i in tqdm(range(0, len(original_data), batch_size), desc="Processing batches"):
            batch = original_data[i:i + batch_size]
            batch_results = []
            
            for entry in batch:
                processed_entries = self.process_single_entry(entry)
                batch_results.extend(processed_entries)
            
            all_enhanced_entries.extend(batch_results)
            
            # ì¤‘ê°„ ì§„í–‰ ìƒí™© ë¡œê¹…
            if i % (batch_size * 10) == 0 and i > 0:
                logger.info(f"Processed {i + batch_size} entries, generated {len(all_enhanced_entries)} total entries")
        
        # ìµœì¢… ë‹¤ì–‘ì„± í‰ê°€
        if self.config['validation_enabled']:
            final_diversity = self.evaluate_final_diversity(all_enhanced_entries)
            logger.info(f"Final diversity score: {final_diversity:.2f}%")
        
        # ê²°ê³¼ ì €ì¥
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(all_enhanced_entries, f, ensure_ascii=False, indent=2)
        
        # í†µê³„ ë¦¬í¬íŠ¸ ìƒì„±
        stats_report = self.generate_processing_report(
            len(original_data), len(all_enhanced_entries)
        )
        
        logger.info("âœ… Integrated augmentation completed!")
        return stats_report
    
    def evaluate_final_diversity(self, entries: List[Dict]) -> float:
        """ìµœì¢… ë‹¤ì–‘ì„± í‰ê°€"""
        texts = [entry.get('augmented_text', '') for entry in entries]
        texts = [t for t in texts if t.strip()]  # ë¹ˆ í…ìŠ¤íŠ¸ ì œì™¸
        
        if not texts:
            return 0.0
        
        # ìƒ˜í”Œë§í•´ì„œ í‰ê°€ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
        if len(texts) > 1000:
            import random
            texts = random.sample(texts, 1000)
        
        lexical = self.diversity_engine.measure_lexical_diversity(texts)
        structural = self.diversity_engine.measure_structural_diversity(texts)
        semantic = self.diversity_engine.measure_semantic_diversity(texts)
        length = self.diversity_engine.measure_length_diversity(texts)
        
        overall = (lexical * 0.25 + structural * 0.30 + semantic * 0.35 + length * 0.10)
        return overall
    
    def generate_processing_report(self, original_count: int, final_count: int) -> Dict:
        """ì²˜ë¦¬ ë¦¬í¬íŠ¸ ìƒì„±"""
        report = {
            'processing_summary': {
                'original_entries': original_count,
                'final_entries': final_count,
                'enhancement_ratio': f"{final_count / original_count:.2f}x",
                'processing_time': time.time()
            },
            'enhancement_statistics': dict(self.processing_stats),
            'quality_metrics': {
                'particle_error_prevention': self.processing_stats.get('particle_corrections', 0),
                'diversity_improvements': self.processing_stats.get('diversity_enhancements', 0),
                'advanced_technique_applications': self.processing_stats.get('advanced_techniques', 0),
                'tow_integrity_maintained': 100 - (self.processing_stats.get('tow_integrity_failures', 0) / max(final_count, 1)) * 100
            },
            'domain_distribution': {
                key: value for key, value in self.processing_stats.items() 
                if key.startswith('domain_')
            },
            'error_summary': {
                'processing_errors': self.processing_stats.get('processing_errors', 0),
                'validation_failures': self.processing_stats.get('particle_validation_failures', 0) + self.processing_stats.get('tow_integrity_failures', 0)
            }
        }
        
        # ë¦¬í¬íŠ¸ íŒŒì¼ë¡œ ì €ì¥
        report_file = self.config['output_file'].replace('.json', '_report.json')
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ“Š Processing report saved to {report_file}")
        
        return report
    
    def print_summary_report(self, report: Dict):
        """ìš”ì•½ ë¦¬í¬íŠ¸ ì¶œë ¥"""
        print("\n" + "="*60)
        print("ğŸ¯ INTEGRATED ToW AUGMENTATION SYSTEM RESULTS")
        print("="*60)
        
        summary = report['processing_summary']
        print(f"ğŸ“Š Dataset Scale:")
        print(f"  Original entries: {summary['original_entries']:,}")
        print(f"  Enhanced entries: {summary['final_entries']:,}")
        print(f"  Enhancement ratio: {summary['enhancement_ratio']}")
        
        quality = report['quality_metrics']
        print(f"\nğŸ”§ Quality Improvements:")
        print(f"  Particle corrections: {quality['particle_error_prevention']:,}")
        print(f"  Diversity enhancements: {quality['diversity_improvements']:,}")
        print(f"  Advanced techniques: {quality['advanced_technique_applications']:,}")
        print(f"  ToW integrity: {quality['tow_integrity_maintained']:.1f}%")
        
        if report['domain_distribution']:
            print(f"\nğŸ·ï¸ Domain Distribution:")
            for domain, count in report['domain_distribution'].items():
                domain_name = domain.replace('domain_', '').capitalize()
                print(f"  {domain_name}: {count:,}")
        
        errors = report['error_summary']
        if errors['processing_errors'] > 0 or errors['validation_failures'] > 0:
            print(f"\nâš ï¸ Error Summary:")
            print(f"  Processing errors: {errors['processing_errors']}")
            print(f"  Validation failures: {errors['validation_failures']}")
        
        print("="*60)

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    
    # ì‚¬ìš©ì ì •ì˜ ì„¤ì •
    config = {
        'input_file': 'ToW_koconovel_augmented.json',
        'output_file': 'ToW_koconovel_enhanced_final.json',
        'batch_size': 50,  # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ìœ„í•´ ì¶•ì†Œ
        'max_variants_per_entry': 6,
        'target_diversity_score': 65.0,
        'enable_particle_fixing': True,
        'enable_diversity_enhancement': True,
        'enable_advanced_techniques': True,
        'domain_detection': True,
        'validation_enabled': True
    }
    
    try:
        # ì‹œìŠ¤í…œ ì´ˆê¸°í™” ë° ì‹¤í–‰
        system = IntegratedToWAugmentationSystem(config)
        
        # ì…ë ¥ íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not Path(config['input_file']).exists():
            logger.error(f"Input file not found: {config['input_file']}")
            logger.info("Please ensure the augmented dataset exists")
            return
        
        # ì²˜ë¦¬ ì‹¤í–‰
        start_time = time.time()
        report = system.process_dataset(config['input_file'], config['output_file'])
        end_time = time.time()
        
        # ì²˜ë¦¬ ì‹œê°„ ì¶”ê°€
        report['processing_summary']['processing_time'] = f"{end_time - start_time:.2f}s"
        
        # ê²°ê³¼ ì¶œë ¥
        system.print_summary_report(report)
        
        logger.info(f"âœ… All improvements applied successfully!")
        logger.info(f"ğŸ“ Enhanced dataset: {config['output_file']}")
        
    except Exception as e:
        logger.error(f"âŒ System execution failed: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()