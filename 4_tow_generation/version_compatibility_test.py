#!/usr/bin/env python3
"""
ë¼ì´ë¸ŒëŸ¬ë¦¬ ë²„ì „ í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸
GPT-OSS 120Bì™€ í˜¸í™˜ë˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¡°í•© ì°¾ê¸°
"""

import subprocess
import sys
import importlib
import pkg_resources

# í…ŒìŠ¤íŠ¸í•  ë²„ì „ ì¡°í•©ë“¤
VERSION_COMBINATIONS = [
    {
        "name": "ìµœì‹  ì•ˆì • ë²„ì „",
        "versions": {
            "transformers": "4.35.2",
            "torch": "2.1.0",
            "accelerate": "0.24.1",
            "bitsandbytes": "0.41.3"
        }
    },
    {
        "name": "ì¤‘ê°„ ë²„ì „",
        "versions": {
            "transformers": "4.33.0",
            "torch": "2.0.1",
            "accelerate": "0.22.0",
            "bitsandbytes": "0.40.0"
        }
    },
    {
        "name": "ì´ì „ ì•ˆì • ë²„ì „",
        "versions": {
            "transformers": "4.30.0",
            "torch": "1.13.1",
            "accelerate": "0.20.0",
            "bitsandbytes": "0.39.0"
        }
    },
    {
        "name": "GPT-OSS ê¶Œì¥ (ì¶”ì •)",
        "versions": {
            "transformers": "4.28.0",
            "torch": "1.13.0",
            "accelerate": "0.18.0",
            "bitsandbytes": "0.37.0"
        }
    }
]

def get_current_version(package_name):
    """í˜„ì¬ ì„¤ì¹˜ëœ íŒ¨í‚¤ì§€ ë²„ì „ í™•ì¸"""
    try:
        return pkg_resources.get_distribution(package_name).version
    except:
        return "ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ"

def check_current_environment():
    """í˜„ì¬ í™˜ê²½ì˜ ë²„ì „ ì •ë³´ ì¶œë ¥"""
    print("="*60)
    print("í˜„ì¬ í™˜ê²½ ì •ë³´")
    print("="*60)
    
    packages = ["transformers", "torch", "accelerate", "bitsandbytes", "datasets", "tokenizers"]
    
    for package in packages:
        version = get_current_version(package)
        print(f"{package}: {version}")
    
    print()

def test_import_compatibility():
    """í˜„ì¬ í™˜ê²½ì—ì„œ import í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸"""
    print("="*60)
    print("Import í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸")
    print("="*60)
    
    tests = [
        ("transformers ê¸°ë³¸", "from transformers import AutoTokenizer, AutoModelForCausalLM"),
        ("BitsAndBytesConfig", "from transformers import BitsAndBytesConfig"),
        ("BitsAndBytesConfig ìƒì„±", """
from transformers import BitsAndBytesConfig
import torch
config = BitsAndBytesConfig(load_in_4bit=True)
        """),
        ("ê³ ê¸‰ BitsAndBytesConfig", """
from transformers import BitsAndBytesConfig
import torch
config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
)
        """),
        ("torch ê¸°ë³¸", "import torch; print(torch.cuda.is_available())"),
        ("accelerate", "import accelerate"),
        ("bitsandbytes", "import bitsandbytes"),
    ]
    
    for test_name, code in tests:
        try:
            exec(code)
            print(f"âœ… {test_name}: ì„±ê³µ")
        except Exception as e:
            print(f"âŒ {test_name}: ì‹¤íŒ¨ - {e}")
    
    print()

def test_model_loading_compatibility():
    """ê°„ë‹¨í•œ ëª¨ë¸ë¡œ ë¡œë”© í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸"""
    print("="*60)
    print("ëª¨ë¸ ë¡œë”© í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸")
    print("="*60)
    
    # GPT2ë¡œ ê¸°ë³¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
    try:
        from transformers import AutoTokenizer, AutoModelForCausalLM
        
        print("GPT2 tokenizer í…ŒìŠ¤íŠ¸...")
        tokenizer = AutoTokenizer.from_pretrained("gpt2")
        print("âœ… GPT2 tokenizer ë¡œë”© ì„±ê³µ")
        
        print("GPT2 model í…ŒìŠ¤íŠ¸...")
        model = AutoModelForCausalLM.from_pretrained("gpt2")
        print("âœ… GPT2 model ë¡œë”© ì„±ê³µ")
        
        # ì–‘ìí™” í…ŒìŠ¤íŠ¸
        try:
            from transformers import BitsAndBytesConfig
            print("4-bit ì–‘ìí™” í…ŒìŠ¤íŠ¸...")
            
            config = BitsAndBytesConfig(load_in_4bit=True)
            model_quantized = AutoModelForCausalLM.from_pretrained(
                "gpt2", 
                quantization_config=config,
                device_map="auto"
            )
            print("âœ… 4-bit ì–‘ìí™” í…ŒìŠ¤íŠ¸ ì„±ê³µ")
            
        except Exception as e:
            print(f"âŒ ì–‘ìí™” í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        
    except Exception as e:
        print(f"âŒ ê¸°ë³¸ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
    
    print()

def generate_requirements_files():
    """ë‹¤ì–‘í•œ ë²„ì „ ì¡°í•©ì˜ requirements.txt íŒŒì¼ ìƒì„±"""
    print("="*60)
    print("Requirements íŒŒì¼ ìƒì„±")
    print("="*60)
    
    for i, combo in enumerate(VERSION_COMBINATIONS):
        filename = f"requirements_combo_{i+1}_{combo['name'].replace(' ', '_')}.txt"
        
        with open(filename, 'w') as f:
            f.write(f"# {combo['name']}\n")
            for package, version in combo['versions'].items():
                f.write(f"{package}=={version}\n")
            
            # ì¶”ê°€ ì˜ì¡´ì„±
            f.write("\n# ì¶”ê°€ ì˜ì¡´ì„±\n")
            f.write("datasets\n")
            f.write("tokenizers\n")
            f.write("safetensors\n")
            f.write("huggingface_hub\n")
        
        print(f"âœ… {filename} ìƒì„± ì™„ë£Œ")
    
    print()

def create_test_script():
    """ê° í™˜ê²½ì—ì„œ ì‹¤í–‰í•  í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±"""
    
    script_content = '''#!/usr/bin/env python3
"""
í™˜ê²½ë³„ GPT-OSS í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

def test_gptoss_loading():
    model_path = "../1_models/gpt_oss/gpt-oss-120b"
    
    print("GPT-OSS 120B ë¡œë”© í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    # ë°©ë²• 1: ê¸°ë³¸ ë¡œë”©
    try:
        print("\\n[í…ŒìŠ¤íŠ¸ 1] ê¸°ë³¸ ë¡œë”©")
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        model = AutoModelForCausalLM.from_pretrained(model_path)
        print("âœ… ê¸°ë³¸ ë¡œë”© ì„±ê³µ!")
        return model, tokenizer
    except Exception as e:
        print(f"âŒ ê¸°ë³¸ ë¡œë”© ì‹¤íŒ¨: {e}")
    
    # ë°©ë²• 2: trust_remote_code=False
    try:
        print("\\n[í…ŒìŠ¤íŠ¸ 2] trust_remote_code=False")
        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=False)
        model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=False)
        print("âœ… trust_remote_code=False ì„±ê³µ!")
        return model, tokenizer
    except Exception as e:
        print(f"âŒ trust_remote_code=False ì‹¤íŒ¨: {e}")
    
    # ë°©ë²• 3: ì–‘ìí™” ì—†ì´
    try:
        print("\\n[í…ŒìŠ¤íŠ¸ 3] ì–‘ìí™” ì—†ì´ float16")
        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            trust_remote_code=True,
            torch_dtype=torch.float16,
            device_map="auto"
        )
        print("âœ… float16 ë¡œë”© ì„±ê³µ!")
        return model, tokenizer
    except Exception as e:
        print(f"âŒ float16 ë¡œë”© ì‹¤íŒ¨: {e}")
    
    # ë°©ë²• 4: CPU only
    try:
        print("\\n[í…ŒìŠ¤íŠ¸ 4] CPU only")
        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            trust_remote_code=True,
            device_map="cpu"
        )
        print("âœ… CPU ë¡œë”© ì„±ê³µ!")
        return model, tokenizer
    except Exception as e:
        print(f"âŒ CPU ë¡œë”© ì‹¤íŒ¨: {e}")
    
    print("\\nâŒ ëª¨ë“  ë¡œë”© ë°©ë²• ì‹¤íŒ¨")
    return None, None

if __name__ == "__main__":
    print("="*50)
    print("GPT-OSS 120B í™˜ê²½ë³„ í…ŒìŠ¤íŠ¸")
    print("="*50)
    
    model, tokenizer = test_gptoss_loading()
    
    if model and tokenizer:
        print("\\nğŸ‰ ëª¨ë¸ ë¡œë”© ì„±ê³µ! ê°„ë‹¨í•œ ìƒì„± í…ŒìŠ¤íŠ¸...")
        try:
            inputs = tokenizer("ì•ˆë…•í•˜ì„¸ìš”", return_tensors="pt")
            with torch.no_grad():
                outputs = model.generate(**inputs, max_new_tokens=5)
            result = tokenizer.decode(outputs[0], skip_special_tokens=True)
            print(f"ìƒì„± ê²°ê³¼: {result}")
        except Exception as e:
            print(f"ìƒì„± í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
    else:
        print("\\nâŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
'''
    
    with open("test_gptoss_environment.py", 'w') as f:
        f.write(script_content)
    
    print("âœ… test_gptoss_environment.py ìƒì„± ì™„ë£Œ")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    check_current_environment()
    test_import_compatibility()
    test_model_loading_compatibility()
    generate_requirements_files()
    create_test_script()
    
    print("="*60)
    print("ê¶Œì¥ í…ŒìŠ¤íŠ¸ ìˆœì„œ:")
    print("="*60)
    print("1. í˜„ì¬ í™˜ê²½ì—ì„œ: python environment_check.py")
    print("2. í˜„ì¬ í™˜ê²½ì—ì„œ: python alternative_loaders.py")
    print("3. ìƒˆ í™˜ê²½ ë§Œë“¤ê¸°: conda create -n gptoss_test python=3.8")
    print("4. í™˜ê²½ í™œì„±í™”: conda activate gptoss_test")
    print("5. ë²„ì „ ì¡°í•© ì„¤ì¹˜: pip install -r requirements_combo_1_ìµœì‹ _ì•ˆì •_ë²„ì „.txt")
    print("6. í…ŒìŠ¤íŠ¸ ì‹¤í–‰: python test_gptoss_environment.py")
    print("7. ì‹¤íŒ¨ì‹œ ë‹¤ë¥¸ requirements íŒŒì¼ë¡œ ì¬ì‹œë„")
    print()
    print("ê° ì¡°í•©ì„ í…ŒìŠ¤íŠ¸í•˜ì—¬ ì–´ë–¤ ë²„ì „ì—ì„œ ì„±ê³µí•˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”!")

if __name__ == "__main__":
    main()