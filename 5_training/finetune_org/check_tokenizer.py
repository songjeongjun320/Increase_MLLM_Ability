#!/usr/bin/env python3
"""
ToW Token Checker - ëª¨ë¸ ê²½ë¡œì—ì„œ ToW í† í° ì¡´ì¬ ì—¬ë¶€ë¥¼ í™•ì¸í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
"""

import os
import json
from transformers import AutoTokenizer
from pathlib import Path

def check_tow_tokens(model_path):
    """
    ì£¼ì–´ì§„ ëª¨ë¸ ê²½ë¡œì—ì„œ ToW í† í°ì˜ ì¡´ì¬ ì—¬ë¶€ë¥¼ ìƒì„¸íˆ í™•ì¸
    """
    print(f"Checking ToW tokens in: {model_path}")
    print("=" * 80)
    
    # 1. ê²½ë¡œ ì¡´ì¬ í™•ì¸
    if not os.path.exists(model_path):
        print(f"âŒ Error: Path does not exist: {model_path}")
        return
    
    # 2. í† í¬ë‚˜ì´ì € íŒŒì¼ë“¤ í™•ì¸
    tokenizer_files = [
        "tokenizer.json",
        "tokenizer.model", 
        "tokenizer_config.json",
        "special_tokens_map.json",
        "added_tokens.json"
    ]
    
    print("ğŸ“ Tokenizer files check:")
    for file in tokenizer_files:
        file_path = os.path.join(model_path, file)
        exists = "âœ…" if os.path.exists(file_path) else "âŒ"
        print(f"  {exists} {file}")
    
    print()
    
    try:
        # 3. í† í¬ë‚˜ì´ì € ë¡œë”©
        print("ğŸ”„ Loading tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        
        # 4. ê¸°ë³¸ ì •ë³´
        print(f"ğŸ“Š Basic tokenizer info:")
        print(f"  Vocab size: {len(tokenizer)}")
        print(f"  Model max length: {tokenizer.model_max_length}")
        print(f"  Tokenizer class: {tokenizer.__class__.__name__}")
        print()
        
        # 5. íŠ¹ìˆ˜ í† í° í™•ì¸
        print("ğŸ” Special tokens:")
        special_tokens = {
            'bos_token': tokenizer.bos_token,
            'eos_token': tokenizer.eos_token,
            'unk_token': tokenizer.unk_token,
            'pad_token': tokenizer.pad_token,
        }
        
        for name, token in special_tokens.items():
            print(f"  {name}: {token} (ID: {tokenizer.convert_tokens_to_ids(token) if token else 'None'})")
        
        # 6. Additional special tokens í™•ì¸
        additional_tokens = tokenizer.additional_special_tokens
        print(f"  additional_special_tokens: {additional_tokens}")
        print()
        
        # 7. ToW í† í° ì¡´ì¬ í™•ì¸
        print("ğŸ¯ ToW Token Analysis:")
        
        # Vocabì—ì„œ ToW í† í° ì°¾ê¸°
        vocab = tokenizer.get_vocab()
        tow_start_in_vocab = '<ToW>' in vocab
        tow_end_in_vocab = '</ToW>' in vocab
        
        print(f"  <ToW> in vocab: {tow_start_in_vocab}")
        print(f"  </ToW> in vocab: {tow_end_in_vocab}")
        
        if tow_start_in_vocab:
            tow_start_id = vocab['<ToW>']
            print(f"    <ToW> ID: {tow_start_id}")
        
        if tow_end_in_vocab:
            tow_end_id = vocab['</ToW>']
            print(f"    </ToW> ID: {tow_end_id}")
        
        # Additional special tokensì—ì„œ í™•ì¸
        tow_start_in_additional = '<ToW>' in (additional_tokens or [])
        tow_end_in_additional = '</ToW>' in (additional_tokens or [])
        
        print(f"  <ToW> in additional_special_tokens: {tow_start_in_additional}")
        print(f"  </ToW> in additional_special_tokens: {tow_end_in_additional}")
        
        # 8. í† í°í™” í…ŒìŠ¤íŠ¸
        print("\nğŸ§ª Tokenization test:")
        test_text = "Question: What is 2+2? <ToW> Let me think </ToW> Answer: 4"
        
        try:
            tokens = tokenizer.tokenize(test_text)
            token_ids = tokenizer.encode(test_text, add_special_tokens=False)
            decoded = tokenizer.decode(token_ids)
            
            print(f"  Input: {test_text}")
            print(f"  Tokens: {tokens}")
            print(f"  Token IDs: {token_ids}")
            print(f"  Decoded: {decoded}")
            
            # ToW í† í°ì´ ì˜¬ë°”ë¥´ê²Œ í† í°í™”ë˜ëŠ”ì§€ í™•ì¸
            tow_start_found = any('<ToW>' in str(token) for token in tokens)
            tow_end_found = any('</ToW>' in str(token) for token in tokens)
            
            print(f"  <ToW> found in tokens: {tow_start_found}")
            print(f"  </ToW> found in tokens: {tow_end_found}")
            
        except Exception as e:
            print(f"  âŒ Tokenization test failed: {e}")
        
        # 9. ê°œë³„ í† í° í…ŒìŠ¤íŠ¸
        print("\nğŸ”¬ Individual token tests:")
        for token in ['<ToW>', '</ToW>']:
            try:
                direct_tokens = tokenizer.tokenize(token)
                direct_ids = tokenizer.encode(token, add_special_tokens=False)
                convert_id = tokenizer.convert_tokens_to_ids(token)
                
                print(f"  {token}:")
                print(f"    tokenize(): {direct_tokens}")
                print(f"    encode(): {direct_ids}")
                print(f"    convert_tokens_to_ids(): {convert_id}")
                
            except Exception as e:
                print(f"    âŒ Error: {e}")
        
        # 10. ìš”ì•½
        print(f"\nğŸ“‹ Summary:")
        tow_exists = tow_start_in_vocab and tow_end_in_vocab
        print(f"  ToW tokens exist: {'âœ… YES' if tow_exists else 'âŒ NO'}")
        
        if tow_exists:
            print(f"  Status: ToW tokens are properly configured")
        else:
            print(f"  Status: ToW tokens need to be added")
            
    except Exception as e:
        print(f"âŒ Error loading tokenizer: {e}")

if __name__ == "__main__":
    # ëª¨ë¸ ê²½ë¡œ ì„¤ì •
    model_path = "/scratch/jsong132/Increase_MLLM_Ability/5_training/finetune_org/merged_models/llama-3.2-3b-tow-09_11_2epoch_fix_tow-merged"
    
    check_tow_tokens(model_path)