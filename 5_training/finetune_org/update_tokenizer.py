#!/usr/bin/env python3
"""
ê¸°ì¡´ merged_models í´ë”ì˜ í† í¬ë‚˜ì´ì €ì— ToW í† í°ì„ ì¶”ê°€í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
"""

import os
from transformers import AutoTokenizer

def update_tokenizer_with_tow(model_folder_path):
    """
    ê¸°ì¡´ ëª¨ë¸ í´ë”ì˜ í† í¬ë‚˜ì´ì €ì— ToW í† í°ì„ ì¶”ê°€í•˜ê³  ì €ì¥
    """
    print(f"Processing: {model_folder_path}")
    
    # 1. ê¸°ì¡´ í† í¬ë‚˜ì´ì € ë¡œë“œ
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_folder_path)
        print(f"  Original vocab size: {len(tokenizer)}")
        print(f"  Original additional_special_tokens: {tokenizer.additional_special_tokens}")
    except Exception as e:
        print(f"  âŒ Error loading tokenizer: {e}")
        return False
    
    # 2. ToW í† í° ì¡´ì¬ í™•ì¸
    existing_special_tokens = tokenizer.additional_special_tokens or []
    tow_start_exists = '<ToW>' in existing_special_tokens
    tow_end_exists = '</ToW>' in existing_special_tokens
    
    if tow_start_exists and tow_end_exists:
        print(f"  âœ… ToW tokens already exist - skipping")
        return True
    
    # 3. ToW í† í° ì¶”ê°€
    tokens_to_add = []
    if not tow_start_exists:
        tokens_to_add.append('<ToW>')
    if not tow_end_exists:
        tokens_to_add.append('</ToW>')
    
    if tokens_to_add:
        all_special_tokens = existing_special_tokens + tokens_to_add
        num_added = tokenizer.add_special_tokens({'additional_special_tokens': all_special_tokens})
        print(f"  â• Added tokens: {tokens_to_add}")
        print(f"  ğŸ“Š New vocab size: {len(tokenizer)}")
    
    # 4. ë°±ì—… ìƒì„± (ì„ íƒì‚¬í•­)
    backup_dir = os.path.join(model_folder_path, "tokenizer_backup")
    if not os.path.exists(backup_dir):
        os.makedirs(backup_dir)
        # ê¸°ì¡´ í† í¬ë‚˜ì´ì € íŒŒì¼ë“¤ ë°±ì—…
        tokenizer_files = [
            "tokenizer.json", "tokenizer_config.json", 
            "special_tokens_map.json", "added_tokens.json"
        ]
        for file in tokenizer_files:
            src = os.path.join(model_folder_path, file)
            dst = os.path.join(backup_dir, file)
            if os.path.exists(src):
                import shutil
                shutil.copy2(src, dst)
        print(f"  ğŸ’¾ Backup created at: {backup_dir}")
    
    # 5. ìˆ˜ì •ëœ í† í¬ë‚˜ì´ì € ì €ì¥
    try:
        tokenizer.save_pretrained(model_folder_path)
        print(f"  âœ… Tokenizer updated successfully")
    except Exception as e:
        print(f"  âŒ Error saving tokenizer: {e}")
        return False
    
    # 6. ê²€ì¦
    try:
        test_tokenizer = AutoTokenizer.from_pretrained(model_folder_path)
        tow_start_id = test_tokenizer.convert_tokens_to_ids('<ToW>')
        tow_end_id = test_tokenizer.convert_tokens_to_ids('</ToW>')
        
        if tow_start_id != test_tokenizer.unk_token_id and tow_end_id != test_tokenizer.unk_token_id:
            print(f"  âœ… Validation successful - ToW IDs: {tow_start_id}, {tow_end_id}")
            
            # í† í°í™” í…ŒìŠ¤íŠ¸
            test_text = "Question: <ToW> thinking </ToW> Answer"
            tokens = test_tokenizer.tokenize(test_text)
            if '<ToW>' in str(tokens) and '</ToW>' in str(tokens):
                print(f"  âœ… Tokenization test passed")
            else:
                print(f"  âš ï¸ Tokenization test: {tokens}")
        else:
            print(f"  âŒ Validation failed - ToW tokens not properly saved")
            return False
            
    except Exception as e:
        print(f"  âŒ Validation error: {e}")
        return False
    
    return True

def main():
    # ëª¨ë¸ í´ë”ë“¤ ê²½ë¡œ ì„¤ì •
    base_path = "/scratch/jsong132/Increase_MLLM_Ability/5_training/finetune_org/merged_models"
    
    print("Starting tokenizer update process...")
    print(f"Scanning all folders in: {base_path}")
    print("=" * 60)
    
    # base_path ë°‘ì˜ ëª¨ë“  í´ë” ìë™ ê²€ìƒ‰ (tokenizer íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ ë¬´ê´€í•˜ê²Œ ëª¨ë“  í´ë” ì²˜ë¦¬)
    model_folders = []
    if os.path.exists(base_path):
        for item in os.listdir(base_path):
            folder_path = os.path.join(base_path, item)
            if os.path.isdir(folder_path):
                model_folders.append(item)
                print(f"Found folder: {item}")
    else:
        print(f"âŒ Base path not found: {base_path}")
        return
    
    if not model_folders:
        print("âŒ No folders found")
        return
    
    print(f"Total model folders found: {len(model_folders)}")
    print("-" * 60)
    
    success_count = 0
    for folder in model_folders:
        folder_path = os.path.join(base_path, folder)
        
        if update_tokenizer_with_tow(folder_path):
            success_count += 1
        print("-" * 60)
    
    print(f"Process completed: {success_count}/{len(model_folders)} models updated successfully")

if __name__ == "__main__":
    main()