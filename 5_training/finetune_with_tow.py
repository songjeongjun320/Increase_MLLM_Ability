#!/usr/bin/env python3
"""
Fine-tune Base Models with ToW Data
===================================

Fine-tunes the 3 base models (DeepSeek-R1-7B, Qwen-7B, Llama-8B) with 
Korean stories + English ToW tokens generated by GPT-OSS.

Training Flow:
1. Load base models from 1_models/
2. Load ToW-augmented Korean stories from 4_tow_generation/
3. Fine-tune each model with ToW data  
4. Save ToW-enhanced models for evaluation

Key Principle: ToW tokens contain ONLY English reasoning
"""

import os
import sys
import json
import time
from datetime import datetime
from typing import List, Dict, Any, Optional
from pathlib import Path
import logging
from dataclasses import dataclass

import torch
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from datasets import Dataset
import numpy as np

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from utils.text_processing import validate_tow_token_format, count_tow_tokens

@dataclass
class ModelTrainingConfig:
    """Training configuration for each base model"""
    model_name: str
    model_path: str
    output_name: str
    learning_rate: float
    batch_size: int
    num_epochs: int
    max_length: int

class ToWModelTrainer:
    """Fine-tune base models with ToW-augmented Korean data"""
    
    def __init__(self, training_data_path: str = None, output_dir: str = None):
        """
        Initialize ToW model trainer
        
        Args:
            training_data_path: Path to ToW-augmented training data
            output_dir: Directory to save trained models
        """
        self.training_data_path = training_data_path or self._find_tow_data()
        self.output_dir = Path(output_dir) if output_dir else Path(__file__).parent / "checkpoints"
        self.output_dir.mkdir(exist_ok=True)
        
        # Base models to train (from 1_models/)
        self.base_models = self._get_base_models()
        
        # Training statistics
        self.training_stats = {}
        
        # Setup logging
        self._setup_logging()
    
    def _find_tow_data(self) -> str:
        """Find ToW-augmented training data"""
        data_paths = [
            Path(__file__).parent.parent / "4_tow_generation" / "korean_stories_with_tow.jsonl",
            Path(__file__).parent.parent / "2_datasets" / "korean_stories" / "processed" / "tow_augmented.jsonl"
        ]
        
        for path in data_paths:
            if path.exists():
                return str(path)
        
        raise FileNotFoundError("No ToW training data found. Please run korean_tow_generator.py first.")
    
    def _get_base_models(self) -> List[ModelTrainingConfig]:
        """Get base models for training"""
        models_dir = Path(__file__).parent.parent / "1_models"
        
        base_models = [
            ModelTrainingConfig(
                model_name="deepseek-r1-distill-qwen-7b",
                model_path=str(models_dir / "deepseek-r1-distill-qwen-7b"),
                output_name="deepseek-r1-qwen-7b-tow",
                learning_rate=2e-5,
                batch_size=4,
                num_epochs=3,
                max_length=1024
            ),
            ModelTrainingConfig(
                model_name="deepseek-r1-distill-llama-8b",  
                model_path=str(models_dir / "deepseek-r1-distill-llama-8b"),
                output_name="deepseek-r1-llama-8b-tow",
                learning_rate=1.5e-5,
                batch_size=4,
                num_epochs=3,
                max_length=1024
            ),
            ModelTrainingConfig(
                model_name="qwen2.5-7b-instruct",
                model_path=str(models_dir / "qwen2.5-7b-instruct"),
                output_name="qwen2.5-7b-tow",
                learning_rate=2e-5,
                batch_size=4,
                num_epochs=3,
                max_length=1024
            )
        ]
        
        # Filter only available models
        available_models = []
        for model in base_models:
            if Path(model.model_path).exists():
                available_models.append(model)
                self.logger.info(f"‚úÖ Found base model: {model.model_name}")
            else:
                self.logger.warning(f"‚ö†Ô∏è  Base model not found: {model.model_name}")
        
        return available_models
    
    def _setup_logging(self):
        """Setup logging for training"""
        log_file = self.output_dir / f"tow_training_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file),
                logging.StreamHandler()
            ]
        )
        
        self.logger = logging.getLogger(__name__)
    
    def load_tow_training_data(self) -> Dataset:
        """
        Load and prepare ToW training data
        
        Returns:
            Hugging Face Dataset with ToW-augmented Korean stories
        """
        self.logger.info(f"üì• Loading ToW training data from {self.training_data_path}")
        
        training_texts = []
        tow_compliance_count = 0
        total_tow_tokens = 0
        
        with open(self.training_data_path, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    entry = json.loads(line.strip())
                    
                    # Use augmented text with ToW tokens
                    text = entry.get('augmented_text', entry.get('text', ''))
                    
                    if text and len(text.strip()) > 10:
                        training_texts.append(text)
                        
                        # Validate ToW compliance
                        tow_count = count_tow_tokens(text)
                        total_tow_tokens += tow_count
                        
                        # Check if ToW tokens are English-only
                        if entry.get('tow_tokens'):
                            for token in entry['tow_tokens']:
                                if validate_tow_token_format(token):
                                    tow_compliance_count += 1
                
                except Exception as e:
                    self.logger.warning(f"Error processing training entry: {e}")
                    continue
        
        # Calculate compliance rate
        compliance_rate = (tow_compliance_count / total_tow_tokens * 100) if total_tow_tokens > 0 else 0
        
        self.logger.info(f"üìä Training Data Statistics:")
        self.logger.info(f"   Total training examples: {len(training_texts)}")
        self.logger.info(f"   Total ToW tokens: {total_tow_tokens}")
        self.logger.info(f"   English compliance rate: {compliance_rate:.1f}%")
        
        # Create Hugging Face dataset
        dataset = Dataset.from_dict({"text": training_texts})
        
        return dataset
    
    def prepare_tokenized_dataset(self, dataset: Dataset, tokenizer, max_length: int) -> Dataset:
        """
        Tokenize dataset for training
        
        Args:
            dataset: Raw text dataset
            tokenizer: Model tokenizer
            max_length: Maximum sequence length
            
        Returns:
            Tokenized dataset ready for training
        """
        def tokenize_function(examples):
            return tokenizer(
                examples["text"],
                truncation=True,
                padding=False,
                max_length=max_length,
                return_overflowing_tokens=False,
            )
        
        tokenized_dataset = dataset.map(
            tokenize_function,
            batched=True,
            remove_columns=dataset.column_names,
        )
        
        return tokenized_dataset
    
    def train_model(self, config: ModelTrainingConfig) -> str:
        """
        Train a single base model with ToW data
        
        Args:
            config: Model training configuration
            
        Returns:
            Path to trained model
        """
        self.logger.info(f"üöÄ Starting ToW training for {config.model_name}")
        start_time = time.time()
        
        # Load base model and tokenizer
        self.logger.info(f"üì• Loading base model: {config.model_name}")
        
        try:
            tokenizer = AutoTokenizer.from_pretrained(config.model_path)
            model = AutoModelForCausalLM.from_pretrained(
                config.model_path,
                torch_dtype=torch.float16,
                device_map="auto",
                low_cpu_mem_usage=True
            )
            
            # Add padding token if missing
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            
        except Exception as e:
            self.logger.error(f"‚ùå Failed to load base model {config.model_name}: {e}")
            return None
        
        # Load and prepare training data
        training_dataset = self.load_tow_training_data()
        tokenized_dataset = self.prepare_tokenized_dataset(training_dataset, tokenizer, config.max_length)
        
        # Training arguments
        output_path = self.output_dir / config.output_name
        training_args = TrainingArguments(
            output_dir=str(output_path),
            overwrite_output_dir=True,
            num_train_epochs=config.num_epochs,
            per_device_train_batch_size=config.batch_size,
            learning_rate=config.learning_rate,
            warmup_steps=100,
            logging_steps=50,
            save_steps=500,
            save_strategy="steps",
            evaluation_strategy="no",
            fp16=True,
            dataloader_drop_last=True,
            remove_unused_columns=True,
            run_name=f"tow-{config.output_name}",
            report_to=None,  # Disable wandb/tensorboard
        )
        
        # Data collator
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=tokenizer,
            mlm=False,
        )
        
        # Initialize trainer
        trainer = Trainer(
            model=model,
            args=training_args,
            data_collator=data_collator,
            train_dataset=tokenized_dataset,
            tokenizer=tokenizer,
        )
        
        # Start training
        self.logger.info(f"üéØ Training {config.model_name} with ToW data...")
        self.logger.info(f"   Learning rate: {config.learning_rate}")
        self.logger.info(f"   Batch size: {config.batch_size}")
        self.logger.info(f"   Epochs: {config.num_epochs}")
        self.logger.info(f"   Max length: {config.max_length}")
        
        try:
            trainer.train()
            
            # Save final model
            final_model_path = output_path / "final_model"
            trainer.save_model(str(final_model_path))
            tokenizer.save_pretrained(str(final_model_path))
            
            training_time = time.time() - start_time
            
            # Save training statistics
            self.training_stats[config.model_name] = {
                "training_time": training_time,
                "output_path": str(final_model_path),
                "learning_rate": config.learning_rate,
                "epochs": config.num_epochs,
                "batch_size": config.batch_size,
                "final_loss": trainer.state.log_history[-1].get('train_loss', 0) if trainer.state.log_history else 0
            }
            
            self.logger.info(f"‚úÖ Completed ToW training for {config.model_name}")
            self.logger.info(f"   Training time: {training_time:.1f}s")
            self.logger.info(f"   Model saved to: {final_model_path}")
            
            # Clean up GPU memory
            del model
            del trainer
            torch.cuda.empty_cache() if torch.cuda.is_available() else None
            
            return str(final_model_path)
            
        except Exception as e:
            self.logger.error(f"‚ùå Training failed for {config.model_name}: {e}")
            return None
    
    def train_all_models(self) -> Dict[str, str]:
        """
        Train all available base models with ToW data
        
        Returns:
            Dictionary mapping model names to trained model paths
        """
        self.logger.info(f"üéØ Starting ToW training for {len(self.base_models)} base models")
        
        trained_models = {}
        
        for config in self.base_models:
            try:
                model_path = self.train_model(config)
                if model_path:
                    trained_models[config.model_name] = model_path
                    
                    # Save progress
                    self._save_training_progress(trained_models)
                
            except Exception as e:
                self.logger.error(f"‚ùå Error training {config.model_name}: {e}")
        
        # Save final training summary
        self._save_training_summary(trained_models)
        
        self.logger.info(f"üéâ Completed ToW training for all models!")
        return trained_models
    
    def _save_training_progress(self, trained_models: Dict[str, str]):
        """Save training progress"""
        progress_file = self.output_dir / "training_progress.json"
        
        progress = {
            "timestamp": datetime.now().isoformat(),
            "trained_models": trained_models,
            "training_stats": self.training_stats,
            "training_data_path": self.training_data_path
        }
        
        with open(progress_file, 'w', encoding='utf-8') as f:
            json.dump(progress, f, indent=2, ensure_ascii=False)
    
    def _save_training_summary(self, trained_models: Dict[str, str]):
        """Save final training summary"""
        summary_file = self.output_dir / "tow_training_summary.json"
        
        summary = {
            "training_date": datetime.now().isoformat(),
            "total_models_trained": len(trained_models),
            "trained_models": trained_models,
            "training_statistics": self.training_stats,
            "training_data_source": self.training_data_path,
            "next_steps": [
                "Run evaluation: cd ../3_evaluation/",
                "python compare_baseline_vs_tow.py",
                "Compare Before (baseline) vs After (ToW-trained) performance"
            ]
        }
        
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"üìä Training summary saved to {summary_file}")

def main():
    """Main function for ToW model training"""
    
    print("üöÄ Option 2 TOW Model Training")
    print("=" * 60)
    print("üéØ Training base models with Korean stories + English ToW tokens")
    print("üìä Before/After comparison will show ToW effectiveness")
    
    # Initialize trainer
    trainer = ToWModelTrainer()
    
    # Check available models
    if not trainer.base_models:
        print("‚ùå No base models found for training!")
        print("üí° Please download base models first:")
        print("   cd ../1_models/")
        print("   python download_deepseek_r1_distill_qwen_7b.py")
        return
    
    print(f"\nüìã Found {len(trainer.base_models)} base models for training:")
    for model in trainer.base_models:
        print(f"   ‚Ä¢ {model.model_name}")
    
    # Check ToW data
    try:
        dataset = trainer.load_tow_training_data()
        print(f"üìä ToW training data: {len(dataset)} examples")
    except FileNotFoundError as e:
        print(f"‚ùå {e}")
        print("üí° Please generate ToW data first:")
        print("   cd ../4_tow_generation/")
        print("   python korean_tow_generator.py")
        return
    
    # Start training
    trained_models = trainer.train_all_models()
    
    print("\nüéâ ToW Training Results:")
    print("=" * 60)
    
    for model_name, model_path in trained_models.items():
        stats = trainer.training_stats.get(model_name, {})
        print(f"\nü§ñ {model_name}:")
        print(f"   ‚úÖ Trained successfully")
        print(f"   üìÇ Model path: {model_path}")
        print(f"   ‚è±Ô∏è  Training time: {stats.get('training_time', 0):.1f}s")
        print(f"   üìâ Final loss: {stats.get('final_loss', 0):.4f}")
    
    print(f"\nüéØ Next Steps:")
    print("   1. Run evaluation comparison:")
    print("      cd ../3_evaluation/")
    print("      python compare_baseline_vs_tow.py")
    print("   2. Measure ToW effectiveness on KMMLU, KLUE, Translation")

if __name__ == "__main__":
    main()