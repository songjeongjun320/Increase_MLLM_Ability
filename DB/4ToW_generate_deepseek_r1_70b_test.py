# -*- coding: utf-8 -*-
import torch
import gc
from transformers import AutoTokenizer, AutoModelForCausalLM

# bitsandbytes ê°€ì ¸ì˜¤ê¸° ì‹œë„
try:
    from transformers import BitsAndBytesConfig
    BITSANDBYTES_AVAILABLE = True
    print("BitsAndBytesConfigë¥¼ ì„±ê³µì ìœ¼ë¡œ importí–ˆìŠµë‹ˆë‹¤.")
except ImportError:
    BITSANDBYTES_AVAILABLE = False
    print("BitsAndBytesConfig import ì‹¤íŒ¨ - ê¸°ë³¸ ì–‘ìí™”ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")

def clear_memory():
    """ë©”ëª¨ë¦¬ ì •ë¦¬ í•¨ìˆ˜"""
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

def ask_deepseek(question, model, tokenizer, device, max_new_tokens=2048, temperature=0.7, top_p=0.9):
    """DeepSeek ëª¨ë¸ì—ê²Œ ì§ˆë¬¸í•˜ê³  ë‹µë³€ ë°›ê¸°"""
    # ë©”ëª¨ë¦¬ ì •ë¦¬
    clear_memory()
    
    # DeepSeek-R1ì— ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì‚¬ìš©
    # ì¶”ë¡  ê³¼ì •ì„ ìˆ¨ê¸°ê³  ìµœì¢… ë‹µë³€ë§Œ ì¶œë ¥í•˜ë„ë¡ ì„¤ì •
    prompt = f"""
<ï½œstartâ–headerâ–idï½œ>user<ï½œendâ–headerâ–idï½œ>

{question}<ï½œeotâ–idï½œ><ï½œstartâ–headerâ–idï½œ>assistant<ï½œendâ–headerâ–idï½œ>

<ï½œthinkingï½œ>"""

    print(f"\n--- ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ ---\n{question}\n--------------------")

    inputs = tokenizer(prompt, return_tensors="pt", return_attention_mask=True)
    input_ids = inputs.input_ids.to(device)
    attention_mask = inputs.attention_mask.to(device)

    # eos_token_idê°€ ì—¬ëŸ¬ ê°œì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ë¦¬ìŠ¤íŠ¸ë¡œ ì „ë‹¬
    eos_token_ids = [tokenizer.eos_token_id]
    if hasattr(tokenizer, 'additional_special_tokens_ids'):
        eos_token_ids.extend(tokenizer.additional_special_tokens_ids)
    
    # pad_token_idê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•Šë‹¤ë©´ eos_token_idë¡œ ì„¤ì •
    if tokenizer.pad_token_id is None:
        tokenizer.pad_token_id = tokenizer.eos_token_id

    print("ë‹µë³€ ìƒì„± ì¤‘...")
    # ìƒì„± íŒŒë¼ë¯¸í„° ì„¤ì •
    generation_kwargs = {
        "max_new_tokens": max_new_tokens,
        "temperature": temperature,
        "top_p": top_p,
        "do_sample": True,
        "pad_token_id": tokenizer.pad_token_id,
        "eos_token_id": eos_token_ids,
        "use_cache": True,
    }

    # ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ìƒì„±ì„ ìœ„í•´ torch.no_grad() ì‚¬ìš©
    with torch.no_grad():
        generated_ids = model.generate(
            input_ids,
            attention_mask=attention_mask,
            **generation_kwargs
        )

    # ì…ë ¥ ë¶€ë¶„ì„ ì œì™¸í•˜ê³  ìƒì„±ëœ í…ìŠ¤íŠ¸ë§Œ ë””ì½”ë”©
    answer_ids = generated_ids[0][input_ids.shape[-1]:]
    full_response = tokenizer.decode(answer_ids, skip_special_tokens=True)

    # DeepSeek-R1ì˜ ì¶”ë¡  ê³¼ì •ê³¼ ìµœì¢… ë‹µë³€ ë¶„ë¦¬
    final_answer = extract_final_answer(full_response)

    # ë©”ëª¨ë¦¬ ì •ë¦¬
    del input_ids, attention_mask, generated_ids
    clear_memory()

    return final_answer

def extract_final_answer(response):
    """DeepSeek-R1 ì‘ë‹µì—ì„œ ìµœì¢… ë‹µë³€ë§Œ ì¶”ì¶œ"""
    # thinking íƒœê·¸ ì‚¬ì´ì˜ ì¶”ë¡  ê³¼ì • ì œê±°
    if "<ï½œthinkingï½œ>" in response and "<ï½œ/thinkingï½œ>" in response:
        # thinking ë¶€ë¶„ ì´í›„ì˜ ìµœì¢… ë‹µë³€ë§Œ ì¶”ì¶œ
        parts = response.split("<ï½œ/thinkingï½œ>")
        if len(parts) > 1:
            final_answer = parts[1].strip()
        else:
            final_answer = response.strip()
    else:
        # thinking íƒœê·¸ê°€ ì—†ëŠ” ê²½ìš°, ì „ì²´ ì‘ë‹µì—ì„œ ì •ë¦¬
        final_answer = response.strip()
    
    # ë¶ˆí•„ìš”í•œ í† í°ë“¤ ì œê±°
    unwanted_tokens = ["<ï½œthinkingï½œ>", "<ï½œ/thinkingï½œ>", "<ï½œeotâ–idï½œ>", "<ï½œendâ–ofâ–textï½œ>"]
    for token in unwanted_tokens:
        final_answer = final_answer.replace(token, "")
    
    return final_answer.strip()

def load_model():
    """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë”©"""
    global BITSANDBYTES_AVAILABLE
    
    # ë¡œì»¬ ëª¨ë¸ ê²½ë¡œ ì„¤ì •
    model_path = "/scratch/jsong132/Increase_MLLM_Ability/DeepSeek_R1_Distill_Llama_70B"
    
    print(f"ë¡œì»¬ ê²½ë¡œ '{model_path}'ì—ì„œ ëª¨ë¸ ë¡œë”© ì¤‘...")
    print("A100 1ê°œì— ìµœì í™”ëœ ì„¤ì •ìœ¼ë¡œ ë¡œë”©í•©ë‹ˆë‹¤.")

    try:
        # í† í¬ë‚˜ì´ì € ë¡œë”©
        print("í† í¬ë‚˜ì´ì € ë¡œë”© ì¤‘...")
        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True, local_files_only=True)
        
        # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ì‚¬ìš© ì¥ì¹˜: {device}")
        
        # GPU ë©”ëª¨ë¦¬ ì •ë³´ ì¶œë ¥
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            print(f"GPU ë©”ëª¨ë¦¬: {gpu_memory:.1f} GB")

        # ëª¨ë¸ ë¡œë”© ì„¤ì •
        model_kwargs = {}
        
        if BITSANDBYTES_AVAILABLE:
            print("BitsAndBytesConfigë¥¼ ì‚¬ìš©í•œ 4-bit ì–‘ìí™” ì„¤ì •")
            try:
                # A100 1ê°œì— ìµœì í™”ëœ 4-bit ì–‘ìí™” ì„¤ì •
                bnb_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_use_double_quant=True,
                    bnb_4bit_quant_type="nf4",
                    bnb_4bit_compute_dtype=torch.bfloat16,
                )
                
                model_kwargs = {
                    "quantization_config": bnb_config,
                    "device_map": "auto",
                    "torch_dtype": torch.bfloat16,
                    "trust_remote_code": True,
                    "low_cpu_mem_usage": True,
                    "local_files_only": True,
                    "offload_folder": "./offload",
                }
                print("4-bit ì–‘ìí™” ì„¤ì • ì™„ë£Œ")
            except Exception as e:
                print(f"BitsAndBytesConfig ì„¤ì • ì‹¤íŒ¨: {e}")
                print("ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ fallbackí•©ë‹ˆë‹¤.")
                BITSANDBYTES_AVAILABLE = False
        
        if not BITSANDBYTES_AVAILABLE:
            print("ê¸°ë³¸ float16 ì–‘ìí™” ì„¤ì • ì‚¬ìš©")
            model_kwargs = {
                "torch_dtype": torch.float16,
                "device_map": "auto",
                "trust_remote_code": True,
                "low_cpu_mem_usage": True,
                "local_files_only": True,
                "offload_folder": "./offload",
            }

        print("ëª¨ë¸ ë¡œë”© ì‹œì‘... (ìˆ˜ ë¶„ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
        model = AutoModelForCausalLM.from_pretrained(model_path, **model_kwargs)
        
        # ì¶”ë¡  ëª¨ë“œë¡œ ì„¤ì •
        model.eval()
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        clear_memory()

        print(f"ë¡œì»¬ ëª¨ë¸ '{model_path}' ë¡œë”© ì™„ë£Œ.")
        
        # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated() / 1024**3
            cached = torch.cuda.memory_reserved() / 1024**3
            print(f"GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {allocated:.1f} GB allocated, {cached:.1f} GB cached")

        return model, tokenizer, device

    except FileNotFoundError as e:
        print(f"\nâŒ ë¡œì»¬ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        print(f"ë‹¤ìŒì„ í™•ì¸í•´ì£¼ì„¸ìš”:")
        print(f"1. ê²½ë¡œê°€ ì •í™•í•œì§€: {model_path}")
        print(f"2. ëª¨ë¸ íŒŒì¼ë“¤ì´ í•´ë‹¹ ê²½ë¡œì— ì¡´ì¬í•˜ëŠ”ì§€")
        print(f"3. íŒŒì¼ ì ‘ê·¼ ê¶Œí•œì´ ìˆëŠ”ì§€")
        return None, None, None
    except RuntimeError as e:
        if "out of memory" in str(e).lower():
            print("\nCUDA out of memory ì˜¤ë¥˜ ë°œìƒ!")
            print("ë‹¤ìŒ í•´ê²° ë°©ë²•ë“¤ì„ ì‹œë„í•´ë³´ì„¸ìš”:")
            print("1. ë” ì‘ì€ ë°°ì¹˜ í¬ê¸° ì‚¬ìš©")
            print("2. max_new_tokens ê°’ ì¤„ì´ê¸°")
            print("3. ë” ì ê·¹ì ì¸ ë©”ëª¨ë¦¬ ê´€ë¦¬:")
            print("   - PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128 í™˜ê²½ë³€ìˆ˜ ì„¤ì •")
            print("4. bitsandbytes ì¬ì„¤ì¹˜")
        else:
            print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None, None, None
    except Exception as e:
        if "bitsandbytes" in str(e).lower() or "quantization" in str(e).lower():
            print(f"\nBitsandbytes ê´€ë ¨ ì˜¤ë¥˜: {e}")
            print("\ní•´ê²° ë°©ë²•:")
            print("1. bitsandbytes ì¬ì„¤ì¹˜:")
            print("   pip uninstall bitsandbytes -y")
            print("   pip install bitsandbytes")
        else:
            print(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None, None, None

def interactive_chat():
    """ëŒ€í™”í˜• ì±„íŒ… ì‹œìŠ¤í…œ"""
    print("=" * 60)
    print("ğŸ¤– DeepSeek ëŒ€í™”í˜• ì±„íŒ… ì‹œìŠ¤í…œ (ë¡œì»¬ ëª¨ë¸)")
    print("=" * 60)
    
    # ëª¨ë¸ ë¡œë”©
    model, tokenizer, device = load_model()
    
    if model is None:
        print("ëª¨ë¸ ë¡œë”©ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return
    
    print("\nâœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ! ì´ì œ ì§ˆë¬¸í•´ë³´ì„¸ìš”.")
    print("ğŸ’¡ ëª…ë ¹ì–´:")
    print("  - 'quit', 'exit', 'ì¢…ë£Œ' : í”„ë¡œê·¸ë¨ ì¢…ë£Œ")
    print("  - 'clear', 'í´ë¦¬ì–´' : í™”ë©´ ì •ë¦¬")
    print("  - 'memory', 'ë©”ëª¨ë¦¬' : GPU ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸")
    print("  - 'settings', 'ì„¤ì •' : ìƒì„± ì„¤ì • ë³€ê²½")
    print("-" * 60)
    
    # ê¸°ë³¸ ìƒì„± ì„¤ì • (ë” ê¸´ ë‹µë³€ì„ ìœ„í•´ í† í° ìˆ˜ ì¦ê°€)
    settings = {
        'max_new_tokens': 2048,  # ë” ê¸´ ë‹µë³€ í—ˆìš©
        'temperature': 0.9,
        'top_p': 0.9
    }
    
    # ê³ ì •ëœ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
    prompt_template = (
        "Task Instruction: Given certain text, you need to predict the next word of it. Moreover, before your output, you could first give short thoughts about how you infer the next word based on the provided context.\\n"
        "Here are five examples for the task:\\n"
        "Example 0: {\"ìš°ë¦¬ëŠ” ê°€ë” ì˜¨ë¼ì¸ ì¿ í°ê³¼ ê¸°íƒ€ íŠ¹ë³„ í˜œíƒì„ ì œê³µí•©ë‹ˆë‹¤. <hCoT> Customers can explore additional ways to find deals beyond online coupons, like subscribing. </hCoT> ë˜ëŠ” ì œí’ˆ ì—°êµ¬ì— ì°¸ì—¬í•˜ê³  ì‹¶ë‹¤ë©´ 'í™ˆ ì œí’ˆ ë°°ì¹˜'ë¥¼ ì²´í¬í•˜ê³  ëª‡ ê°€ì§€ ì§ˆë¬¸ì— ë‹µí•´ì£¼ì„¸ìš”. ë¬´ì—‡ì„ ê¸°ë‹¤ë¦¬ê³  ìˆë‚˜ìš”?\"}\\n\\n"
        "Example 1: {\"ë°©ì •ì‹ 2x + 5 = 17ì„ í’€ì–´ë³´ì„¸ìš”. ë¨¼ì € ì–‘ë³€ì—ì„œ 5ë¥¼ <hCoT> The context presents an equation 2x + 5 = 17 and mentions subtracting 5 from both sides, so the next word should be 'ë¹¼ë©´' to describe the subtraction operation. </hCoT> ë¹¼ë©´ 2x = 12ê°€ ë©ë‹ˆë‹¤. ê·¸ ë‹¤ìŒ ì–‘ë³€ì„ 2ë¡œ <hCoT> The context shows 2x = 12 and mentions dividing both sides by 2, so the next word should be 'ë‚˜ëˆ„ë©´' to complete the division step. </hCoT> ë‚˜ëˆ„ë©´ x = 6ì´ ë‹µì…ë‹ˆë‹¤.\"}\\n\\n"
        "Example 2: {\"Unityì—ì„œ 2D ê°ì²´ë¥¼ ë“œë˜ê·¸í•  ë•Œ ë‹¤ë¥¸ ê°ì²´ì™€ì˜ ìµœì†Œ ê±°ë¦¬ëŠ” 1.5fì…ë‹ˆë‹¤. ë‘ ê°ì²´ê°€ <hCoT> The context describes distance constraints for 2D objects in Unity, so the next word should be 'ì—°ê²°ë˜ë©´' to describe what happens when objects connect. </hCoT> ì—°ê²°ë˜ë©´ ë“œë˜ê·¸ê°€ ë” ì œí•œë©ë‹ˆë‹¤.\"}\\n\\n"
        "Example 3: {\"ëŒ€ìˆ˜í•™ì—ì„œ ëŒ€ì²´ëŠ” ë¬¸ìë¥¼ ìˆ«ìë¡œ ë°”ê¾¸ëŠ” ê²ƒì…ë‹ˆë‹¤. ìˆ«ìì™€ <hCoT> The context explains algebraic substitution involving numbers, so the next word should be 'ë¬¸ì' as algebra deals with both numbers and variables. </hCoT> ë¬¸ì ì‚¬ì´ì—ëŠ” ê³±ì…ˆ ê¸°í˜¸ê°€ ìˆ¨ê²¨ì ¸ ìˆìŠµë‹ˆë‹¤.\"}\\n\\n"
        "Example 4: {\"ëœë‹¬ì¦ˆë¹Œ ì´ì‚¬ íšŒì‚¬ Movers MAX ë””ë ‰í† ë¦¬ëŠ” <hCoT> The context introduces a moving company directory called Movers MAX, so the next word should be 'ì´ì‚¬' to specify what kind of resources this directory provides. </hCoT> ì´ì‚¬ ìì›ì„ ìœ„í•œ ì›ìŠ¤í†± ì†ŒìŠ¤ì…ë‹ˆë‹¤.\"}\\n\\n"
        "Now please give me your prediction for the thought and next word based on the following context:\\n\\n"
        "{<user_input_context>}\\n\\n"
        "Thought:\\n"
        "Next Word:"
    )
    
    conversation_count = 0
    
    while True:
        try:
            # ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
            user_input = input(f"\n[{conversation_count + 1}] ë‹¹ì‹ : ").strip()
            
            # ì¢…ë£Œ ëª…ë ¹ì–´ ì²´í¬
            if user_input.lower() in ['quit', 'exit', 'ì¢…ë£Œ', 'q']:
                print("\nğŸ‘‹ ì±„íŒ…ì„ ì¢…ë£Œí•©ë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤!")
                break
            
            # í™”ë©´ ì •ë¦¬ ëª…ë ¹ì–´
            elif user_input.lower() in ['clear', 'í´ë¦¬ì–´']:
                import os
                os.system('clear' if os.name == 'posix' else 'cls')
                continue
            
            # ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
            elif user_input.lower() in ['memory', 'ë©”ëª¨ë¦¬']:
                if torch.cuda.is_available():
                    allocated = torch.cuda.memory_allocated() / 1024**3
                    cached = torch.cuda.memory_reserved() / 1024**3
                    total = torch.cuda.get_device_properties(0).total_memory / 1024**3
                    print(f"ğŸ“Š GPU ë©”ëª¨ë¦¬ ìƒíƒœ:")
                    print(f"  - í• ë‹¹ë¨: {allocated:.1f} GB")
                    print(f"  - ìºì‹œë¨: {cached:.1f} GB") 
                    print(f"  - ì „ì²´: {total:.1f} GB")
                    print(f"  - ì‚¬ìš©ë¥ : {(allocated/total)*100:.1f}%")
                else:
                    print("CUDAê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.")
                continue
            
            # ì„¤ì • ë³€ê²½
            elif user_input.lower() in ['settings', 'ì„¤ì •']:
                print(f"\nâš™ï¸ í˜„ì¬ ì„¤ì •:")
                print(f"  - max_new_tokens: {settings['max_new_tokens']}")
                print(f"  - temperature: {settings['temperature']}")
                print(f"  - top_p: {settings['top_p']}")
                
                try:
                    new_max_tokens = input(f"ìƒˆë¡œìš´ max_new_tokens (í˜„ì¬: {settings['max_new_tokens']}): ").strip()
                    if new_max_tokens:
                        settings['max_new_tokens'] = int(new_max_tokens)
                    
                    new_temp = input(f"ìƒˆë¡œìš´ temperature (í˜„ì¬: {settings['temperature']}): ").strip()
                    if new_temp:
                        settings['temperature'] = float(new_temp)
                    
                    new_top_p = input(f"ìƒˆë¡œìš´ top_p (í˜„ì¬: {settings['top_p']}): ").strip()
                    if new_top_p:
                        settings['top_p'] = float(new_top_p)
                    
                    print("âœ… ì„¤ì •ì´ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤!")
                except ValueError:
                    print("âŒ ì˜ëª»ëœ ê°’ì…ë‹ˆë‹¤. ì„¤ì •ì´ ë³€ê²½ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                continue
            
            # ë¹ˆ ì…ë ¥ ì²´í¬
            elif not user_input:
                print("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                continue
            
            # ì‚¬ìš©ì ì…ë ¥ì„ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì— ì¹˜í™˜
            formatted_prompt = prompt_template.replace("<user_input_context>", user_input)
            
            # DeepSeekì—ê²Œ ì§ˆë¬¸
            print(f"\nğŸ¤– DeepSeek ë‹µë³€ ìƒì„± ì¤‘...")
            
            answer = ask_deepseek(
                formatted_prompt, 
                model, 
                tokenizer, 
                device,
                max_new_tokens=settings['max_new_tokens'],
                temperature=settings['temperature'],
                top_p=settings['top_p']
            )
            
            print(f"\nğŸ¤– DeepSeek: {answer}")
            conversation_count += 1
            
            # ì£¼ê¸°ì ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì •ë¦¬ (10ë²ˆ ëŒ€í™”ë§ˆë‹¤)
            if conversation_count % 10 == 0:
                print("\nğŸ”„ ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘...")
                clear_memory()
                print("âœ… ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
            
        except KeyboardInterrupt:
            print("\n\nâš ï¸ Ctrl+Cê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
            user_choice = input("ì •ë§ ì¢…ë£Œí•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").strip().lower()
            if user_choice in ['y', 'yes', 'ì˜ˆ']:
                break
            else:
                print("ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤...")
                continue
        except Exception as e:
            print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            print("ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
            continue

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    interactive_chat()

if __name__ == "__main__":
    main()