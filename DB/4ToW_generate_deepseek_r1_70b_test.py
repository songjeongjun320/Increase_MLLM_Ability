# -*- coding: utf-8 -*-
import torch
import gc
import os
from transformers import AutoTokenizer, AutoModelForCausalLM
from typing import Optional, Dict, Any
import warnings
warnings.filterwarnings("ignore")

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (ì„±ëŠ¥ ìµœì í™”)
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["CUDA_LAUNCH_BLOCKING"] = "0"
os.environ["TRANSFORMERS_OFFLINE"] = "1"  # ì˜¤í”„ë¼ì¸ ëª¨ë“œ ê°•ì œ

# bitsandbytes ê°€ì ¸ì˜¤ê¸° ì‹œë„
try:
    from transformers import BitsAndBytesConfig
    
    BITSANDBYTES_AVAILABLE = True
    print("âœ… BitsAndBytesConfigë¥¼ ì„±ê³µì ìœ¼ë¡œ importí–ˆìŠµë‹ˆë‹¤.")
except ImportError:
    BITSANDBYTES_AVAILABLE = False
    print("âš ï¸ BitsAndBytesConfig import ì‹¤íŒ¨ - FP16 ì–‘ìí™”ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")

def setup_torch_optimizations():
    """PyTorch ìµœì í™” ì„¤ì •"""
    # ë©”ëª¨ë¦¬ ê´€ë¦¬ ìµœì í™”
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True
    torch.backends.cudnn.benchmark = True
    torch.backends.cudnn.deterministic = False
    
    # CUDA ìºì‹œ ìµœì í™”
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        # ë©”ëª¨ë¦¬ í”„ë˜ê·¸ë©˜í…Œì´ì…˜ ë°©ì§€
        os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:512,expandable_segments:True"

def clear_memory():
    """íš¨ìœ¨ì ì¸ ë©”ëª¨ë¦¬ ì •ë¦¬ í•¨ìˆ˜"""
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()

class OptimizedDeepSeekChat:
    def __init__(self, model_path: str):
        self.model_path = model_path
        self.model = None
        self.tokenizer = None
        self.device = None
        self.generation_config = None
        
        # ìºì‹œëœ í† í° ì‹œí€€ìŠ¤ë“¤
        self.cached_tokens = {}
        
        # ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
        self.prompt_template = """Task Instruction: Given certain text, you need to predict the next word of it. Moreover, before your output, you could first give short thoughts about how you infer the next word based on the provided context.

Here are five examples for the task:

Example 0: ìš°ë¦¬ëŠ” ê°€ë” ì˜¨ë¼ì¸ ì¿ í°ê³¼ ê¸°íƒ€ íŠ¹ë³„ í˜œíƒì„ ì œê³µí•©ë‹ˆë‹¤. <hCoT> Customers can explore additional ways to find deals beyond online coupons, like subscribing. </hCoT> ë˜ëŠ” ì œí’ˆ ì—°êµ¬ì— ì°¸ì—¬í•˜ê³  ì‹¶ë‹¤ë©´ 'í™ˆ ì œí’ˆ ë°°ì¹˜'ë¥¼ ì²´í¬í•˜ê³  ëª‡ ê°€ì§€ ì§ˆë¬¸ì— ë‹µí•´ì£¼ì„¸ìš”. ë¬´ì—‡ì„ ê¸°ë‹¤ë¦¬ê³  ìˆë‚˜ìš”?

Example 1: ë°©ì •ì‹ 2x + 5 = 17ì„ í’€ì–´ë³´ì„¸ìš”. ë¨¼ì € ì–‘ë³€ì—ì„œ 5ë¥¼ <hCoT> The context presents an equation 2x + 5 = 17 and mentions subtracting 5 from both sides, so the next word should be 'ë¹¼ë©´' to describe the subtraction operation. </hCoT> ë¹¼ë©´ 2x = 12ê°€ ë©ë‹ˆë‹¤. ê·¸ ë‹¤ìŒ ì–‘ë³€ì„ 2ë¡œ <hCoT> The context shows 2x = 12 and mentions dividing both sides by 2, so the next word should be 'ë‚˜ëˆ„ë©´' to complete the division step. </hCoT> ë‚˜ëˆ„ë©´ x = 6ì´ ë‹µì…ë‹ˆë‹¤.

Example 2: Unityì—ì„œ 2D ê°ì²´ë¥¼ ë“œë˜ê·¸í•  ë•Œ ë‹¤ë¥¸ ê°ì²´ì™€ì˜ ìµœì†Œ ê±°ë¦¬ëŠ” 1.5fì…ë‹ˆë‹¤. ë‘ ê°ì²´ê°€ <hCoT> The context describes distance constraints for 2D objects in Unity, so the next word should be 'ì—°ê²°ë˜ë©´' to describe what happens when objects connect. </hCoT> ì—°ê²°ë˜ë©´ ë“œë˜ê·¸ê°€ ë” ì œí•œë©ë‹ˆë‹¤.

Example 3: ëŒ€ìˆ˜í•™ì—ì„œ ëŒ€ì²´ëŠ” ë¬¸ìë¥¼ ìˆ«ìë¡œ ë°”ê¾¸ëŠ” ê²ƒì…ë‹ˆë‹¤. ìˆ«ìì™€ <hCoT> The context explains algebraic substitution involving numbers, so the next word should be 'ë¬¸ì' as algebra deals with both numbers and variables. </hCoT> ë¬¸ì ì‚¬ì´ì—ëŠ” ê³±ì…ˆ ê¸°í˜¸ê°€ ìˆ¨ê²¨ì ¸ ìˆìŠµë‹ˆë‹¤.

Example 4: ëœë‹¬ì¦ˆë¹Œ ì´ì‚¬ íšŒì‚¬ Movers MAX ë””ë ‰í† ë¦¬ëŠ” <hCoT> The context introduces a moving company directory called Movers MAX, so the next word should be 'ì´ì‚¬' to specify what kind of resources this directory provides. </hCoT> ì´ì‚¬ ìì›ì„ ìœ„í•œ ì›ìŠ¤í†± ì†ŒìŠ¤ì…ë‹ˆë‹¤.

Now please give me your prediction for the thought and next word based on the following context:

{user_input_context}

Thought:
Next Word:"""
        
        # ì„±ëŠ¥ ìµœì í™” ì„¤ì •
        setup_torch_optimizations()
        
    def load_model(self) -> bool:
        """ìµœì í™”ëœ ëª¨ë¸ ë¡œë”©"""
        print(f"ğŸš€ ë¡œì»¬ ê²½ë¡œ '{self.model_path}'ì—ì„œ ëª¨ë¸ ë¡œë”© ì¤‘...")
        print("ğŸ”§ A100 ìµœì í™” ì„¤ì • ì ìš©... (ì•ˆì •ì ì¸ eager attention)")

        try:
            # ëª¨ë¸ ì„¤ì • íŒŒì¼ í™•ì¸
            config_path = os.path.join(self.model_path, "config.json")
            if not os.path.exists(config_path):
                print(f"âŒ config.jsonì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {config_path}")
                return False
            
            # í† í¬ë‚˜ì´ì € ë¡œë”©
            print("ğŸ“ í† í¬ë‚˜ì´ì € ë¡œë”© ì¤‘...")
            try:
                self.tokenizer = AutoTokenizer.from_pretrained(
                    self.model_path, 
                    trust_remote_code=True, 
                    local_files_only=True,
                    use_fast=False,  # fast tokenizer ë¹„í™œì„±í™”ë¡œ í˜¸í™˜ì„± ê°œì„ 
                    padding_side="left"
                )
                print("âœ… AutoTokenizerë¡œ ë¡œë”© ì„±ê³µ")
            except Exception as e:
                print(f"âŒ AutoTokenizer ë¡œë”© ì‹¤íŒ¨: {e}")
                return False
            
            # pad_token ì„¤ì •
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # ë””ë°”ì´ìŠ¤ ì„¤ì •
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            print(f"ğŸ¯ ì‚¬ìš© ì¥ì¹˜: {self.device}")
            
            # GPU ë©”ëª¨ë¦¬ ì •ë³´
            if torch.cuda.is_available():
                gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
                print(f"ğŸ’¾ GPU ë©”ëª¨ë¦¬: {gpu_memory:.1f} GB")

            # ëª¨ë¸ ë¡œë”©
            try:
                model_kwargs = self._get_optimized_model_config()
                print("ğŸ”¥ AutoModelForCausalLMìœ¼ë¡œ ëª¨ë¸ ë¡œë”© ì‹œì‘...")
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_path, 
                    **model_kwargs
                )
                print("âœ… AutoModelForCausalLMìœ¼ë¡œ ë¡œë”© ì„±ê³µ")
            except Exception as e:
                print(f"âš ï¸ ì–‘ìí™” ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨, ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì¬ì‹œë„: {e}")
                # ì–‘ìí™” ì—†ì´ ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì¬ì‹œë„
                try:
                    basic_kwargs = {
                        "trust_remote_code": True,
                        "local_files_only": True,
                        "low_cpu_mem_usage": True,
                        "device_map": "auto",
                        "torch_dtype": torch.float16,
                        "attn_implementation": "eager",
                    }
                    self.model = AutoModelForCausalLM.from_pretrained(
                        self.model_path,
                        **basic_kwargs
                    )
                    print("âœ… ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ë¡œë”© ì„±ê³µ")
                except Exception as e2:
                    print(f"âŒ ëª¨ë¸ ë¡œë”© ì™„ì „ ì‹¤íŒ¨: {e2}")
                    return False
            
            # ì¶”ë¡  ìµœì í™”
            self.model.eval()
            
            # Torch compile ì‚¬ìš© (PyTorch 2.0+) - ì„ íƒì  ì ìš©
            if hasattr(torch, 'compile') and torch.cuda.is_available():
                print("âš¡ Torch compile ì ìš© ì‹œë„ ì¤‘...")
                try:
                    self.model = torch.compile(self.model, mode="reduce-overhead")
                    print("âœ… Torch compile ì ìš© ì™„ë£Œ")
                except Exception as e:
                    print(f"âš ï¸ Torch compile ì‹¤íŒ¨ (ë¬´ì‹œí•˜ê³  ê³„ì†): {e}")
            
            # ìƒì„± ì„¤ì • ìµœì í™”
            self._setup_generation_config()
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            clear_memory()
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
            if torch.cuda.is_available():
                allocated = torch.cuda.memory_allocated() / 1024**3
                cached = torch.cuda.memory_reserved() / 1024**3
                print(f"ğŸ“Š GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {allocated:.1f} GB allocated, {cached:.1f} GB cached")
            
            print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
            return True
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def _get_optimized_model_config(self) -> Dict[str, Any]:
        """ìµœì í™”ëœ ëª¨ë¸ ì„¤ì • ë°˜í™˜ (í˜¸í™˜ì„± ê°œì„ )"""
        base_config = {
            "trust_remote_code": True,
            "local_files_only": True,
            "low_cpu_mem_usage": True,
            "device_map": "auto",
            "attn_implementation": "eager",  # FlashAttention ëŒ€ì‹  ì•ˆì •ì ì¸ eager ì‚¬ìš©
        }
        
        # ì–‘ìí™” ì„¤ì •
        if BITSANDBYTES_AVAILABLE and torch.cuda.is_available():
            print("ğŸ”§ 4-bit ì–‘ìí™” ì„¤ì • (BitsAndBytes)")
            try:
                bnb_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_use_double_quant=True,
                    bnb_4bit_quant_type="nf4",
                    bnb_4bit_compute_dtype=torch.bfloat16,
                )
                base_config.update({
                    "quantization_config": bnb_config,
                    "torch_dtype": torch.bfloat16,
                })
            except Exception as e:
                print(f"âš ï¸ ì–‘ìí™” ì„¤ì • ì‹¤íŒ¨, FP16 ì‚¬ìš©: {e}")
                base_config["torch_dtype"] = torch.float16
        else:
            print("ğŸ”§ FP16 ì„¤ì •")
            base_config["torch_dtype"] = torch.float16
        
        return base_config
    
    def _setup_generation_config(self):
        """ìµœì í™”ëœ ìƒì„± ì„¤ì •"""
        self.generation_config = {
            "do_sample": True,
            "temperature": 0.7,
            "top_p": 0.9,
            "top_k": 50,
            "repetition_penalty": 1.05,
            "pad_token_id": self.tokenizer.pad_token_id,
            "eos_token_id": self.tokenizer.eos_token_id,
            "use_cache": True,
            "num_beams": 1,  # ë¹ ë¥¸ ì¶”ë¡ ì„ ìœ„í•´ beam search ë¹„í™œì„±í™”
        }
    
    def _build_optimized_prompt(self, user_input_context: str) -> str:
        """ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ì‚¬ìš©í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        # DeepSeek í˜•ì‹ í™•ì¸ í›„ ì ìš©
        if hasattr(self.tokenizer, 'chat_template') and self.tokenizer.chat_template:
            # ì±„íŒ… í…œí”Œë¦¿ì´ ìˆëŠ” ê²½ìš°
            messages = [{"role": "user", "content": self.prompt_template.format(user_input_context=user_input_context)}]
            try:
                full_prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            except:
                # ì±„íŒ… í…œí”Œë¦¿ ì ìš© ì‹¤íŒ¨ì‹œ ê¸°ë³¸ í˜•ì‹ ì‚¬ìš©
                full_prompt = f"""<ï½œstartâ–headerâ–idï½œ>user<ï½œendâ–headerâ–idï½œ>

{self.prompt_template.format(user_input_context=user_input_context)}<ï½œeotâ–idï½œ><ï½œstartâ–headerâ–idï½œ>assistant<ï½œendâ–headerâ–idï½œ>

"""
        else:
            # ê¸°ë³¸ DeepSeek í˜•ì‹
            full_prompt = f"""<ï½œstartâ–headerâ–idï½œ>user<ï½œendâ–headerâ–idï½œ>

{self.prompt_template.format(user_input_context=user_input_context)}<ï½œeotâ–idï½œ><ï½œstartâ–headerâ–idï½œ>assistant<ï½œendâ–headerâ–idï½œ>

"""
        return full_prompt
    
    def ask_deepseek(self, user_input_context: str, max_new_tokens: int = 1024, **kwargs) -> str:
        """ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•œ DeepSeek ì¶”ë¡ """
        if self.model is None or self.tokenizer is None:
            return "âŒ ëª¨ë¸ì´ ë¡œë”©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        # ìƒì„± ì„¤ì • ì—…ë°ì´íŠ¸
        generation_config = self.generation_config.copy()
        generation_config.update(kwargs)
        generation_config["max_new_tokens"] = max_new_tokens
        
        # ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = self._build_optimized_prompt(user_input_context)
        
        # í† í¬ë‚˜ì´ì§• (ë°°ì¹˜ ì²˜ë¦¬ ì¤€ë¹„)
        try:
            inputs = self.tokenizer(
                prompt, 
                return_tensors="pt", 
                return_attention_mask=True,
                truncation=True,
                max_length=4096,  # ìµœëŒ€ ê¸¸ì´ ì œí•œ
                padding=False
            )
        except Exception as e:
            print(f"âŒ í† í¬ë‚˜ì´ì§• ì‹¤íŒ¨: {e}")
            return "í† í¬ë‚˜ì´ì§• ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        
        input_ids = inputs.input_ids.to(self.device)
        attention_mask = inputs.attention_mask.to(self.device)
        
        print(f"ğŸ¤” ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì¤‘: {user_input_context[:50]}{'...' if len(user_input_context) > 50 else ''}")
        
        try:
            # ìµœì í™”ëœ ì¶”ë¡ 
            with torch.no_grad():
                with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):  # Mixed precision
                    generated_ids = self.model.generate(
                        input_ids=input_ids,
                        attention_mask=attention_mask,
                        **generation_config
                    )
            
            # ì‘ë‹µ ë””ì½”ë”©
            answer_ids = generated_ids[0][input_ids.shape[-1]:]
            full_response = self.tokenizer.decode(answer_ids, skip_special_tokens=True)
            
            # ìµœì¢… ë‹µë³€ ì¶”ì¶œ
            final_answer = self._extract_final_answer(full_response)
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            del input_ids, attention_mask, generated_ids
            clear_memory()
            
            return final_answer
            
        except Exception as e:
            print(f"âŒ ì¶”ë¡  ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            clear_memory()
            return f"ì£„ì†¡í•©ë‹ˆë‹¤. ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    def _extract_final_answer(self, response: str) -> str:
        """DeepSeek-R1 ì‘ë‹µì—ì„œ ìµœì¢… ë‹µë³€ ì¶”ì¶œ (ìµœì í™”)"""
        # thinking íƒœê·¸ ì œê±°
        if "<ï½œthinkingï½œ>" in response and "<ï½œ/thinkingï½œ>" in response:
            parts = response.split("<ï½œ/thinkingï½œ>")
            final_answer = parts[1].strip() if len(parts) > 1 else response.strip()
        else:
            final_answer = response.strip()
        
        # ë¶ˆí•„ìš”í•œ í† í°ë“¤ ì¼ê´„ ì œê±°
        unwanted_tokens = ["<ï½œthinkingï½œ>", "<ï½œ/thinkingï½œ>", "<ï½œeotâ–idï½œ>", "<ï½œendâ–ofâ–textï½œ>"]
        for token in unwanted_tokens:
            final_answer = final_answer.replace(token, "")
        
        return final_answer.strip()
    
    def get_memory_stats(self) -> Dict[str, float]:
        """ë©”ëª¨ë¦¬ ìƒíƒœ ë°˜í™˜"""
        if not torch.cuda.is_available():
            return {"error": "CUDA ì‚¬ìš© ë¶ˆê°€"}
        
        allocated = torch.cuda.memory_allocated() / 1024**3
        cached = torch.cuda.memory_reserved() / 1024**3
        total = torch.cuda.get_device_properties(0).total_memory / 1024**3
        
        return {
            "allocated_gb": allocated,
            "cached_gb": cached,
            "total_gb": total,
            "usage_percent": (allocated / total) * 100
        }

def check_model_files(model_path: str):
    """ëª¨ë¸ íŒŒì¼ êµ¬ì¡° í™•ì¸"""
    print(f"ğŸ” ëª¨ë¸ íŒŒì¼ êµ¬ì¡° í™•ì¸: {model_path}")
    
    required_files = [
        "config.json",
        "tokenizer.json",
        "tokenizer_config.json",
    ]
    
    optional_files = [
        "tokenizer.model",
        "vocab.txt",
        "merges.txt"
    ]
    
    for file in required_files:
        file_path = os.path.join(model_path, file)
        if os.path.exists(file_path):
            print(f"âœ… {file} ì¡´ì¬")
        else:
            print(f"âŒ {file} ëˆ„ë½")
    
    for file in optional_files:
        file_path = os.path.join(model_path, file)
        if os.path.exists(file_path):
            print(f"âœ… {file} ì¡´ì¬ (ì„ íƒì‚¬í•­)")
    
    # ëª¨ë¸ ê°€ì¤‘ì¹˜ íŒŒì¼ í™•ì¸
    weight_files = [f for f in os.listdir(model_path) if f.endswith(('.bin', '.safetensors'))]
    print(f"ğŸ“¦ ëª¨ë¸ ê°€ì¤‘ì¹˜ íŒŒì¼: {len(weight_files)}ê°œ")
    for file in weight_files[:5]:  # ì²˜ìŒ 5ê°œë§Œ í‘œì‹œ
        print(f"  - {file}")
    if len(weight_files) > 5:
        print(f"  ... ë° {len(weight_files) - 5}ê°œ ë”")

def interactive_chat():
    """ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•œ ëŒ€í™”í˜• ì±„íŒ… ì‹œìŠ¤í…œ"""
    print("=" * 70)
    print("ğŸš€ DeepSeek ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡ ì‹œìŠ¤í…œ (ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸)")
    print("=" * 70)
    
    # ëª¨ë¸ ê²½ë¡œ
    model_path = "/scratch/jsong132/Increase_MLLM_Ability/DeepSeek_R1_Distill_Llama_70B"
    
    # ëª¨ë¸ íŒŒì¼ êµ¬ì¡° í™•ì¸
    check_model_files(model_path)
    
    # ìµœì í™”ëœ ì±„íŒ… ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    chat_system = OptimizedDeepSeekChat(model_path)
    
    # ëª¨ë¸ ë¡œë”©
    if not chat_system.load_model():
        print("âŒ ëª¨ë¸ ë¡œë”©ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return
    
    print("\nâœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ! (ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡ ëª¨ë“œ)")
    print("ğŸ’¡ ì‚¬ìš©ë²•: ë¬¸ë§¥ì„ ì…ë ¥í•˜ë©´ ë‹¤ìŒì— ì˜¬ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤.")
    print("ğŸ’¡ ëª…ë ¹ì–´:")
    print("  - 'quit', 'exit', 'ì¢…ë£Œ' : í”„ë¡œê·¸ë¨ ì¢…ë£Œ")
    print("  - 'clear', 'í´ë¦¬ì–´' : í™”ë©´ ì •ë¦¬")
    print("  - 'memory', 'ë©”ëª¨ë¦¬' : GPU ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸")
    print("  - 'example', 'ì˜ˆì‹œ' : ì‚¬ìš© ì˜ˆì‹œ ë³´ê¸°")
    print("-" * 70)
    
    # ì„±ëŠ¥ ì„¤ì •
    settings = {
        'max_new_tokens': 512,  # ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡ì´ë¯€ë¡œ ì§§ê²Œ
        'temperature': 0.3,     # ë” í™•ì •ì ì¸ ì˜ˆì¸¡ì„ ìœ„í•´ ë‚®ê²Œ
        'top_p': 0.9,
    }
    
    conversation_count = 0
    
    # ì²« ë²ˆì§¸ ì¶”ë¡  ì›Œë°ì—…
    print("ğŸ”¥ ëª¨ë¸ ì›Œë°ì—… ì¤‘...")
    try:
        warmup_response = chat_system.ask_deepseek("ì•ˆë…•í•˜ì„¸ìš”. ì˜¤ëŠ˜", max_new_tokens=50)
        print(f"ğŸ”¥ ì›Œë°ì—… ì™„ë£Œ")
    except Exception as e:
        print(f"âš ï¸ ì›Œë°ì—… ì¤‘ ì˜¤ë¥˜: {e}")
    
    while True:
        try:
            user_input = input(f"\n[{conversation_count + 1}] ë¬¸ë§¥ ì…ë ¥: ").strip()
            
            # ëª…ë ¹ì–´ ì²˜ë¦¬
            if user_input.lower() in ['quit', 'exit', 'ì¢…ë£Œ', 'q']:
                print("\nğŸ‘‹ ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡ ì‹œìŠ¤í…œì„ ì¢…ë£Œí•©ë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤!")
                break
            elif user_input.lower() in ['clear', 'í´ë¦¬ì–´']:
                os.system('clear' if os.name == 'posix' else 'cls')
                continue
            elif user_input.lower() in ['memory', 'ë©”ëª¨ë¦¬']:
                stats = chat_system.get_memory_stats()
                if "error" not in stats:
                    print(f"ğŸ“Š GPU ë©”ëª¨ë¦¬ ìƒíƒœ:")
                    print(f"  - í• ë‹¹ë¨: {stats['allocated_gb']:.1f} GB")
                    print(f"  - ìºì‹œë¨: {stats['cached_gb']:.1f} GB")
                    print(f"  - ì „ì²´: {stats['total_gb']:.1f} GB")
                    print(f"  - ì‚¬ìš©ë¥ : {stats['usage_percent']:.1f}%")
                else:
                    print("CUDAê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.")
                continue
            elif user_input.lower() in ['example', 'ì˜ˆì‹œ']:
                print("ğŸ“‹ ì‚¬ìš© ì˜ˆì‹œ:")
                print("  ì…ë ¥: 'ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì •ë§'")
                print("  ì¶œë ¥: Thought: [ëª¨ë¸ì˜ ì¶”ë¡  ê³¼ì •] Next Word: ì¢‹ë„¤ìš”")
                print("")
                print("  ì…ë ¥: 'íŒŒì´ì¬ì—ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼'")
                print("  ì¶œë ¥: Thought: [ëª¨ë¸ì˜ ì¶”ë¡  ê³¼ì •] Next Word: ìƒì„±í•˜ë ¤ë©´")
                continue
            elif not user_input:
                print("ë¬¸ë§¥ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                continue
            
            # DeepSeekì—ê²Œ ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡ ìš”ì²­
            import time
            start_time = time.time()
            
            answer = chat_system.ask_deepseek(
                user_input,  # user_input_contextë¡œ ì‚¬ìš©
                max_new_tokens=settings['max_new_tokens'],
                temperature=settings['temperature'],
                top_p=settings['top_p']
            )
            
            end_time = time.time()
            response_time = end_time - start_time
            
            print(f"\nğŸ¤– DeepSeek ì˜ˆì¸¡ ê²°ê³¼ ({response_time:.1f}ì´ˆ):")
            print(f"{answer}")
            conversation_count += 1
            
            # ì£¼ê¸°ì  ë©”ëª¨ë¦¬ ì •ë¦¬
            if conversation_count % 5 == 0:
                clear_memory()
            
        except KeyboardInterrupt:
            print("\n\nâš ï¸ Ctrl+Cê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
            user_choice = input("ì •ë§ ì¢…ë£Œí•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").strip().lower()
            if user_choice in ['y', 'yes', 'ì˜ˆ']:
                break
            else:
                print("ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤...")
                continue
        except Exception as e:
            print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            print("ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
            continue

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # ì‹œìŠ¤í…œ ì²´í¬
    print("ğŸ” ì‹œìŠ¤í…œ í™˜ê²½ ì²´í¬...")
    print(f"PyTorch ë²„ì „: {torch.__version__}")
    print(f"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"CUDA ë²„ì „: {torch.version.cuda}")
        print(f"GPU ì¥ì¹˜: {torch.cuda.get_device_name()}")
    
    # Transformers ë²„ì „ í™•ì¸
    try:
        import transformers
        print(f"Transformers ë²„ì „: {transformers.__version__}")
    except:
        pass
    
    interactive_chat()

if __name__ == "__main__":
    main()