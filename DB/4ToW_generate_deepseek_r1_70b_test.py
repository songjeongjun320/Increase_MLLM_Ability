# -*- coding: utf-8 -*-
import torch
import gc
import os
from transformers import AutoTokenizer, AutoModelForCausalLM
from typing import Optional, Dict, Any
import warnings
warnings.filterwarnings("ignore")

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (ì„±ëŠ¥ ìµœì í™”)
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["CUDA_LAUNCH_BLOCKING"] = "0"
os.environ["TRANSFORMERS_OFFLINE"] = "1"  # ì˜¤í”„ë¼ì¸ ëª¨ë“œ ê°•ì œ

# ë©€í‹° GPU ì„¤ì • - A100 80GB x2 ìµœì í™”
os.environ["CUDA_VISIBLE_DEVICES"] = "0,1"  # GPU 0, 1 ì‚¬ìš©

# A100 80GB ìµœì í™”ë¥¼ ìœ„í•œ ì¶”ê°€ í™˜ê²½ ë³€ìˆ˜
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:2048,expandable_segments:True,roundup_power2_divisions:8"
os.environ["NCCL_DEBUG"] = "WARN"  # ë©€í‹° GPU í†µì‹  ìµœì í™”

# bitsandbytes ê°€ì ¸ì˜¤ê¸° ì‹œë„
try:
    from transformers import BitsAndBytesConfig
    BITSANDBYTES_AVAILABLE = True
    print("âœ… BitsAndBytesConfig successfully imported")
except ImportError:
    BITSANDBYTES_AVAILABLE = False
    print("âš ï¸ BitsAndBytesConfig import failed - using FP16 quantization")

# FlashAttention ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
try:
    import flash_attn
    FLASH_ATTENTION_AVAILABLE = True
    print("âœ… FlashAttention available for maximum performance")
except ImportError:
    FLASH_ATTENTION_AVAILABLE = False
    print("âš ï¸ FlashAttention not available - using optimized attention")

def setup_torch_optimizations():
    """PyTorch ìµœì í™” ì„¤ì • - A100 80GB íŠ¹í™”"""
    # ë©”ëª¨ë¦¬ ê´€ë¦¬ ìµœì í™”
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True
    torch.backends.cudnn.benchmark = True
    torch.backends.cudnn.deterministic = False
    torch.backends.cuda.enable_flash_sdp(True)  # Scaled Dot Product Attention ìµœì í™”
    
    # CUDA ìºì‹œ ìµœì í™” - A100 80GB íŠ¹í™”
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        # A100 80GBë¥¼ ìœ„í•œ ë” í° ë©”ëª¨ë¦¬ í’€ ì„¤ì •
        torch.cuda.set_per_process_memory_fraction(0.95)  # 76GB ì‚¬ìš© ê°€ëŠ¥

def clear_memory():
    """íš¨ìœ¨ì ì¸ ë©”ëª¨ë¦¬ ì •ë¦¬ í•¨ìˆ˜ (A100 80GB x2)"""
    gc.collect()
    if torch.cuda.is_available():
        for i in range(torch.cuda.device_count()):
            with torch.cuda.device(i):
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
                torch.cuda.ipc_collect()  # IPC ë©”ëª¨ë¦¬ ì •ë¦¬

def get_multi_gpu_device_map(num_gpus: int = 2):
    """A100 80GB x2 ìµœì  ë””ë°”ì´ìŠ¤ ë§µ ìƒì„±"""
    if num_gpus == 2:
        # A100 80GB x2 ìµœì  ë¶„ë°° - 70GB í™œìš©
        device_map = {
            "model.embed_tokens": 0,
            "model.norm": 1,
            "lm_head": 1,
        }
        
        # ë ˆì´ì–´ë¥¼ ë‘ GPUì— ê· ë“± ë¶„ë°° (70B ëª¨ë¸ ê¸°ì¤€)
        num_layers = 80  # DeepSeek-R1 Distill Llama 70B ë ˆì´ì–´ ìˆ˜
        layers_per_gpu = num_layers // 2
        
        for i in range(num_layers):
            if i < layers_per_gpu:
                device_map[f"model.layers.{i}"] = 0
            else:
                device_map[f"model.layers.{i}"] = 1
        
        return device_map
    else:
        return "auto"

class OptimizedDeepSeekChat:
    def __init__(self, model_path: str, num_gpus: int = 2):
        self.model_path = model_path
        self.num_gpus = num_gpus
        self.model = None
        self.tokenizer = None
        self.device = None
        self.generation_config = None
        
        # ê³ ì„±ëŠ¥ ìºì‹œ ì‹œìŠ¤í…œ
        self.cached_tokens = {}
        self.kv_cache = None
        
        # ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
        self.prompt_template = """Task Instruction: Given certain text, you need to predict the next word of it. Moreover, before your output, you could first give short thoughts about how you infer the next word based on the provided context.

Here are five examples for the task:

Example 0: ìš°ë¦¬ëŠ” ê°€ë” ì˜¨ë¼ì¸ ì¿ í°ê³¼ ê¸°íƒ€ íŠ¹ë³„ í˜œíƒì„ ì œê³µí•©ë‹ˆë‹¤. <hCoT> Customers can explore additional ways to find deals beyond online coupons, like subscribing. </hCoT> ë˜ëŠ” ì œí’ˆ ì—°êµ¬ì— ì°¸ì—¬í•˜ê³  ì‹¶ë‹¤ë©´ 'í™ˆ ì œí’ˆ ë°°ì¹˜'ë¥¼ ì²´í¬í•˜ê³  ëª‡ ê°€ì§€ ì§ˆë¬¸ì— ë‹µí•´ì£¼ì„¸ìš”. ë¬´ì—‡ì„ ê¸°ë‹¤ë¦¬ê³  ìˆë‚˜ìš”?

Example 1: ë°©ì •ì‹ 2x + 5 = 17ì„ í’€ì–´ë³´ì„¸ìš”. ë¨¼ì € ì–‘ë³€ì—ì„œ 5ë¥¼ <hCoT> The context presents an equation 2x + 5 = 17 and mentions subtracting 5 from both sides, so the next word should be 'ë¹¼ë©´' to describe the subtraction operation. </hCoT> ë¹¼ë©´ 2x = 12ê°€ ë©ë‹ˆë‹¤. ê·¸ ë‹¤ìŒ ì–‘ë³€ì„ 2ë¡œ <hCoT> The context shows 2x = 12 and mentions dividing both sides by 2, so the next word should be 'ë‚˜ëˆ„ë©´' to complete the division step. </hCoT> ë‚˜ëˆ„ë©´ x = 6ì´ ë‹µì…ë‹ˆë‹¤.

Example 2: Unityì—ì„œ 2D ê°ì²´ë¥¼ ë“œë˜ê·¸í•  ë•Œ ë‹¤ë¥¸ ê°ì²´ì™€ì˜ ìµœì†Œ ê±°ë¦¬ëŠ” 1.5fì…ë‹ˆë‹¤. ë‘ ê°ì²´ê°€ <hCoT> The context describes distance constraints for 2D objects in Unity, so the next word should be 'ì—°ê²°ë˜ë©´' to describe what happens when objects connect. </hCoT> ì—°ê²°ë˜ë©´ ë“œë˜ê·¸ê°€ ë” ì œí•œë©ë‹ˆë‹¤.

Example 3: ëŒ€ìˆ˜í•™ì—ì„œ ëŒ€ì²´ëŠ” ë¬¸ìë¥¼ ìˆ«ìë¡œ ë°”ê¾¸ëŠ” ê²ƒì…ë‹ˆë‹¤. ìˆ«ìì™€ <hCoT> The context explains algebraic substitution involving numbers, so the next word should be 'ë¬¸ì' as algebra deals with both numbers and variables. </hCoT> ë¬¸ì ì‚¬ì´ì—ëŠ” ê³±ì…ˆ ê¸°í˜¸ê°€ ìˆ¨ê²¨ì ¸ ìˆìŠµë‹ˆë‹¤.

Example 4: ëœë‹¬ì¦ˆë¹Œ ì´ì‚¬ íšŒì‚¬ Movers MAX ë””ë ‰í† ë¦¬ëŠ” <hCoT> The context introduces a moving company directory called Movers MAX, so the next word should be 'ì´ì‚¬' to specify what kind of resources this directory provides. </hCoT> ì´ì‚¬ ìì›ì„ ìœ„í•œ ì›ìŠ¤í†± ì†ŒìŠ¤ì…ë‹ˆë‹¤.

Now please give me a pair of your prediction for the thought and next word based on the following context:

{user_input_context}

Thought:
Next Word:"""
        
        # A100 80GB ì„±ëŠ¥ ìµœì í™” ì„¤ì •
        setup_torch_optimizations()
        
    def load_model(self) -> bool:
        """A100 80GB x2 ìµœì í™”ëœ ëª¨ë¸ ë¡œë”©"""
        print(f"ğŸš€ Loading model from local path '{self.model_path}'...")
        print(f"ğŸ”§ Applying A100 80GB x{self.num_gpus} optimization settings...")

        # GPU ì •ë³´ ì¶œë ¥
        if torch.cuda.is_available():
            print(f"ğŸ¯ Available GPUs: {torch.cuda.device_count()}")
            for i in range(min(self.num_gpus, torch.cuda.device_count())):
                gpu_name = torch.cuda.get_device_name(i)
                gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
                print(f"  GPU {i}: {gpu_name} ({gpu_memory:.1f} GB)")
        else:
            print("âŒ CUDA is not available")
            return False

        try:
            # ëª¨ë¸ ì„¤ì • íŒŒì¼ í™•ì¸
            config_path = os.path.join(self.model_path, "config.json")
            if not os.path.exists(config_path):
                print(f"âŒ config.json not found: {config_path}")
                return False
            
            # í† í¬ë‚˜ì´ì € ë¡œë”©
            print("ğŸ“ Loading tokenizer...")
            try:
                self.tokenizer = AutoTokenizer.from_pretrained(
                    self.model_path, 
                    trust_remote_code=True, 
                    local_files_only=True,
                    use_fast=True,  # Fast tokenizer for A100 optimization
                    padding_side="left"
                )
                print("âœ… AutoTokenizer loaded successfully")
            except Exception as e:
                print(f"âŒ AutoTokenizer loading failed: {e}")
                return False
            
            # pad_token ì„¤ì •
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # A100 80GB x2 ë””ë°”ì´ìŠ¤ ë§µ ìƒì„±
            device_map = get_multi_gpu_device_map(self.num_gpus)
            print(f"ğŸ—ºï¸ A100 80GB x2 device map created")
            
            # ëª¨ë¸ ë¡œë”©
            try:
                model_kwargs = self._get_optimized_model_config(device_map)
                print("ğŸ”¥ Starting A100 80GB x2 optimized model loading...")
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_path, 
                    **model_kwargs
                )
                print("âœ… A100 80GB x2 model loading successful")
            except Exception as e:
                print(f"âš ï¸ Quantized model loading failed, retrying with basic settings: {e}")
                # ì–‘ìí™” ì—†ì´ ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì¬ì‹œë„
                try:
                    basic_kwargs = {
                        "trust_remote_code": True,
                        "local_files_only": True,
                        "low_cpu_mem_usage": True,
                        "device_map": device_map,
                        "torch_dtype": torch.bfloat16,  # A100ì—ì„œ bfloat16ì´ ë” ë¹ ë¦„
                        "attn_implementation": "flash_attention_2" if FLASH_ATTENTION_AVAILABLE else "eager",
                        "max_memory": {0: "70GB", 1: "70GB"},  # A100 80GB ìµœëŒ€ í™œìš©
                    }
                    self.model = AutoModelForCausalLM.from_pretrained(
                        self.model_path,
                        **basic_kwargs
                    )
                    print("âœ… Basic settings A100 80GB x2 loading successful")
                except Exception as e2:
                    print(f"âŒ Model loading completely failed: {e2}")
                    return False
            
            # ì¶”ë¡  ìµœì í™”
            self.model.eval()
            
            # A100 80GB íŠ¹í™” ì»´íŒŒì¼ ìµœì í™”
            try:
                # PyTorch 2.0+ ì»´íŒŒì¼ ìµœì í™”
                if hasattr(torch, 'compile'):
                    print("ğŸš€ Applying PyTorch compile optimization for A100...")
                    self.model = torch.compile(
                        self.model, 
                        mode="max-autotune",  # ìµœëŒ€ ì„±ëŠ¥ ëª¨ë“œ
                        fullgraph=False,      # ì•ˆì •ì„±ì„ ìœ„í•´
                        dynamic=True         # ë™ì  í˜•íƒœ ì§€ì›
                    )
                    print("âœ… PyTorch compile optimization applied")
            except Exception as e:
                print(f"âš ï¸ Compile optimization failed: {e}")
            
            # ë©”ì¸ ë””ë°”ì´ìŠ¤ ì„¤ì • (ì²« ë²ˆì§¸ GPU)
            self.device = torch.device("cuda:0")
            
            # A100 80GB íŠ¹í™” ìƒì„± ì„¤ì •
            self._setup_generation_config()
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            clear_memory()
            
            # A100 80GB x2 ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
            print("ğŸ“Š A100 80GB x2 memory usage:")
            total_allocated = 0
            total_cached = 0
            for i in range(min(self.num_gpus, torch.cuda.device_count())):
                allocated = torch.cuda.memory_allocated(i) / 1024**3
                cached = torch.cuda.memory_reserved(i) / 1024**3
                total_allocated += allocated
                total_cached += cached
                print(f"  GPU {i}: {allocated:.1f} GB allocated, {cached:.1f} GB cached")
            print(f"  Total: {total_allocated:.1f} GB allocated, {total_cached:.1f} GB cached")
            
            print("âœ… A100 80GB x2 model loading complete!")
            return True
            
        except Exception as e:
            print(f"âŒ Model loading failed: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def _get_optimized_model_config(self, device_map) -> Dict[str, Any]:
        """A100 80GB x2 ìµœì í™”ëœ ëª¨ë¸ ì„¤ì • ë°˜í™˜"""
        base_config = {
            "trust_remote_code": True,
            "local_files_only": True,
            "low_cpu_mem_usage": True,
            "device_map": device_map,
            "attn_implementation": "flash_attention_2" if FLASH_ATTENTION_AVAILABLE else "eager",
            "max_memory": {0: "70GB", 1: "70GB"},  # A100 80GB ê°ê° 70GBê¹Œì§€ ì‚¬ìš©
        }
        
        # A100 80GBë¥¼ ìœ„í•œ ê³ ì„±ëŠ¥ ì–‘ìí™” ì„¤ì •
        if BITSANDBYTES_AVAILABLE and torch.cuda.is_available():
            print("ğŸ”§ A100 80GB optimized 4-bit quantization (maximum performance)")
            try:
                bnb_config = BitsAndBytesConfig(
                    load_in_4bit=True,  # 4bit ì–‘ìí™”ë¡œ ë” ë§ì€ ë©”ëª¨ë¦¬ í™•ë³´
                    bnb_4bit_compute_dtype=torch.bfloat16,  # A100ì—ì„œ bfloat16ì´ ìµœì 
                    bnb_4bit_use_double_quant=True,  # ë”ë¸” ì–‘ìí™”ë¡œ ì„±ëŠ¥ í–¥ìƒ
                    bnb_4bit_quant_type="nf4",  # NormalFloat 4bit
                    llm_int8_enable_fp32_cpu_offload=False,  # A100ì—ì„œëŠ” CPU ì˜¤í”„ë¡œë“œ ë¶ˆí•„ìš”
                )
                base_config.update({
                    "quantization_config": bnb_config,
                    "torch_dtype": torch.bfloat16,
                })
            except Exception as e:
                print(f"âš ï¸ Quantization setup failed, using bfloat16: {e}")
                base_config["torch_dtype"] = torch.bfloat16
        else:
            print("ğŸ”§ A100 80GB bfloat16 configuration")
            base_config["torch_dtype"] = torch.bfloat16
        
        return base_config
    
    def _setup_generation_config(self):
        """A100 80GB íŠ¹í™” ìƒì„± ì„¤ì •"""
        self.generation_config = {
            "do_sample": True,
            "temperature": 0.7,
            "top_p": 0.9,
            "top_k": 50,
            "repetition_penalty": 1.05,
            "pad_token_id": self.tokenizer.pad_token_id,
            "eos_token_id": self.tokenizer.eos_token_id,
            "use_cache": True,
            "num_beams": 1,  # ë¹ ë¥¸ ì¶”ë¡ ì„ ìœ„í•´ beam search ë¹„í™œì„±í™”
            "max_length": 8192,  # A100 80GBì—ì„œ ë” ê¸´ ì»¨í…ìŠ¤íŠ¸ ì§€ì›
            "early_stopping": True,  # ë¶ˆí•„ìš”í•œ ìƒì„± ì¤‘ë‹¨
        }
    
    def _build_optimized_prompt(self, user_input_context: str) -> str:
        """ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ì‚¬ìš©í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        # DeepSeek í˜•ì‹ í™•ì¸ í›„ ì ìš©
        if hasattr(self.tokenizer, 'chat_template') and self.tokenizer.chat_template:
            # ì±„íŒ… í…œí”Œë¦¿ì´ ìˆëŠ” ê²½ìš°
            messages = [{"role": "user", "content": self.prompt_template.format(user_input_context=user_input_context)}]
            try:
                full_prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            except:
                # ì±„íŒ… í…œí”Œë¦¿ ì ìš© ì‹¤íŒ¨ì‹œ ê¸°ë³¸ í˜•ì‹ ì‚¬ìš©
                full_prompt = f"""<ï½œstartâ–headerâ–idï½œ>user<ï½œendâ–headerâ–idï½œ>

{self.prompt_template.format(user_input_context=user_input_context)}<ï½œeotâ–idï½œ><ï½œstartâ–headerâ–idï½œ>assistant<ï½œendâ–headerâ–idï½œ>

"""
        else:
            # ê¸°ë³¸ DeepSeek í˜•ì‹
            full_prompt = f"""<ï½œstartâ–headerâ–idï½œ>user<ï½œendâ–headerâ–idï½œ>

{self.prompt_template.format(user_input_context=user_input_context)}<ï½œeotâ–idï½œ><ï½œstartâ–headerâ–idï½œ>assistant<ï½œendâ–headerâ–idï½œ>

"""
        return full_prompt
    
    def ask_deepseek(self, user_input_context: str, max_new_tokens: int = 1024, **kwargs) -> str:
        """A100 80GB x2 ê³ ì„±ëŠ¥ DeepSeek ì¶”ë¡ """
        if self.model is None or self.tokenizer is None:
            return "âŒ Model not loaded"
        
        # ìƒì„± ì„¤ì • ì—…ë°ì´íŠ¸
        generation_config = self.generation_config.copy()
        generation_config.update(kwargs)
        generation_config["max_new_tokens"] = max_new_tokens
        
        # ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = self._build_optimized_prompt(user_input_context)
        
        # A100 ìµœì í™” í† í¬ë‚˜ì´ì§•
        try:
            inputs = self.tokenizer(
                prompt, 
                return_tensors="pt", 
                return_attention_mask=True,
                truncation=True,
                max_length=6144,  # A100 80GBì—ì„œ ë” ê¸´ ì»¨í…ìŠ¤íŠ¸
                padding=False
            )
        except Exception as e:
            print(f"âŒ Tokenization failed: {e}")
            return "Tokenization error occurred"
        
        # ë©”ì¸ GPUë¡œ ì…ë ¥ ì „ì†¡
        input_ids = inputs.input_ids.to(self.device, non_blocking=True)  # ë¹„ë™ê¸° ì „ì†¡
        attention_mask = inputs.attention_mask.to(self.device, non_blocking=True)
        
        print(f"ğŸ¤” Processing context on A100 80GB x2: {user_input_context[:50]}{'...' if len(user_input_context) > 50 else ''}")
        
        try:
            # A100 80GB ìµœì í™”ëœ ì¶”ë¡ 
            with torch.no_grad():
                # bfloat16 autocast for A100
                with torch.cuda.amp.autocast(enabled=True, dtype=torch.bfloat16):
                    generated_ids = self.model.generate(
                        input_ids=input_ids,
                        attention_mask=attention_mask,
                        **generation_config
                    )
            
            # ì‘ë‹µ ë””ì½”ë”©
            answer_ids = generated_ids[0][input_ids.shape[-1]:]
            full_response = self.tokenizer.decode(answer_ids, skip_special_tokens=True)
            
            # ìµœì¢… ë‹µë³€ ì¶”ì¶œ
            final_answer = self._extract_final_answer(full_response)
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            del input_ids, attention_mask, generated_ids
            clear_memory()
            
            return final_answer
            
        except Exception as e:
            print(f"âŒ Inference error: {e}")
            clear_memory()
            return f"Sorry, an error occurred during processing: {str(e)}"
    
    def _extract_final_answer(self, response: str) -> str:
        """DeepSeek-R1 ì‘ë‹µì—ì„œ ìµœì¢… ë‹µë³€ ì¶”ì¶œ (ìµœì í™”)"""
        # thinking íƒœê·¸ ì œê±°
        if "<ï½œthinkingï½œ>" in response and "<ï½œ/thinkingï½œ>" in response:
            parts = response.split("<ï½œ/thinkingï½œ>")
            final_answer = parts[1].strip() if len(parts) > 1 else response.strip()
        else:
            final_answer = response.strip()
        
        # ë¶ˆí•„ìš”í•œ í† í°ë“¤ ì¼ê´„ ì œê±°
        unwanted_tokens = ["<ï½œthinkingï½œ>", "<ï½œ/thinkingï½œ>", "<ï½œeotâ–idï½œ>", "<ï½œendâ–ofâ–textï½œ>"]
        for token in unwanted_tokens:
            final_answer = final_answer.replace(token, "")
        
        return final_answer.strip()
    
    def get_memory_stats(self) -> Dict[str, Any]:
        """A100 80GB x2 ë©”ëª¨ë¦¬ ìƒíƒœ ë°˜í™˜"""
        if not torch.cuda.is_available():
            return {"error": "CUDA not available"}
        
        gpu_stats = []
        total_allocated = 0
        total_cached = 0
        total_memory = 0
        
        for i in range(min(self.num_gpus, torch.cuda.device_count())):
            allocated = torch.cuda.memory_allocated(i) / 1024**3
            cached = torch.cuda.memory_reserved(i) / 1024**3
            gpu_total = torch.cuda.get_device_properties(i).total_memory / 1024**3
            
            gpu_stats.append({
                "gpu_id": i,
                "allocated_gb": allocated,
                "cached_gb": cached,
                "total_gb": gpu_total,
                "usage_percent": (allocated / gpu_total) * 100
            })
            
            total_allocated += allocated
            total_cached += cached
            total_memory += gpu_total
        
        return {
            "gpu_stats": gpu_stats,
            "total_allocated_gb": total_allocated,
            "total_cached_gb": total_cached,
            "total_memory_gb": total_memory,
            "total_usage_percent": (total_allocated / total_memory) * 100
        }

def check_model_files(model_path: str):
    """ëª¨ë¸ íŒŒì¼ êµ¬ì¡° í™•ì¸"""
    print(f"ğŸ” ëª¨ë¸ íŒŒì¼ êµ¬ì¡° í™•ì¸: {model_path}")
    
    required_files = [
        "config.json",
        "tokenizer.json",
        "tokenizer_config.json",
    ]
    
    optional_files = [
        "tokenizer.model",
        "vocab.txt",
        "merges.txt"
    ]
    
    for file in required_files:
        file_path = os.path.join(model_path, file)
        if os.path.exists(file_path):
            print(f"âœ… {file} ì¡´ì¬")
        else:
            print(f"âŒ {file} ëˆ„ë½")
    
    for file in optional_files:
        file_path = os.path.join(model_path, file)
        if os.path.exists(file_path):
            print(f"âœ… {file} ì¡´ì¬ (ì„ íƒì‚¬í•­)")
    
    # ëª¨ë¸ ê°€ì¤‘ì¹˜ íŒŒì¼ í™•ì¸
    weight_files = [f for f in os.listdir(model_path) if f.endswith(('.bin', '.safetensors'))]
    print(f"ğŸ“¦ ëª¨ë¸ ê°€ì¤‘ì¹˜ íŒŒì¼: {len(weight_files)}ê°œ")
    for file in weight_files[:5]:  # ì²˜ìŒ 5ê°œë§Œ í‘œì‹œ
        print(f"  - {file}")
    if len(weight_files) > 5:
        print(f"  ... ë° {len(weight_files) - 5}ê°œ ë”")

def interactive_chat():
    """A100 80GB x2ë¥¼ ì‚¬ìš©í•œ ê³ ì„±ëŠ¥ ëŒ€í™”í˜• ì±„íŒ… ì‹œìŠ¤í…œ"""
    print("=" * 70)
    print("ğŸš€ DeepSeek Next Word Prediction System (A100 80GB x2 Optimized)")
    print("=" * 70)
    
    # ëª¨ë¸ ê²½ë¡œ
    model_path = "/scratch/jsong132/Increase_MLLM_Ability/DeepSeek_R1_Distill_Llama_70B"
    
    # ëª¨ë¸ íŒŒì¼ êµ¬ì¡° í™•ì¸
    check_model_files(model_path)
    
    # A100 80GB x2 ì±„íŒ… ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    chat_system = OptimizedDeepSeekChat(model_path, num_gpus=2)
    
    # ëª¨ë¸ ë¡œë”©
    if not chat_system.load_model():
        print("âŒ Model loading failed. Terminating program.")
        return
    
    print("\nâœ… A100 80GB x2 model loading complete! (Next word prediction mode)")
    print("ğŸ’¡ Usage: Input context to predict the next word")
    print("ğŸ’¡ Commands:")
    print("  - 'quit', 'exit', 'ì¢…ë£Œ' : Exit program")
    print("  - 'clear', 'í´ë¦¬ì–´' : Clear screen")
    print("  - 'memory', 'ë©”ëª¨ë¦¬' : Check A100 80GB x2 memory status")
    print("  - 'example', 'ì˜ˆì‹œ' : View usage examples")
    print("-" * 70)
    
    # A100 80GB ìµœì í™” ì„±ëŠ¥ ì„¤ì •
    settings = {
        'max_new_tokens': 3072,  # A100 80GBì—ì„œ ë” ê¸´ ìƒì„±
        'temperature': 0.8,      # ê· í˜•ì¡íŒ ì°½ì˜ì„±
        'top_p': 0.9,
        'repetition_penalty': 1.05,
    }
    
    conversation_count = 0
    
    # A100 80GB ì›Œë°ì—…
    print("ğŸ”¥ A100 80GB x2 model warmup...")
    try:
        warmup_response = chat_system.ask_deepseek("ì•ˆë…•í•˜ì„¸ìš”. ì˜¤ëŠ˜", max_new_tokens=50)
        print(f"ğŸ”¥ Warmup complete")
    except Exception as e:
        print(f"âš ï¸ Warmup error: {e}")
    
    while True:
        try:
            user_input = input(f"\n[{conversation_count + 1}] ë¬¸ë§¥ ì…ë ¥: ").strip()
            
            # ëª…ë ¹ì–´ ì²˜ë¦¬
            if user_input.lower() in ['quit', 'exit', 'ì¢…ë£Œ', 'q']:
                print("\nğŸ‘‹ Terminating A100 80GB x2 next word prediction system. Thank you!")
                break
            elif user_input.lower() in ['clear', 'í´ë¦¬ì–´']:
                os.system('clear' if os.name == 'posix' else 'cls')
                continue
            elif user_input.lower() in ['memory', 'ë©”ëª¨ë¦¬']:
                stats = chat_system.get_memory_stats()
                if "error" not in stats:
                    print(f"ğŸ“Š A100 80GB x2 memory status:")
                    for gpu_stat in stats['gpu_stats']:
                        print(f"  GPU {gpu_stat['gpu_id']}:")
                        print(f"    - Allocated: {gpu_stat['allocated_gb']:.1f} GB")
                        print(f"    - Cached: {gpu_stat['cached_gb']:.1f} GB")
                        print(f"    - Total: {gpu_stat['total_gb']:.1f} GB")
                        print(f"    - Usage: {gpu_stat['usage_percent']:.1f}%")
                    print(f"  Total usage: {stats['total_usage_percent']:.1f}%")
                else:
                    print("CUDA not available")
                continue
            elif user_input.lower() in ['example', 'ì˜ˆì‹œ']:
                print("ğŸ“‹ Usage examples:")
                print("  Input: 'ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì •ë§'")
                print("  Output: Thought: [Model's reasoning process] Next Word: ì¢‹ë„¤ìš”")
                print("")
                print("  Input: 'íŒŒì´ì¬ì—ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼'")
                print("  Output: Thought: [Model's reasoning process] Next Word: ìƒì„±í•˜ë ¤ë©´")
                continue
            elif not user_input:
                print("Please input context")
                continue
            
            # DeepSeek A100 80GB x2 ê³ ì„±ëŠ¥ ì¶”ë¡ 
            import time
            start_time = time.time()
            
            answer = chat_system.ask_deepseek(
                user_input,  # user_input_contextë¡œ ì‚¬ìš©
                max_new_tokens=settings['max_new_tokens'],
                temperature=settings['temperature'],
                top_p=settings['top_p'],
                repetition_penalty=settings['repetition_penalty']
            )
            
            end_time = time.time()
            response_time = end_time - start_time
            
            print(f"\nğŸ¤– DeepSeek A100 80GB x2 prediction result ({response_time:.1f}s):")
            print(f"{answer}")
            conversation_count += 1
            
            # A100 80GB ì£¼ê¸°ì  ë©”ëª¨ë¦¬ ì •ë¦¬
            if conversation_count % 5 == 0:  # A100 80GBì—ì„œëŠ” ëœ ìì£¼ ì •ë¦¬
                clear_memory()
            
        except KeyboardInterrupt:
            print("\n\nâš ï¸ Ctrl+C detected")
            user_choice = input("Really want to exit? (y/n): ").strip().lower()
            if user_choice in ['y', 'yes', 'ì˜ˆ']:
                break
            else:
                print("Continuing...")
                continue
        except Exception as e:
            print(f"\nâŒ Error occurred: {e}")
            print("Please try again")
            continue

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # ì‹œìŠ¤í…œ ì²´í¬
    print("ğŸ” System environment check...")
    print(f"PyTorch version: {torch.__version__}")
    print(f"CUDA available: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"CUDA version: {torch.version.cuda}")
        print(f"GPU device: {torch.cuda.get_device_name()}")
    
    # Transformers ë²„ì „ í™•ì¸
    try:
        import transformers
        print(f"Transformers version: {transformers.__version__}")
    except:
        pass
    
    interactive_chat()

if __name__ == "__main__":
    main()