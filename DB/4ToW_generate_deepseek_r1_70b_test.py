# -*- coding: utf-8 -*-
import torch
import gc
import os
from transformers import AutoTokenizer, AutoModelForCausalLM
from typing import Optional, Dict, Any
import warnings
warnings.filterwarnings("ignore")

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (ì„±ëŠ¥ ìµœì í™”)
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["CUDA_LAUNCH_BLOCKING"] = "0"
os.environ["TRANSFORMERS_OFFLINE"] = "1"  # ì˜¤í”„ë¼ì¸ ëª¨ë“œ ê°•ì œ

# ë©€í‹° GPU ì„¤ì •
os.environ["CUDA_VISIBLE_DEVICES"] = "0,1"  # GPU 0, 1 ì‚¬ìš©

# bitsandbytes ê°€ì ¸ì˜¤ê¸° ì‹œë„
try:
    from transformers import BitsAndBytesConfig
    BITSANDBYTES_AVAILABLE = True
    print("âœ… BitsAndBytesConfigë¥¼ ì„±ê³µì ìœ¼ë¡œ importí–ˆìŠµë‹ˆë‹¤.")
except ImportError:
    BITSANDBYTES_AVAILABLE = False
    print("âš ï¸ BitsAndBytesConfig import ì‹¤íŒ¨ - FP16 ì–‘ìí™”ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")

def setup_torch_optimizations():
    """PyTorch ìµœì í™” ì„¤ì •"""
    # ë©”ëª¨ë¦¬ ê´€ë¦¬ ìµœì í™”
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True
    torch.backends.cudnn.benchmark = True
    torch.backends.cudnn.deterministic = False
    
    # CUDA ìºì‹œ ìµœì í™”
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        # ë©”ëª¨ë¦¬ í”„ë˜ê·¸ë©˜í…Œì´ì…˜ ë°©ì§€ - ë©€í‹° GPUìš© ì„¤ì •
        os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:1024,expandable_segments:True"

def clear_memory():
    """íš¨ìœ¨ì ì¸ ë©”ëª¨ë¦¬ ì •ë¦¬ í•¨ìˆ˜ (ë©€í‹° GPU)"""
    gc.collect()
    if torch.cuda.is_available():
        for i in range(torch.cuda.device_count()):
            with torch.cuda.device(i):
                torch.cuda.empty_cache()
                torch.cuda.synchronize()

def get_multi_gpu_device_map(num_gpus: int = 2):
    """ë©€í‹° GPU ë””ë°”ì´ìŠ¤ ë§µ ìƒì„±"""
    if num_gpus == 2:
        # A100 x2 ìµœì  ë¶„ë°°
        device_map = {
            "model.embed_tokens": 0,
            "model.norm": 1,
            "lm_head": 1,
        }
        
        # ë ˆì´ì–´ë¥¼ ë‘ GPUì— ê· ë“± ë¶„ë°° (70B ëª¨ë¸ ê¸°ì¤€)
        num_layers = 80  # DeepSeek-R1 Distill Llama 70B ë ˆì´ì–´ ìˆ˜
        layers_per_gpu = num_layers // 2
        
        for i in range(num_layers):
            if i < layers_per_gpu:
                device_map[f"model.layers.{i}"] = 0
            else:
                device_map[f"model.layers.{i}"] = 1
        
        return device_map
    else:
        return "auto"

class OptimizedDeepSeekChat:
    def __init__(self, model_path: str, num_gpus: int = 2):
        self.model_path = model_path
        self.num_gpus = num_gpus
        self.model = None
        self.tokenizer = None
        self.device = None
        self.generation_config = None
        
        # ìºì‹œëœ í† í° ì‹œí€€ìŠ¤ë“¤
        self.cached_tokens = {}
        
        # ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
        self.prompt_template = """Task Instruction: Given certain text, you need to predict the next word of it. Moreover, before your output, you could first give short thoughts about how you infer the next word based on the provided context.

Here are five examples for the task:

Example 0: ìš°ë¦¬ëŠ” ê°€ë” ì˜¨ë¼ì¸ ì¿ í°ê³¼ ê¸°íƒ€ íŠ¹ë³„ í˜œíƒì„ ì œê³µí•©ë‹ˆë‹¤. <hCoT> Customers can explore additional ways to find deals beyond online coupons, like subscribing. </hCoT> ë˜ëŠ” ì œí’ˆ ì—°êµ¬ì— ì°¸ì—¬í•˜ê³  ì‹¶ë‹¤ë©´ 'í™ˆ ì œí’ˆ ë°°ì¹˜'ë¥¼ ì²´í¬í•˜ê³  ëª‡ ê°€ì§€ ì§ˆë¬¸ì— ë‹µí•´ì£¼ì„¸ìš”. ë¬´ì—‡ì„ ê¸°ë‹¤ë¦¬ê³  ìˆë‚˜ìš”?

Example 1: ë°©ì •ì‹ 2x + 5 = 17ì„ í’€ì–´ë³´ì„¸ìš”. ë¨¼ì € ì–‘ë³€ì—ì„œ 5ë¥¼ <hCoT> The context presents an equation 2x + 5 = 17 and mentions subtracting 5 from both sides, so the next word should be 'ë¹¼ë©´' to describe the subtraction operation. </hCoT> ë¹¼ë©´ 2x = 12ê°€ ë©ë‹ˆë‹¤. ê·¸ ë‹¤ìŒ ì–‘ë³€ì„ 2ë¡œ <hCoT> The context shows 2x = 12 and mentions dividing both sides by 2, so the next word should be 'ë‚˜ëˆ„ë©´' to complete the division step. </hCoT> ë‚˜ëˆ„ë©´ x = 6ì´ ë‹µì…ë‹ˆë‹¤.

Example 2: Unityì—ì„œ 2D ê°ì²´ë¥¼ ë“œë˜ê·¸í•  ë•Œ ë‹¤ë¥¸ ê°ì²´ì™€ì˜ ìµœì†Œ ê±°ë¦¬ëŠ” 1.5fì…ë‹ˆë‹¤. ë‘ ê°ì²´ê°€ <hCoT> The context describes distance constraints for 2D objects in Unity, so the next word should be 'ì—°ê²°ë˜ë©´' to describe what happens when objects connect. </hCoT> ì—°ê²°ë˜ë©´ ë“œë˜ê·¸ê°€ ë” ì œí•œë©ë‹ˆë‹¤.

Example 3: ëŒ€ìˆ˜í•™ì—ì„œ ëŒ€ì²´ëŠ” ë¬¸ìë¥¼ ìˆ«ìë¡œ ë°”ê¾¸ëŠ” ê²ƒì…ë‹ˆë‹¤. ìˆ«ìì™€ <hCoT> The context explains algebraic substitution involving numbers, so the next word should be 'ë¬¸ì' as algebra deals with both numbers and variables. </hCoT> ë¬¸ì ì‚¬ì´ì—ëŠ” ê³±ì…ˆ ê¸°í˜¸ê°€ ìˆ¨ê²¨ì ¸ ìˆìŠµë‹ˆë‹¤.

Example 4: ëœë‹¬ì¦ˆë¹Œ ì´ì‚¬ íšŒì‚¬ Movers MAX ë””ë ‰í† ë¦¬ëŠ” <hCoT> The context introduces a moving company directory called Movers MAX, so the next word should be 'ì´ì‚¬' to specify what kind of resources this directory provides. </hCoT> ì´ì‚¬ ìì›ì„ ìœ„í•œ ì›ìŠ¤í†± ì†ŒìŠ¤ì…ë‹ˆë‹¤.

Now please give me a pair of your prediction for the thought and next word based on the following context:

{user_input_context}

Thought:
Next Word:"""
        
        # ì„±ëŠ¥ ìµœì í™” ì„¤ì •
        setup_torch_optimizations()
        
    def load_model(self) -> bool:
        """ë©€í‹° GPU ìµœì í™”ëœ ëª¨ë¸ ë¡œë”©"""
        print(f"ğŸš€ ë¡œì»¬ ê²½ë¡œ '{self.model_path}'ì—ì„œ ëª¨ë¸ ë¡œë”© ì¤‘...")
        print(f"ğŸ”§ A100 x{self.num_gpus} ë©€í‹° GPU ìµœì í™” ì„¤ì • ì ìš©...")

        # GPU ì •ë³´ ì¶œë ¥
        if torch.cuda.is_available():
            print(f"ğŸ¯ ì‚¬ìš© ê°€ëŠ¥í•œ GPU: {torch.cuda.device_count()}ê°œ")
            for i in range(min(self.num_gpus, torch.cuda.device_count())):
                gpu_name = torch.cuda.get_device_name(i)
                gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
                print(f"  GPU {i}: {gpu_name} ({gpu_memory:.1f} GB)")
        else:
            print("âŒ CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False

        try:
            # ëª¨ë¸ ì„¤ì • íŒŒì¼ í™•ì¸
            config_path = os.path.join(self.model_path, "config.json")
            if not os.path.exists(config_path):
                print(f"âŒ config.jsonì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {config_path}")
                return False
            
            # í† í¬ë‚˜ì´ì € ë¡œë”©
            print("ğŸ“ í† í¬ë‚˜ì´ì € ë¡œë”© ì¤‘...")
            try:
                self.tokenizer = AutoTokenizer.from_pretrained(
                    self.model_path, 
                    trust_remote_code=True, 
                    local_files_only=True,
                    use_fast=False,  # fast tokenizer ë¹„í™œì„±í™”ë¡œ í˜¸í™˜ì„± ê°œì„ 
                    padding_side="left"
                )
                print("âœ… AutoTokenizerë¡œ ë¡œë”© ì„±ê³µ")
            except Exception as e:
                print(f"âŒ AutoTokenizer ë¡œë”© ì‹¤íŒ¨: {e}")
                return False
            
            # pad_token ì„¤ì •
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # ë©€í‹° GPU ë””ë°”ì´ìŠ¤ ë§µ ìƒì„±
            device_map = get_multi_gpu_device_map(self.num_gpus)
            print(f"ğŸ—ºï¸ ë©€í‹° GPU ë””ë°”ì´ìŠ¤ ë§µ ìƒì„± ì™„ë£Œ")
            
            # ëª¨ë¸ ë¡œë”©
            try:
                model_kwargs = self._get_optimized_model_config(device_map)
                print("ğŸ”¥ AutoModelForCausalLMìœ¼ë¡œ ë©€í‹° GPU ëª¨ë¸ ë¡œë”© ì‹œì‘...")
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_path, 
                    **model_kwargs
                )
                print("âœ… ë©€í‹° GPU ëª¨ë¸ ë¡œë”© ì„±ê³µ")
            except Exception as e:
                print(f"âš ï¸ ì–‘ìí™” ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨, ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì¬ì‹œë„: {e}")
                # ì–‘ìí™” ì—†ì´ ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì¬ì‹œë„
                try:
                    basic_kwargs = {
                        "trust_remote_code": True,
                        "local_files_only": True,
                        "low_cpu_mem_usage": True,
                        "device_map": device_map,
                        "torch_dtype": torch.float16,
                        "attn_implementation": "eager",
                    }
                    self.model = AutoModelForCausalLM.from_pretrained(
                        self.model_path,
                        **basic_kwargs
                    )
                    print("âœ… ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ë©€í‹° GPU ë¡œë”© ì„±ê³µ")
                except Exception as e2:
                    print(f"âŒ ëª¨ë¸ ë¡œë”© ì™„ì „ ì‹¤íŒ¨: {e2}")
                    return False
            
            # ì¶”ë¡  ìµœì í™”
            self.model.eval()
            
            # ë©”ì¸ ë””ë°”ì´ìŠ¤ ì„¤ì • (ì²« ë²ˆì§¸ GPU)
            self.device = torch.device("cuda:0")
            
            # ìƒì„± ì„¤ì • ìµœì í™”
            self._setup_generation_config()
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            clear_memory()
            
            # ë©€í‹° GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
            print("ğŸ“Š ë©€í‹° GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:")
            total_allocated = 0
            total_cached = 0
            for i in range(min(self.num_gpus, torch.cuda.device_count())):
                allocated = torch.cuda.memory_allocated(i) / 1024**3
                cached = torch.cuda.memory_reserved(i) / 1024**3
                total_allocated += allocated
                total_cached += cached
                print(f"  GPU {i}: {allocated:.1f} GB allocated, {cached:.1f} GB cached")
            print(f"  ì´í•©: {total_allocated:.1f} GB allocated, {total_cached:.1f} GB cached")
            
            print("âœ… ë©€í‹° GPU ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
            return True
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def _get_optimized_model_config(self, device_map) -> Dict[str, Any]:
        """ë©€í‹° GPU ìµœì í™”ëœ ëª¨ë¸ ì„¤ì • ë°˜í™˜"""
        base_config = {
            "trust_remote_code": True,
            "local_files_only": True,
            "low_cpu_mem_usage": True,
            "device_map": device_map,
            "attn_implementation": "eager",  # FlashAttention ëŒ€ì‹  ì•ˆì •ì ì¸ eager ì‚¬ìš©
            "max_memory": {0: "35GB", 1: "35GB"},  # ê° GPUë³„ ìµœëŒ€ ë©”ëª¨ë¦¬ ì œí•œ
        }
        
        # ì–‘ìí™” ì„¤ì • (ë©€í‹° GPUì—ì„œëŠ” ë” ë³´ìˆ˜ì ìœ¼ë¡œ)
        if BITSANDBYTES_AVAILABLE and torch.cuda.is_available():
            print("ğŸ”§ ë©€í‹° GPU 8-bit ì–‘ìí™” ì„¤ì • (ì•ˆì •ì„± ìš°ì„ )")
            try:
                bnb_config = BitsAndBytesConfig(
                    load_in_8bit=True,  # 4bit ëŒ€ì‹  8bitë¡œ ì•ˆì •ì„± í™•ë³´
                    llm_int8_enable_fp32_cpu_offload=True,
                )
                base_config.update({
                    "quantization_config": bnb_config,
                    "torch_dtype": torch.float16,
                })
            except Exception as e:
                print(f"âš ï¸ ì–‘ìí™” ì„¤ì • ì‹¤íŒ¨, FP16 ì‚¬ìš©: {e}")
                base_config["torch_dtype"] = torch.float16
        else:
            print("ğŸ”§ ë©€í‹° GPU FP16 ì„¤ì •")
            base_config["torch_dtype"] = torch.float16
        
        return base_config
    
    def _setup_generation_config(self):
        """ìµœì í™”ëœ ìƒì„± ì„¤ì •"""
        self.generation_config = {
            "do_sample": True,
            "temperature": 0.7,
            "top_p": 0.9,
            "top_k": 50,
            "repetition_penalty": 1.05,
            "pad_token_id": self.tokenizer.pad_token_id,
            "eos_token_id": self.tokenizer.eos_token_id,
            "use_cache": True,
            "num_beams": 1,  # ë¹ ë¥¸ ì¶”ë¡ ì„ ìœ„í•´ beam search ë¹„í™œì„±í™”
        }
    
    def _build_optimized_prompt(self, user_input_context: str) -> str:
        """ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ì‚¬ìš©í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        # DeepSeek í˜•ì‹ í™•ì¸ í›„ ì ìš©
        if hasattr(self.tokenizer, 'chat_template') and self.tokenizer.chat_template:
            # ì±„íŒ… í…œí”Œë¦¿ì´ ìˆëŠ” ê²½ìš°
            messages = [{"role": "user", "content": self.prompt_template.format(user_input_context=user_input_context)}]
            try:
                full_prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            except:
                # ì±„íŒ… í…œí”Œë¦¿ ì ìš© ì‹¤íŒ¨ì‹œ ê¸°ë³¸ í˜•ì‹ ì‚¬ìš©
                full_prompt = f"""<ï½œstartâ–headerâ–idï½œ>user<ï½œendâ–headerâ–idï½œ>

{self.prompt_template.format(user_input_context=user_input_context)}<ï½œeotâ–idï½œ><ï½œstartâ–headerâ–idï½œ>assistant<ï½œendâ–headerâ–idï½œ>

"""
        else:
            # ê¸°ë³¸ DeepSeek í˜•ì‹
            full_prompt = f"""<ï½œstartâ–headerâ–idï½œ>user<ï½œendâ–headerâ–idï½œ>

{self.prompt_template.format(user_input_context=user_input_context)}<ï½œeotâ–idï½œ><ï½œstartâ–headerâ–idï½œ>assistant<ï½œendâ–headerâ–idï½œ>

"""
        return full_prompt
    
    def ask_deepseek(self, user_input_context: str, max_new_tokens: int = 1024, **kwargs) -> str:
        """ë©€í‹° GPUë¥¼ ì‚¬ìš©í•œ DeepSeek ì¶”ë¡ """
        if self.model is None or self.tokenizer is None:
            return "âŒ ëª¨ë¸ì´ ë¡œë”©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        # ìƒì„± ì„¤ì • ì—…ë°ì´íŠ¸
        generation_config = self.generation_config.copy()
        generation_config.update(kwargs)
        generation_config["max_new_tokens"] = max_new_tokens
        
        # ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = self._build_optimized_prompt(user_input_context)
        
        # í† í¬ë‚˜ì´ì§• (ë°°ì¹˜ ì²˜ë¦¬ ì¤€ë¹„)
        try:
            inputs = self.tokenizer(
                prompt, 
                return_tensors="pt", 
                return_attention_mask=True,
                truncation=True,
                max_length=4096,  # ìµœëŒ€ ê¸¸ì´ ì œí•œ
                padding=False
            )
        except Exception as e:
            print(f"âŒ í† í¬ë‚˜ì´ì§• ì‹¤íŒ¨: {e}")
            return "í† í¬ë‚˜ì´ì§• ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        
        # ë©”ì¸ GPUë¡œ ì…ë ¥ ì „ì†¡
        input_ids = inputs.input_ids.to(self.device)
        attention_mask = inputs.attention_mask.to(self.device)
        
        print(f"ğŸ¤” ë©€í‹° GPUì—ì„œ ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì¤‘: {user_input_context[:50]}{'...' if len(user_input_context) > 50 else ''}")
        
        try:
            # ìµœì í™”ëœ ì¶”ë¡ 
            with torch.no_grad():
                with torch.cuda.amp.autocast(enabled=True):  # Mixed precision
                    generated_ids = self.model.generate(
                        input_ids=input_ids,
                        attention_mask=attention_mask,
                        **generation_config
                    )
            
            # ì‘ë‹µ ë””ì½”ë”©
            answer_ids = generated_ids[0][input_ids.shape[-1]:]
            full_response = self.tokenizer.decode(answer_ids, skip_special_tokens=True)
            
            # ìµœì¢… ë‹µë³€ ì¶”ì¶œ
            final_answer = self._extract_final_answer(full_response)
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            del input_ids, attention_mask, generated_ids
            clear_memory()
            
            return final_answer
            
        except Exception as e:
            print(f"âŒ ì¶”ë¡  ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            clear_memory()
            return f"ì£„ì†¡í•©ë‹ˆë‹¤. ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    def _extract_final_answer(self, response: str) -> str:
        """DeepSeek-R1 ì‘ë‹µì—ì„œ ìµœì¢… ë‹µë³€ ì¶”ì¶œ (ìµœì í™”)"""
        # thinking íƒœê·¸ ì œê±°
        if "<ï½œthinkingï½œ>" in response and "<ï½œ/thinkingï½œ>" in response:
            parts = response.split("<ï½œ/thinkingï½œ>")
            final_answer = parts[1].strip() if len(parts) > 1 else response.strip()
        else:
            final_answer = response.strip()
        
        # ë¶ˆí•„ìš”í•œ í† í°ë“¤ ì¼ê´„ ì œê±°
        unwanted_tokens = ["<ï½œthinkingï½œ>", "<ï½œ/thinkingï½œ>", "<ï½œeotâ–idï½œ>", "<ï½œendâ–ofâ–textï½œ>"]
        for token in unwanted_tokens:
            final_answer = final_answer.replace(token, "")
        
        return final_answer.strip()
    
    def get_memory_stats(self) -> Dict[str, Any]:
        """ë©€í‹° GPU ë©”ëª¨ë¦¬ ìƒíƒœ ë°˜í™˜"""
        if not torch.cuda.is_available():
            return {"error": "CUDA ì‚¬ìš© ë¶ˆê°€"}
        
        gpu_stats = []
        total_allocated = 0
        total_cached = 0
        total_memory = 0
        
        for i in range(min(self.num_gpus, torch.cuda.device_count())):
            allocated = torch.cuda.memory_allocated(i) / 1024**3
            cached = torch.cuda.memory_reserved(i) / 1024**3
            gpu_total = torch.cuda.get_device_properties(i).total_memory / 1024**3
            
            gpu_stats.append({
                "gpu_id": i,
                "allocated_gb": allocated,
                "cached_gb": cached,
                "total_gb": gpu_total,
                "usage_percent": (allocated / gpu_total) * 100
            })
            
            total_allocated += allocated
            total_cached += cached
            total_memory += gpu_total
        
        return {
            "gpu_stats": gpu_stats,
            "total_allocated_gb": total_allocated,
            "total_cached_gb": total_cached,
            "total_memory_gb": total_memory,
            "total_usage_percent": (total_allocated / total_memory) * 100
        }

def check_model_files(model_path: str):
    """ëª¨ë¸ íŒŒì¼ êµ¬ì¡° í™•ì¸"""
    print(f"ğŸ” ëª¨ë¸ íŒŒì¼ êµ¬ì¡° í™•ì¸: {model_path}")
    
    required_files = [
        "config.json",
        "tokenizer.json",
        "tokenizer_config.json",
    ]
    
    optional_files = [
        "tokenizer.model",
        "vocab.txt",
        "merges.txt"
    ]
    
    for file in required_files:
        file_path = os.path.join(model_path, file)
        if os.path.exists(file_path):
            print(f"âœ… {file} ì¡´ì¬")
        else:
            print(f"âŒ {file} ëˆ„ë½")
    
    for file in optional_files:
        file_path = os.path.join(model_path, file)
        if os.path.exists(file_path):
            print(f"âœ… {file} ì¡´ì¬ (ì„ íƒì‚¬í•­)")
    
    # ëª¨ë¸ ê°€ì¤‘ì¹˜ íŒŒì¼ í™•ì¸
    weight_files = [f for f in os.listdir(model_path) if f.endswith(('.bin', '.safetensors'))]
    print(f"ğŸ“¦ ëª¨ë¸ ê°€ì¤‘ì¹˜ íŒŒì¼: {len(weight_files)}ê°œ")
    for file in weight_files[:5]:  # ì²˜ìŒ 5ê°œë§Œ í‘œì‹œ
        print(f"  - {file}")
    if len(weight_files) > 5:
        print(f"  ... ë° {len(weight_files) - 5}ê°œ ë”")

def interactive_chat():
    """ë©€í‹° GPUë¥¼ ì‚¬ìš©í•œ ëŒ€í™”í˜• ì±„íŒ… ì‹œìŠ¤í…œ"""
    print("=" * 70)
    print("ğŸš€ DeepSeek ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡ ì‹œìŠ¤í…œ (Multi-GPU A100 x2)")
    print("=" * 70)
    
    # ëª¨ë¸ ê²½ë¡œ
    model_path = "/scratch/jsong132/Increase_MLLM_Ability/DeepSeek_R1_Distill_Llama_70B"
    
    # ëª¨ë¸ íŒŒì¼ êµ¬ì¡° í™•ì¸
    check_model_files(model_path)
    
    # ë©€í‹° GPU ì±„íŒ… ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (A100 x2)
    chat_system = OptimizedDeepSeekChat(model_path, num_gpus=2)
    
    # ëª¨ë¸ ë¡œë”©
    if not chat_system.load_model():
        print("âŒ ëª¨ë¸ ë¡œë”©ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return
    
    print("\nâœ… ë©€í‹° GPU ëª¨ë¸ ë¡œë”© ì™„ë£Œ! (ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡ ëª¨ë“œ)")
    print("ğŸ’¡ ì‚¬ìš©ë²•: ë¬¸ë§¥ì„ ì…ë ¥í•˜ë©´ ë‹¤ìŒì— ì˜¬ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤.")
    print("ğŸ’¡ ëª…ë ¹ì–´:")
    print("  - 'quit', 'exit', 'ì¢…ë£Œ' : í”„ë¡œê·¸ë¨ ì¢…ë£Œ")
    print("  - 'clear', 'í´ë¦¬ì–´' : í™”ë©´ ì •ë¦¬")
    print("  - 'memory', 'ë©”ëª¨ë¦¬' : ë©€í‹° GPU ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸")
    print("  - 'example', 'ì˜ˆì‹œ' : ì‚¬ìš© ì˜ˆì‹œ ë³´ê¸°")
    print("-" * 70)
    
    # ì„±ëŠ¥ ì„¤ì •
    settings = {
        'max_new_tokens': 2024,  # ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡ì´ë¯€ë¡œ ì§§ê²Œ
        'temperature': 1.0,     # ë” í™•ì •ì ì¸ ì˜ˆì¸¡ì„ ìœ„í•´ ë‚®ê²Œ
        'top_p': 0.9,
    }
    
    conversation_count = 0
    
    # ì²« ë²ˆì§¸ ì¶”ë¡  ì›Œë°ì—…
    print("ğŸ”¥ ë©€í‹° GPU ëª¨ë¸ ì›Œë°ì—… ì¤‘...")
    try:
        warmup_response = chat_system.ask_deepseek("ì•ˆë…•í•˜ì„¸ìš”. ì˜¤ëŠ˜", max_new_tokens=50)
        print(f"ğŸ”¥ ì›Œë°ì—… ì™„ë£Œ")
    except Exception as e:
        print(f"âš ï¸ ì›Œë°ì—… ì¤‘ ì˜¤ë¥˜: {e}")
    
    while True:
        try:
            user_input = input(f"\n[{conversation_count + 1}] ë¬¸ë§¥ ì…ë ¥: ").strip()
            
            # ëª…ë ¹ì–´ ì²˜ë¦¬
            if user_input.lower() in ['quit', 'exit', 'ì¢…ë£Œ', 'q']:
                print("\nğŸ‘‹ ë©€í‹° GPU ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡ ì‹œìŠ¤í…œì„ ì¢…ë£Œí•©ë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤!")
                break
            elif user_input.lower() in ['clear', 'í´ë¦¬ì–´']:
                os.system('clear' if os.name == 'posix' else 'cls')
                continue
            elif user_input.lower() in ['memory', 'ë©”ëª¨ë¦¬']:
                stats = chat_system.get_memory_stats()
                if "error" not in stats:
                    print(f"ğŸ“Š ë©€í‹° GPU ë©”ëª¨ë¦¬ ìƒíƒœ:")
                    for gpu_stat in stats['gpu_stats']:
                        print(f"  GPU {gpu_stat['gpu_id']}:")
                        print(f"    - í• ë‹¹ë¨: {gpu_stat['allocated_gb']:.1f} GB")
                        print(f"    - ìºì‹œë¨: {gpu_stat['cached_gb']:.1f} GB")
                        print(f"    - ì „ì²´: {gpu_stat['total_gb']:.1f} GB")
                        print(f"    - ì‚¬ìš©ë¥ : {gpu_stat['usage_percent']:.1f}%")
                    print(f"  ì „ì²´ ì‚¬ìš©ë¥ : {stats['total_usage_percent']:.1f}%")
                else:
                    print("CUDAê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.")
                continue
            elif user_input.lower() in ['example', 'ì˜ˆì‹œ']:
                print("ğŸ“‹ ì‚¬ìš© ì˜ˆì‹œ:")
                print("  ì…ë ¥: 'ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì •ë§'")
                print("  ì¶œë ¥: Thought: [ëª¨ë¸ì˜ ì¶”ë¡  ê³¼ì •] Next Word: ì¢‹ë„¤ìš”")
                print("")
                print("  ì…ë ¥: 'íŒŒì´ì¬ì—ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼'")
                print("  ì¶œë ¥: Thought: [ëª¨ë¸ì˜ ì¶”ë¡  ê³¼ì •] Next Word: ìƒì„±í•˜ë ¤ë©´")
                continue
            elif not user_input:
                print("ë¬¸ë§¥ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                continue
            
            # DeepSeekì—ê²Œ ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡ ìš”ì²­
            import time
            start_time = time.time()
            
            answer = chat_system.ask_deepseek(
                user_input,  # user_input_contextë¡œ ì‚¬ìš©
                max_new_tokens=settings['max_new_tokens'],
                temperature=settings['temperature'],
                top_p=settings['top_p']
            )
            
            end_time = time.time()
            response_time = end_time - start_time
            
            print(f"\nğŸ¤– DeepSeek ë©€í‹° GPU ì˜ˆì¸¡ ê²°ê³¼ ({response_time:.1f}ì´ˆ):")
            print(f"{answer}")
            conversation_count += 1
            
            # ì£¼ê¸°ì  ë©”ëª¨ë¦¬ ì •ë¦¬
            if conversation_count % 3 == 0:  # ë©€í‹° GPUì—ì„œëŠ” ë” ìì£¼ ì •ë¦¬
                clear_memory()
            
        except KeyboardInterrupt:
            print("\n\nâš ï¸ Ctrl+Cê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
            user_choice = input("ì •ë§ ì¢…ë£Œí•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").strip().lower()
            if user_choice in ['y', 'yes', 'ì˜ˆ']:
                break
            else:
                print("ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤...")
                continue
        except Exception as e:
            print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            print("ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
            continue

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # ì‹œìŠ¤í…œ ì²´í¬
    print("ğŸ” ì‹œìŠ¤í…œ í™˜ê²½ ì²´í¬...")
    print(f"PyTorch ë²„ì „: {torch.__version__}")
    print(f"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"CUDA ë²„ì „: {torch.version.cuda}")
        print(f"GPU ì¥ì¹˜: {torch.cuda.get_device_name()}")
    
    # Transformers ë²„ì „ í™•ì¸
    try:
        import transformers
        print(f"Transformers ë²„ì „: {transformers.__version__}")
    except:
        pass
    
    interactive_chat()

if __name__ == "__main__":
    main()