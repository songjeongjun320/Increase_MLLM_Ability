# -*- coding: utf-8 -*-
import torch
import gc
from transformers import AutoTokenizer, AutoModelForCausalLM

# bitsandbytes ê°€ì ¸ì˜¤ê¸° ì‹œë„
try:
    from transformers import BitsAndBytesConfig
    BITSANDBYTES_AVAILABLE = True
    print("BitsAndBytesConfigë¥¼ ì„±ê³µì ìœ¼ë¡œ importí–ˆìŠµë‹ˆë‹¤.")
except ImportError:
    BITSANDBYTES_AVAILABLE = False
    print("BitsAndBytesConfig import ì‹¤íŒ¨ - ê¸°ë³¸ ì–‘ìí™”ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")

def clear_memory():
    """ë©”ëª¨ë¦¬ ì •ë¦¬ í•¨ìˆ˜"""
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

def ask_deepseek(question, model, tokenizer, device, max_new_tokens=512, temperature=0.7, top_p=0.9):
    """DeepSeek ëª¨ë¸ì—ê²Œ ì§ˆë¬¸í•˜ê³  ë‹µë³€ ë°›ê¸°"""
    # ë©”ëª¨ë¦¬ ì •ë¦¬
    clear_memory()
    
    # ê°„ë‹¨í•œ Q&A í˜•ì‹ í”„ë¡¬í”„íŠ¸
    prompt = f"Question: {question}\nAnswer:"

    print(f"\n--- ì…ë ¥ í”„ë¡¬í”„íŠ¸ ---\n{prompt}\n--------------------")

    inputs = tokenizer(prompt, return_tensors="pt", return_attention_mask=True)
    input_ids = inputs.input_ids.to(device)
    attention_mask = inputs.attention_mask.to(device)

    # eos_token_idê°€ ì—¬ëŸ¬ ê°œì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ë¦¬ìŠ¤íŠ¸ë¡œ ì „ë‹¬
    eos_token_ids = [tokenizer.eos_token_id]
    if hasattr(tokenizer, 'additional_special_tokens_ids'):
        eos_token_ids.extend(tokenizer.additional_special_tokens_ids)
    
    # pad_token_idê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•Šë‹¤ë©´ eos_token_idë¡œ ì„¤ì •
    if tokenizer.pad_token_id is None:
        tokenizer.pad_token_id = tokenizer.eos_token_id

    print("ë‹µë³€ ìƒì„± ì¤‘...")
    # ìƒì„± íŒŒë¼ë¯¸í„° ì„¤ì •
    generation_kwargs = {
        "max_new_tokens": max_new_tokens,
        "temperature": temperature,
        "top_p": top_p,
        "do_sample": True,
        "pad_token_id": tokenizer.pad_token_id,
        "eos_token_id": eos_token_ids,
        "use_cache": True,
    }

    # ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ìƒì„±ì„ ìœ„í•´ torch.no_grad() ì‚¬ìš©
    with torch.no_grad():
        generated_ids = model.generate(
            input_ids,
            attention_mask=attention_mask,
            **generation_kwargs
        )

    # ì…ë ¥ ë¶€ë¶„ì„ ì œì™¸í•˜ê³  ìƒì„±ëœ í…ìŠ¤íŠ¸ë§Œ ë””ì½”ë”©
    answer_ids = generated_ids[0][input_ids.shape[-1]:]
    answer_text = tokenizer.decode(answer_ids, skip_special_tokens=True)

    # ë©”ëª¨ë¦¬ ì •ë¦¬
    del input_ids, attention_mask, generated_ids
    clear_memory()

    return answer_text.strip()

def load_model():
    """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë”©"""
    global BITSANDBYTES_AVAILABLE
    
    model_name = "deepseek-ai/DeepSeek-R1-Distill-Llama-70B"
    
    print(f"'{model_name}' ëª¨ë¸ ë¡œë”© ì¤‘...")
    print("A100 1ê°œì— ìµœì í™”ëœ ì„¤ì •ìœ¼ë¡œ ë¡œë”©í•©ë‹ˆë‹¤.")

    try:
        # í† í¬ë‚˜ì´ì € ë¡œë”©
        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        
        # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ì‚¬ìš© ì¥ì¹˜: {device}")
        
        # GPU ë©”ëª¨ë¦¬ ì •ë³´ ì¶œë ¥
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            print(f"GPU ë©”ëª¨ë¦¬: {gpu_memory:.1f} GB")

        # ëª¨ë¸ ë¡œë”© ì„¤ì •
        model_kwargs = {}
        
        if BITSANDBYTES_AVAILABLE:
            print("BitsAndBytesConfigë¥¼ ì‚¬ìš©í•œ 4-bit ì–‘ìí™” ì„¤ì •")
            try:
                # A100 1ê°œì— ìµœì í™”ëœ 4-bit ì–‘ìí™” ì„¤ì •
                bnb_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_use_double_quant=True,
                    bnb_4bit_quant_type="nf4",
                    bnb_4bit_compute_dtype=torch.bfloat16,
                )
                
                model_kwargs = {
                    "quantization_config": bnb_config,
                    "device_map": "auto",
                    "torch_dtype": torch.bfloat16,
                    "trust_remote_code": True,
                    "low_cpu_mem_usage": True,
                    "offload_folder": "./offload",
                }
                print("4-bit ì–‘ìí™” ì„¤ì • ì™„ë£Œ")
            except Exception as e:
                print(f"BitsAndBytesConfig ì„¤ì • ì‹¤íŒ¨: {e}")
                print("ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ fallbackí•©ë‹ˆë‹¤.")
                BITSANDBYTES_AVAILABLE = False
        
        if not BITSANDBYTES_AVAILABLE:
            print("ê¸°ë³¸ float16 ì–‘ìí™” ì„¤ì • ì‚¬ìš©")
            model_kwargs = {
                "torch_dtype": torch.float16,
                "device_map": "auto",
                "trust_remote_code": True,
                "low_cpu_mem_usage": True,
                "offload_folder": "./offload",
            }

        print("ëª¨ë¸ ë¡œë”© ì‹œì‘... (ìˆ˜ ë¶„ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
        model = AutoModelForCausalLM.from_pretrained(model_name, **model_kwargs)
        
        # ì¶”ë¡  ëª¨ë“œë¡œ ì„¤ì •
        model.eval()
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        clear_memory()

        print(f"'{model_name}' ëª¨ë¸ ë¡œë”© ì™„ë£Œ.")
        
        # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated() / 1024**3
            cached = torch.cuda.memory_reserved() / 1024**3
            print(f"GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {allocated:.1f} GB allocated, {cached:.1f} GB cached")

        return model, tokenizer, device

    except RuntimeError as e:
        if "out of memory" in str(e).lower():
            print("\nCUDA out of memory ì˜¤ë¥˜ ë°œìƒ!")
            print("ë‹¤ìŒ í•´ê²° ë°©ë²•ë“¤ì„ ì‹œë„í•´ë³´ì„¸ìš”:")
            print("1. ë” ì‘ì€ ë°°ì¹˜ í¬ê¸° ì‚¬ìš©")
            print("2. max_new_tokens ê°’ ì¤„ì´ê¸°")
            print("3. ë” ì ê·¹ì ì¸ ë©”ëª¨ë¦¬ ê´€ë¦¬:")
            print("   - PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128 í™˜ê²½ë³€ìˆ˜ ì„¤ì •")
            print("4. bitsandbytes ì¬ì„¤ì¹˜")
        else:
            print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None, None, None
    except Exception as e:
        if "bitsandbytes" in str(e).lower() or "quantization" in str(e).lower():
            print(f"\nBitsandbytes ê´€ë ¨ ì˜¤ë¥˜: {e}")
            print("\ní•´ê²° ë°©ë²•:")
            print("1. bitsandbytes ì¬ì„¤ì¹˜:")
            print("   pip uninstall bitsandbytes -y")
            print("   pip install bitsandbytes")
        else:
            print(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None, None, None

def interactive_chat():
    """ëŒ€í™”í˜• ì±„íŒ… ì‹œìŠ¤í…œ"""
    print("=" * 60)
    print("ğŸ¤– DeepSeek ëŒ€í™”í˜• ì±„íŒ… ì‹œìŠ¤í…œ")
    print("=" * 60)
    
    # ëª¨ë¸ ë¡œë”©
    model, tokenizer, device = load_model()
    
    if model is None:
        print("ëª¨ë¸ ë¡œë”©ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return
    
    print("\nâœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ! ì´ì œ ì§ˆë¬¸í•´ë³´ì„¸ìš”.")
    print("ğŸ’¡ ëª…ë ¹ì–´:")
    print("  - 'quit', 'exit', 'ì¢…ë£Œ' : í”„ë¡œê·¸ë¨ ì¢…ë£Œ")
    print("  - 'clear', 'í´ë¦¬ì–´' : í™”ë©´ ì •ë¦¬")
    print("  - 'memory', 'ë©”ëª¨ë¦¬' : GPU ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸")
    print("  - 'settings', 'ì„¤ì •' : ìƒì„± ì„¤ì • ë³€ê²½")
    print("-" * 60)
    
    # ê¸°ë³¸ ìƒì„± ì„¤ì •
    settings = {
        'max_new_tokens': 512,
        'temperature': 0.7,
        'top_p': 0.9
    }
    
    conversation_count = 0
    
    while True:
        try:
            # ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°
            user_input = input(f"\n[{conversation_count + 1}] ë‹¹ì‹ : ").strip()
            
            # ì¢…ë£Œ ëª…ë ¹ì–´ ì²´í¬
            if user_input.lower() in ['quit', 'exit', 'ì¢…ë£Œ', 'q']:
                print("\nğŸ‘‹ ì±„íŒ…ì„ ì¢…ë£Œí•©ë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤!")
                break
            
            # í™”ë©´ ì •ë¦¬ ëª…ë ¹ì–´
            elif user_input.lower() in ['clear', 'í´ë¦¬ì–´']:
                import os
                os.system('clear' if os.name == 'posix' else 'cls')
                continue
            
            # ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
            elif user_input.lower() in ['memory', 'ë©”ëª¨ë¦¬']:
                if torch.cuda.is_available():
                    allocated = torch.cuda.memory_allocated() / 1024**3
                    cached = torch.cuda.memory_reserved() / 1024**3
                    total = torch.cuda.get_device_properties(0).total_memory / 1024**3
                    print(f"ğŸ“Š GPU ë©”ëª¨ë¦¬ ìƒíƒœ:")
                    print(f"  - í• ë‹¹ë¨: {allocated:.1f} GB")
                    print(f"  - ìºì‹œë¨: {cached:.1f} GB") 
                    print(f"  - ì „ì²´: {total:.1f} GB")
                    print(f"  - ì‚¬ìš©ë¥ : {(allocated/total)*100:.1f}%")
                else:
                    print("CUDAê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.")
                continue
            
            # ì„¤ì • ë³€ê²½
            elif user_input.lower() in ['settings', 'ì„¤ì •']:
                print(f"\nâš™ï¸ í˜„ì¬ ì„¤ì •:")
                print(f"  - max_new_tokens: {settings['max_new_tokens']}")
                print(f"  - temperature: {settings['temperature']}")
                print(f"  - top_p: {settings['top_p']}")
                
                try:
                    new_max_tokens = input(f"ìƒˆë¡œìš´ max_new_tokens (í˜„ì¬: {settings['max_new_tokens']}): ").strip()
                    if new_max_tokens:
                        settings['max_new_tokens'] = int(new_max_tokens)
                    
                    new_temp = input(f"ìƒˆë¡œìš´ temperature (í˜„ì¬: {settings['temperature']}): ").strip()
                    if new_temp:
                        settings['temperature'] = float(new_temp)
                    
                    new_top_p = input(f"ìƒˆë¡œìš´ top_p (í˜„ì¬: {settings['top_p']}): ").strip()
                    if new_top_p:
                        settings['top_p'] = float(new_top_p)
                    
                    print("âœ… ì„¤ì •ì´ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤!")
                except ValueError:
                    print("âŒ ì˜ëª»ëœ ê°’ì…ë‹ˆë‹¤. ì„¤ì •ì´ ë³€ê²½ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                continue
            
            # ë¹ˆ ì…ë ¥ ì²´í¬
            elif not user_input:
                print("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                continue
            
            # DeepSeekì—ê²Œ ì§ˆë¬¸
            print(f"\nğŸ¤– DeepSeek: ", end="", flush=True)
            
            answer = ask_deepseek(
                user_input, 
                model, 
                tokenizer, 
                device,
                max_new_tokens=settings['max_new_tokens'],
                temperature=settings['temperature'],
                top_p=settings['top_p']
            )
            
            print(answer)
            conversation_count += 1
            
            # ì£¼ê¸°ì ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì •ë¦¬ (10ë²ˆ ëŒ€í™”ë§ˆë‹¤)
            if conversation_count % 10 == 0:
                print("\nğŸ”„ ë©”ëª¨ë¦¬ ì •ë¦¬ ì¤‘...")
                clear_memory()
                print("âœ… ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
            
        except KeyboardInterrupt:
            print("\n\nâš ï¸ Ctrl+Cê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
            user_choice = input("ì •ë§ ì¢…ë£Œí•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").strip().lower()
            if user_choice in ['y', 'yes', 'ì˜ˆ']:
                break
            else:
                print("ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤...")
                continue
        except Exception as e:
            print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            print("ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
            continue

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    interactive_chat()

if __name__ == "__main__":
    main()