#!/usr/bin/env python3
"""
TOW ë°ì´í„°ì…‹ Context í† í° ê¸¸ì´ ë¶„ì„ ë° í•„í„°ë§ ìŠ¤í¬ë¦½íŠ¸

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ”:
1. tow_data í´ë”ì˜ ëª¨ë“  JSON íŒŒì¼ì„ ë¶„ì„
2. context í•„ë“œì˜ í† í° ê¸¸ì´ë¥¼ ê³„ì‚°
3. í† í° ê¸¸ì´ë³„ ë°ì´í„° ë¶„í¬ ì‹œê°í™”
4. ì§€ì •ëœ í† í° ê¸¸ì´ ì´í•˜ì˜ ë°ì´í„°ë§Œ í•„í„°ë§í•˜ì—¬ ìƒˆë¡œìš´ JSON íŒŒì¼ë¡œ ì €ì¥
"""

import json
import os
import re
from collections import Counter, defaultdict
from pathlib import Path
from typing import Dict, List, Tuple
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from tqdm import tqdm

# í•œê¸€ í† í° ê¸¸ì´ ê³„ì‚°ì„ ìœ„í•œ ì„¤ì •
plt.rcParams['font.family'] = 'DejaVu Sans'
sns.set_style("whitegrid")

def count_korean_tokens(text: str) -> int:
    """
    í•œêµ­ì–´ í…ìŠ¤íŠ¸ì˜ ë‹¨ì–´ ìˆ˜ë¥¼ ë„ì–´ì“°ê¸° ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°í•©ë‹ˆë‹¤.
    
    Args:
        text (str): ë¶„ì„í•  í…ìŠ¤íŠ¸
        
    Returns:
        int: ë„ì–´ì“°ê¸° ê¸°ì¤€ ë‹¨ì–´ ìˆ˜
    """
    if not text:
        return 0
    
    # í…ìŠ¤íŠ¸ë¥¼ ê³µë°±ìœ¼ë¡œ ë¶„ë¦¬í•˜ì—¬ ë‹¨ì–´ ìˆ˜ ê³„ì‚°
    words = text.strip().split()
    
    # ë¹ˆ ë¬¸ìì—´ ì œê±°
    words = [word for word in words if word.strip()]
    
    return len(words)

def load_json_files(data_dir: str) -> List[Dict]:
    """
    ë°ì´í„° ë””ë ‰í† ë¦¬ì—ì„œ ëª¨ë“  JSON íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤.
    
    Args:
        data_dir (str): JSON íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ ê²½ë¡œ
        
    Returns:
        List[Dict]: ëª¨ë“  ë°ì´í„° í•­ëª©ë“¤ì˜ ë¦¬ìŠ¤íŠ¸
    """
    data_dir = Path(data_dir)
    all_data = []
    
    json_files = list(data_dir.glob("*.json"))
    print(f"ë°œê²¬ëœ JSON íŒŒì¼ ìˆ˜: {len(json_files)}")
    
    for json_file in tqdm(json_files, desc="JSON íŒŒì¼ ë¡œë”©"):
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
                # íŒŒì¼ëª… ì •ë³´ ì¶”ê°€
                for item in data:
                    item['source_file'] = json_file.name
                
                all_data.extend(data)
                print(f"  {json_file.name}: {len(data)}ê°œ í•­ëª©")
                
        except Exception as e:
            print(f"  âŒ {json_file.name}: ë¡œë”© ì‹¤íŒ¨ - {e}")
    
    print(f"ì´ ë¡œë”©ëœ ë°ì´í„° í•­ëª© ìˆ˜: {len(all_data)}")
    return all_data

def analyze_token_lengths(data: List[Dict]) -> Tuple[List[int], Dict]:
    """
    ë°ì´í„°ì˜ context ë‹¨ì–´ ê¸¸ì´ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
    
    Args:
        data (List[Dict]): ë¶„ì„í•  ë°ì´í„°
        
    Returns:
        Tuple[List[int], Dict]: ë‹¨ì–´ ê¸¸ì´ ë¦¬ìŠ¤íŠ¸ì™€ í†µê³„ ì •ë³´
    """
    token_lengths = []
    file_stats = defaultdict(list)
    
    print("ë‹¨ì–´ ìˆ˜ ë¶„ì„ ì¤‘...")
    for item in tqdm(data, desc="ë‹¨ì–´ ìˆ˜ ê³„ì‚°"):
        context = item.get('context', '')
        word_count = count_korean_tokens(context)
        token_lengths.append(word_count)
        
        # íŒŒì¼ë³„ í†µê³„
        source_file = item.get('source_file', 'unknown')
        file_stats[source_file].append(word_count)
    
    # ì „ì²´ í†µê³„
    stats = {
        'total_items': len(token_lengths),
        'min_words': min(token_lengths) if token_lengths else 0,
        'max_words': max(token_lengths) if token_lengths else 0,
        'avg_words': sum(token_lengths) / len(token_lengths) if token_lengths else 0,
        'median_words': sorted(token_lengths)[len(token_lengths)//2] if token_lengths else 0,
    }
    
    # íŒŒì¼ë³„ í†µê³„
    stats['file_stats'] = {}
    for filename, lengths in file_stats.items():
        stats['file_stats'][filename] = {
            'count': len(lengths),
            'avg_words': sum(lengths) / len(lengths),
            'min_words': min(lengths),
            'max_words': max(lengths)
        }
    
    return token_lengths, stats

def plot_token_distribution(token_lengths: List[int], stats: Dict, output_dir: str = "."):
    """
    ë‹¨ì–´ ê¸¸ì´ ë¶„í¬ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤.
    
    Args:
        token_lengths (List[int]): ë‹¨ì–´ ê¸¸ì´ ë¦¬ìŠ¤íŠ¸
        stats (Dict): í†µê³„ ì •ë³´
        output_dir (str): ì¶œë ¥ ë””ë ‰í† ë¦¬
    """
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    fig.suptitle('TOW ë°ì´í„°ì…‹ Context ë‹¨ì–´ ê¸¸ì´ ë¶„ì„', fontsize=16, fontweight='bold')
    
    # 1. íˆìŠ¤í† ê·¸ë¨
    axes[0, 0].hist(token_lengths, bins=50, alpha=0.7, edgecolor='black')
    axes[0, 0].set_title('ë‹¨ì–´ ê¸¸ì´ ë¶„í¬ (íˆìŠ¤í† ê·¸ë¨)')
    axes[0, 0].set_xlabel('ë‹¨ì–´ ìˆ˜')
    axes[0, 0].set_ylabel('ë¹ˆë„')
    axes[0, 0].axvline(stats['avg_words'], color='red', linestyle='--', label=f'í‰ê· : {stats["avg_words"]:.1f}')
    axes[0, 0].axvline(stats['median_words'], color='green', linestyle='--', label=f'ì¤‘ì•™ê°’: {stats["median_words"]}')
    axes[0, 0].legend()
    
    # 2. ë°•ìŠ¤í”Œë¡¯
    axes[0, 1].boxplot(token_lengths)
    axes[0, 1].set_title('ë‹¨ì–´ ê¸¸ì´ ë¶„í¬ (ë°•ìŠ¤í”Œë¡¯)')
    axes[0, 1].set_ylabel('ë‹¨ì–´ ìˆ˜')
    
    # 3. ëˆ„ì  ë¶„í¬
    sorted_lengths = sorted(token_lengths)
    cumulative_pct = [i/len(sorted_lengths)*100 for i in range(1, len(sorted_lengths)+1)]
    axes[1, 0].plot(sorted_lengths, cumulative_pct)
    axes[1, 0].set_title('ëˆ„ì  ë¶„í¬ í•¨ìˆ˜')
    axes[1, 0].set_xlabel('ë‹¨ì–´ ìˆ˜')
    axes[1, 0].set_ylabel('ëˆ„ì  ë¹„ìœ¨ (%)')
    axes[1, 0].grid(True, alpha=0.3)
    
    # ì£¼ìš” percentile í‘œì‹œ
    percentiles = [50, 80, 90, 95, 99]
    for p in percentiles:
        idx = int(len(sorted_lengths) * p / 100) - 1
        if 0 <= idx < len(sorted_lengths):
            axes[1, 0].axvline(sorted_lengths[idx], color='red', alpha=0.5, linestyle=':', 
                             label=f'{p}%ile: {sorted_lengths[idx]}')
    axes[1, 0].legend(fontsize=8)
    
    # 4. íŒŒì¼ë³„ í†µê³„
    if 'file_stats' in stats:
        file_names = list(stats['file_stats'].keys())
        avg_words = [stats['file_stats'][f]['avg_words'] for f in file_names]
        
        axes[1, 1].bar(range(len(file_names)), avg_words)
        axes[1, 1].set_title('íŒŒì¼ë³„ í‰ê·  ë‹¨ì–´ ìˆ˜')
        axes[1, 1].set_xlabel('íŒŒì¼')
        axes[1, 1].set_ylabel('í‰ê·  ë‹¨ì–´ ìˆ˜')
        axes[1, 1].set_xticks(range(len(file_names)))
        axes[1, 1].set_xticklabels([f.replace('.json', '') for f in file_names], rotation=45)
    
    plt.tight_layout()
    
    output_path = Path(output_dir) / 'word_distribution_analysis.png'
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"ë¶„í¬ ì°¨íŠ¸ ì €ì¥ë¨: {output_path}")
    plt.show()

def print_statistics(token_lengths: List[int], stats: Dict):
    """
    í†µê³„ ì •ë³´ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
    
    Args:
        token_lengths (List[int]): ë‹¨ì–´ ê¸¸ì´ ë¦¬ìŠ¤íŠ¸
        stats (Dict): í†µê³„ ì •ë³´
    """
    print("\n" + "="*60)
    print("ğŸ“Š TOW ë°ì´í„°ì…‹ ë‹¨ì–´ ê¸¸ì´ ë¶„ì„ ê²°ê³¼")
    print("="*60)
    
    print(f"ì´ ë°ì´í„° í•­ëª© ìˆ˜: {stats['total_items']:,}")
    print(f"ìµœì†Œ ë‹¨ì–´ ìˆ˜: {stats['min_words']}")
    print(f"ìµœëŒ€ ë‹¨ì–´ ìˆ˜: {stats['max_words']}")
    print(f"í‰ê·  ë‹¨ì–´ ìˆ˜: {stats['avg_words']:.2f}")
    print(f"ì¤‘ì•™ê°’ ë‹¨ì–´ ìˆ˜: {stats['median_words']}")
    
    # Percentile ë¶„ì„
    sorted_lengths = sorted(token_lengths)
    percentiles = [50, 75, 80, 85, 90, 95, 99]
    
    print(f"\nğŸ“ˆ ë‹¨ì–´ ê¸¸ì´ Percentile ë¶„ì„:")
    for p in percentiles:
        idx = int(len(sorted_lengths) * p / 100) - 1
        if 0 <= idx < len(sorted_lengths):
            value = sorted_lengths[idx]
            count_below = len([x for x in token_lengths if x <= value])
            percentage = count_below / len(token_lengths) * 100
            print(f"  {p:2d}%ile: {value:4d} ë‹¨ì–´ (ì „ì²´ì˜ {percentage:5.1f}%ê°€ ì´ ê°’ ì´í•˜)")
    
    # ë‹¨ì–´ ê¸¸ì´ êµ¬ê°„ë³„ ë¶„í¬
    print(f"\nğŸ“Š ë‹¨ì–´ ê¸¸ì´ êµ¬ê°„ë³„ ë°ì´í„° ë¶„í¬:")
    ranges = [
        (0, 10, "ë§¤ìš° ì§§ìŒ"),
        (11, 20, "ì§§ìŒ"),
        (21, 40, "ë³´í†µ"),
        (41, 60, "ê¸º"),
        (61, 100, "ë§¤ìš° ê¸º"),
        (101, float('inf'), "ê·¹ë„ë¡œ ê¸º")
    ]
    
    for min_len, max_len, desc in ranges:
        if max_len == float('inf'):
            count = len([x for x in token_lengths if x >= min_len])
            range_desc = f"{min_len}+ ë‹¨ì–´"
        else:
            count = len([x for x in token_lengths if min_len <= x <= max_len])
            range_desc = f"{min_len}-{max_len} ë‹¨ì–´"
            
        percentage = count / len(token_lengths) * 100
        print(f"  {range_desc:12s} ({desc:8s}): {count:5,}ê°œ ({percentage:5.1f}%)")
    
    # íŒŒì¼ë³„ í†µê³„
    if 'file_stats' in stats:
        print(f"\nğŸ“ íŒŒì¼ë³„ í†µê³„:")
        for filename, file_stat in stats['file_stats'].items():
            print(f"  {filename:40s}: "
                  f"{file_stat['count']:5,}ê°œ, "
                  f"í‰ê·  {file_stat['avg_words']:6.1f} ë‹¨ì–´, "
                  f"ë²”ìœ„ {file_stat['min_words']}-{file_stat['max_words']} ë‹¨ì–´")

def filter_and_save_data(data: List[Dict], min_word_length: int, output_dir: str = "."):
    """
    ì§€ì •ëœ ë‹¨ì–´ ê¸¸ì´ ì´ˆê³¼ì˜ ë°ì´í„°ë§Œ í•„í„°ë§í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤.
    
    Args:
        data (List[Dict]): ì›ë³¸ ë°ì´í„°
        min_word_length (int): ìµœì†Œ ë‹¨ì–´ ê¸¸ì´ (ì´ ê°’ì„ ì´ˆê³¼í•˜ëŠ” ë°ì´í„°ë§Œ ì €ì¥)
        output_dir (str): ì¶œë ¥ ë””ë ‰í† ë¦¬
    """
    filtered_data = []
    
    print(f"\nğŸ” ìµœì†Œ ë‹¨ì–´ ê¸¸ì´ {min_word_length} ì´ˆê³¼ ë°ì´í„° í•„í„°ë§ ì¤‘...")
    
    for item in tqdm(data, desc="ë°ì´í„° í•„í„°ë§"):
        context = item.get('context', '')
        word_count = count_korean_tokens(context)
        
        if word_count > min_word_length:
            # source_file í•„ë“œ ì œê±° (ë¶ˆí•„ìš”í•œ ë©”íƒ€ë°ì´í„°)
            filtered_item = {k: v for k, v in item.items() if k != 'source_file'}
            filtered_data.append(filtered_item)
    
    # ê²°ê³¼ ì €ì¥
    output_path = Path(output_dir) / f'training_dataset_over_{min_word_length}_words.json'
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(filtered_data, f, ensure_ascii=False, indent=2)
    
    # í•„í„°ë§ ê²°ê³¼ ì¶œë ¥
    original_count = len(data)
    filtered_count = len(filtered_data)
    filtered_ratio = filtered_count / original_count * 100
    removed_count = original_count - filtered_count
    removed_ratio = removed_count / original_count * 100
    
    print(f"\nâœ… í•„í„°ë§ ì™„ë£Œ!")
    print(f"  ì›ë³¸ ë°ì´í„°: {original_count:,}ê°œ")
    print(f"  í•„í„°ë§ëœ ë°ì´í„°: {filtered_count:,}ê°œ ({filtered_ratio:.1f}%) - {min_word_length} ë‹¨ì–´ ì´ˆê³¼")
    print(f"  ì œê±°ëœ ë°ì´í„°: {removed_count:,}ê°œ ({removed_ratio:.1f}%) - {min_word_length} ë‹¨ì–´ ì´í•˜")
    print(f"  ì €ì¥ ê²½ë¡œ: {output_path}")
    
    return filtered_data, output_path

def get_recommended_word_lengths(word_lengths: List[int]) -> List[int]:
    """
    ë°ì´í„° ë¶„í¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê¶Œì¥ ë‹¨ì–´ ê¸¸ì´ ì„ê³„ê°’ë“¤ì„ ì œì•ˆí•©ë‹ˆë‹¤.
    (ê¸´ ë°ì´í„° í•„í„°ë§ì„ ìœ„í•œ ìµœì†Œê°’ ê¸°ì¤€)
    
    Args:
        word_lengths (List[int]): ë‹¨ì–´ ê¸¸ì´ ë¦¬ìŠ¤íŠ¸
        
    Returns:
        List[int]: ê¶Œì¥ ìµœì†Œ ë‹¨ì–´ ê¸¸ì´ ì„ê³„ê°’ë“¤
    """
    sorted_lengths = sorted(word_lengths)
    
    # ìƒìœ„ ë°ì´í„°ë¥¼ ìœ„í•œ percentile ì§€ì ë“¤ (ë‚®ì€ percentile = ë” ë§ì€ ê¸´ ë°ì´í„°)
    percentiles = [20, 25, 30, 50, 75]
    recommendations = []
    
    for p in percentiles:
        idx = int(len(sorted_lengths) * p / 100) - 1
        if 0 <= idx < len(sorted_lengths):
            recommendations.append(sorted_lengths[idx])
    
    # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
    recommendations = sorted(list(set(recommendations)))
    
    return recommendations

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # ì„¤ì •
    DATA_DIR = "4_tow_generation/old/tow_data"
    OUTPUT_DIR = "4_tow_generation/processed"
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)
    
    print("ğŸš€ TOW ë°ì´í„°ì…‹ ë¶„ì„ ì‹œì‘")
    print(f"  ë°ì´í„° ë””ë ‰í† ë¦¬: {DATA_DIR}")
    print(f"  ì¶œë ¥ ë””ë ‰í† ë¦¬: {OUTPUT_DIR}")
    
    # 1. ë°ì´í„° ë¡œë”©
    data = load_json_files(DATA_DIR)
    if not data:
        print("âŒ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # 2. ë‹¨ì–´ ê¸¸ì´ ë¶„ì„
    word_lengths, stats = analyze_token_lengths(data)
    
    # 3. í†µê³„ ì¶œë ¥
    print_statistics(word_lengths, stats)
    
    # 4. ë¶„í¬ ì‹œê°í™”
    plot_token_distribution(word_lengths, stats, OUTPUT_DIR)
    
    # 5. ê¶Œì¥ ì„ê³„ê°’ ì œì•ˆ
    recommended_lengths = get_recommended_word_lengths(word_lengths)
    print(f"\nğŸ’¡ ê¶Œì¥ ìµœì†Œ ë‹¨ì–´ ê¸¸ì´ ì„ê³„ê°’ë“¤ (ì´ ê°’ì„ ì´ˆê³¼í•˜ëŠ” ê¸´ ë°ì´í„°ë§Œ ì €ì¥):")
    for length in recommended_lengths:
        count_above = len([x for x in word_lengths if x > length])
        percentage = count_above / len(word_lengths) * 100
        print(f"  {length} ë‹¨ì–´ ì´ˆê³¼: ì „ì²´ì˜ {percentage:.1f}% ë°ì´í„° ìœ ì§€ ({count_above:,}ê°œ)")
    
    # 6. ì‚¬ìš©ì ì…ë ¥ìœ¼ë¡œ í•„í„°ë§í•  ë‹¨ì–´ ê¸¸ì´ ì„¤ì •
    print(f"\nğŸ¯ í•„í„°ë§í•  ìµœì†Œ ë‹¨ì–´ ê¸¸ì´ë¥¼ ì„ íƒí•˜ì„¸ìš” (ì´ ê°’ì„ ì´ˆê³¼í•˜ëŠ” ë°ì´í„°ë§Œ ì €ì¥ë©ë‹ˆë‹¤):")
    print(f"  (ê¶Œì¥: {recommended_lengths})")
    
    while True:
        try:
            user_input = input("ìµœì†Œ ë‹¨ì–´ ê¸¸ì´ ì…ë ¥ (ì˜ˆ: 20): ").strip()
            if not user_input:
                print("ê°’ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                continue
                
            min_word_length = int(user_input)
            
            if min_word_length < 0:
                print("0 ì´ìƒì˜ ê°’ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                continue
            
            # ì˜ˆìƒ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°
            count_above = len([x for x in word_lengths if x > min_word_length])
            percentage = count_above / len(word_lengths) * 100
            print(f"\nğŸ“‹ ì˜ˆìƒ ê²°ê³¼: {min_word_length} ë‹¨ì–´ ì´ˆê³¼ ë°ì´í„° {count_above:,}ê°œ ({percentage:.1f}%) ì €ì¥ ì˜ˆì •")
            
            confirm = input("ê³„ì†í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").strip().lower()
            if confirm in ['y', 'yes', 'ë„¤', 'ã…‡']:
                break
            elif confirm in ['n', 'no', 'ì•„ë‹ˆìš”', 'ã„´']:
                continue
            else:
                print("y(ì˜ˆ) ë˜ëŠ” n(ì•„ë‹ˆìš”)ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            
        except ValueError:
            print("ì˜¬ë°”ë¥¸ ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        except KeyboardInterrupt:
            print("\ní”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            return
    
    # 7. ë°ì´í„° í•„í„°ë§ ë° ì €ì¥
    filtered_data, output_path = filter_and_save_data(data, min_word_length, OUTPUT_DIR)
    
    print(f"\nğŸ‰ ë¶„ì„ ì™„ë£Œ! ê²°ê³¼ íŒŒì¼ë“¤:")
    print(f"  ğŸ“ˆ ë¶„í¬ ì°¨íŠ¸: {OUTPUT_DIR}/word_distribution_analysis.png")
    print(f"  ğŸ“„ í•„í„°ë§ëœ ë°ì´í„°ì…‹: {output_path}")

if __name__ == "__main__":
    # í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ import í™•ì¸
    try:
        import matplotlib.pyplot as plt
        import seaborn as sns
        import pandas as pd
        from tqdm import tqdm
    except ImportError as e:
        print(f"âŒ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤: {e}")
        print("ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”:")
        print("pip install matplotlib seaborn pandas tqdm")
        exit(1)
    
    main()