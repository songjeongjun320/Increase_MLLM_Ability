# ToW Training Pipeline Dockerfile
# Optimized for distributed training with multiple GPUs

FROM nvidia/cuda:12.1-devel-ubuntu22.04

# Set environment variables
ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONUNBUFFERED=1
ENV CUDA_HOME=/usr/local/cuda
ENV PATH=${CUDA_HOME}/bin:${PATH}
ENV LD_LIBRARY_PATH=${CUDA_HOME}/lib64:${LD_LIBRARY_PATH}

# Install system dependencies
RUN apt-get update && apt-get install -y \
    python3.10 \
    python3.10-dev \
    python3-pip \
    python3.10-venv \
    build-essential \
    cmake \
    git \
    wget \
    curl \
    software-properties-common \
    htop \
    nvtop \
    tmux \
    vim \
    openssh-server \
    && rm -rf /var/lib/apt/lists/*

# Create virtual environment
RUN python3.10 -m venv /opt/venv
ENV PATH="/opt/venv/bin:$PATH"

# Upgrade pip and install build tools
RUN pip install --no-cache-dir --upgrade pip setuptools wheel ninja

# Install PyTorch with CUDA support
RUN pip install --no-cache-dir torch==2.1.0+cu121 torchvision==0.16.0+cu121 torchaudio==2.1.0+cu121 \
    --index-url https://download.pytorch.org/whl/cu121

# Install training-specific dependencies
RUN pip install --no-cache-dir \
    transformers==4.35.2 \
    accelerate==0.24.1 \
    peft==0.6.2 \
    bitsandbytes==0.41.3 \
    deepspeed==0.12.4 \
    flash-attn==2.3.3 \
    wandb==0.16.0 \
    mlflow==2.8.1 \
    optuna==3.4.0 \
    datasets==2.14.6 \
    evaluate==0.4.1 \
    sacrebleu==2.3.1 \
    rouge-score==0.1.2 \
    bert-score==0.3.13

# Copy requirements and install additional dependencies
COPY requirements-mlops.txt /tmp/requirements-mlops.txt
RUN pip install --no-cache-dir -r /tmp/requirements-mlops.txt

# Install distributed training tools
RUN pip install --no-cache-dir \
    torch-audio \
    torchmetrics \
    lightning \
    fairscale

# Create application directory
WORKDIR /workspace

# Create training user
RUN groupadd -r trainer && useradd -r -g trainer trainer

# Copy application code
COPY mlops/ /workspace/mlops/
COPY tow_architecture/ /workspace/tow_architecture/
COPY requirements.txt /workspace/
COPY Data_Generation/ /workspace/Data_Generation/
COPY DB/ /workspace/DB/

# Install the ToW package
RUN pip install -e .

# Create necessary directories
RUN mkdir -p /workspace/logs /workspace/models /workspace/data /workspace/checkpoints /workspace/outputs && \
    chown -R trainer:trainer /workspace

# Copy training scripts
COPY mlops/docker/scripts/start-training.sh /workspace/start-training.sh
COPY mlops/docker/scripts/distributed-training.sh /workspace/distributed-training.sh
RUN chmod +x /workspace/start-training.sh /workspace/distributed-training.sh && \
    chown trainer:trainer /workspace/start-training.sh /workspace/distributed-training.sh

# Set up SSH for multi-node training (optional)
RUN mkdir -p /var/run/sshd && \
    echo 'root:screencast' | chpasswd && \
    sed -i 's/#PermitRootLogin prohibit-password/PermitRootLogin yes/' /etc/ssh/sshd_config && \
    sed 's@session\s*required\s*pam_loginuid.so@session optional pam_loginuid.so@g' -i /etc/pam.d/sshd

# Configure git (for model downloads)
RUN git config --global user.email "trainer@tow.ai" && \
    git config --global user.name "ToW Trainer"

# Switch to training user
USER trainer

# Set default environment variables
ENV MLOPS_ENV=training
ENV CUDA_VISIBLE_DEVICES=0,1,2,3
ENV NCCL_DEBUG=INFO
ENV WANDB_PROJECT=tow-training
ENV MLFLOW_TRACKING_URI=sqlite:///mlruns.db

# Expose ports for distributed training
EXPOSE 29500 29501 29502 29503

# Default command
CMD ["/workspace/start-training.sh"]